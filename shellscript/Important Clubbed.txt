Grub recovery::: (grub hung or reinstall boot partition)  <----- simple procedure

https://www.youtube.com/watch?v=lej9v2hvnAU:

corrupt grub:
dd if=/dev/zero of=/dev/sda bs=1 count=446

recover grub:
put cd 
reboot and at the boot type: linux rescue
at the shell prompt : chroot /mnt/sysimage
grub-install /dev/sda
*************************

Grub recovery :   <-----  Actual detailed  Procedure

Resolution

•Reinstall /boot partition manually with the following steps:

1.Boot the system into rescue mode:
?Insert the Red Hat Enterprise Linux CD #1 and boot your system.
?At boot prompt, type "linux rescue". This will start the rescue mode program.
?You will be prompted for your keyboard and language requirements. Enter these values as you would during the installation of Red Hat Enterprise Linux.
?Next, a screen will appear telling you that the program will now attempt to find a Red Hat Enterprise Linux installation to rescue. Select "Continue" on this screen.

?At the "sh-3.1" prompt, chroot to /mnt/sysimage:
Raw
# chroot /mnt/sysimage

2.Make sure the boot partition is labeled as described in /etc/fstab. (Assuming the boot partition is /dev/sda1):
Raw
# e2label /dev/sda1 /boot

3.Make sure the boot partition is mounted:
Raw
# mount /dev/sda1 /boot

4.Mount the CD to install the following rpms:
Raw
# mkdir /mnt/iso
# mount -o loop,ro /dev/hdc /mnt/iso
# cd /mnt/iso/Server
# rpm -Uvh --replacefiles --replacepkgs grub-0.97-13.i386.rpm
# rpm -Uvh --replacefiles --replacepkgs redhat-logos-4.9.16-1.noarch.rpm
# rpm -ivh --replacefiles --replacepkgs kernel-2.6.18-8.el5.i686.rpm

5.Install the GRUB:
Raw
# grub-install /dev/sda

6.If /boot/grub/grub.conf is lost, then it would be required to recreate it manually. The following is a sample of grub.conf, please make sure the the file "vmlinuz-2.6.18-8.el5" and "initrd-2.6.18-8.el5.img" exist under the directory of /boot (which should be installed after step 4).
Raw
default=0
timeout=5
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu

title Red Hat Enterprise Linux (2.6.18-8.el5)
        root (hd0,0)
        kernel /vmlinuz-2.6.18-8.el5 ro root=LABEL=/
        initrd /initrd-2.6.18-8.el5.img

7.Make a soft link to grub.conf:
Raw
# cd /boot/grub
# ln -s grub.conf menu.lst

8.Then please reboot the system.


*********************************


CREATE NEW FS


sCENARIO 1 : (FREE SPACE IN VG)

lvcreate -L 4G -n /dev/vg_apps/lv_u01 (or) lvcreate -L 4G -n lv_u01 vg_apps (and not:) lvcreate -l100%FREE -n lv_u01 vg_apps
mkfs.ext4 /dev/vg_apps/lv_u01
mkdir -p /u01/app/oracle

Fstab Entries:
/dev/mapper/vg_apps-lv_u01   /u01/app/oracle               ext4    defaults    1  2
mount -a


sCENARIO 2 : ( NO FREE SPACE IN VG)


Added:
Disk /dev/sdc: 68.7 GB, 68719476736 bytes

fdisk /dev/sdc n p 1 t 8e w
pvcreate /dev/sdc1
vgcreate vg_u01 /dev/sdc1
lvcreate -L 4G -n lv_u01 vg_u01
mkfs.ext3 /dev/vg_u01/lv_u01

mkdir -p /u01/app/oracle

/dev/vg_u01/lv_u01      /u01/app/oracle         ext3    defaults        1 2
mount -a

************************************

Increase swapspace 2GB :

Add a hard disk for 2GB and :
fdisk /dev/sdd (n p t 82 w)
mkswap /dev/sdd1
swapon /dev/sdd1
swapon -s (check)
swapon -a (update)
free –m
Make entries in /etc/fstab
mount -a
a

***********************************

MIGRATE DATA FROM A DIRECTORY TO LVM (SEPARATE FS)


Assumptions:
Directory to be migrated: /u01 inside / filesystem (this filesystem could be LVM or non-LVM)
Source Directory: /u01
Destination LVM: /dev/vg_u01/lv_u01

steps to be completed before doWntime:::::

lvcreate -L 10G -n /dev/vg_u01/lv_u01  ( assuming i have space in VG and news separate fs needed is of 10GB )(not : lvcreate -l100%FREE -n lv_data vg_data)
mkfs.ext3 /dev/vg_u01/lv_u01


Ensure no processes using /u01
find /u01 ! -type d -exec fuser -u {} \;
If any process is using files in /u01 directory, better to start the further activity after a reboot to single user mode

mount /dev/vg_u01/lv_u01 on /mnt (Assuming no other filesystem is already mounted on /mnt)

cd /u01

tar cf - * | (cd /mnt; tar xvf - )

cd /root

mv /u01 /u01.back.`date +'%Y%m%d'`

mkdir /u01

umount /mnt

Take a backup of /etc/fstab
cp -vip /etc/fstab /etc/fstab.back.`date +'%Y%m%d'`
Add the following line in /etc/fstab:

New line:
/dev/vg_u01/lv_u01         /u01              ext3    defaults    1 2
Reboot into normal runlevel
Or run:
mount /u01
Validate the new filesystem in "df -h" and handover the server to requestor/Server Contact/ASM

After 24 hours or so, delete the /u01.back.`date +'%Y%m%d'` directory

**********************************

MIGRATE DATA FROM PHYSICAL PARTITION TO LVM:

Assumptions:
Filesystem to be migrated: /u01
Source partition: /dev/sdc1
Destination LVM: /dev/vg_u01/lv_u01


Preparation: Create new Virtual disk of 64 GB, discover it, create PV, vg_u01, lv_u01 (ext3) on it :

example:

Steps to be Completed before downtime:
Added a 40GB  (new lvm based fs ie asked by user)
pvcreate /dev/sdd1
vgcreate vg_apps /dev/sdd1
lvcreate -l100%FREE -n lv_apps_local vg_apps
mkfs.ext3 /dev/vg_apps/lv_apps_local


 
Reboot into single user mode (just to ensure no process uses /u01)
OR
Stop all processes using /u01 and run

umount /u01
Don't proceed with the activity if /u01 can't be unmounted, else reboot to single user mode

mount /dev/vg_u01/lv_u01 on /mnt (Assuming no other filesystem is already mounted on /mnt)

cd /mnt

dump 0uf - /dev/sdc1 | restore -rf -

cd /root

umount /mnt

Take a backup of /etc/fstab
cp -vip /etc/fstab /etc/fstab.back.`date +'%Y%m%d'`
Replace the following line in /etc/fstab:
Old line:
/dev/sdc1                       /u01               ext3    defaults   1 2
New line:
/dev/vg_u01/lv_u01         /u01              ext3    defaults    1 2
Reboot into normal runlevel
Or run:
mount /u01
Validate the new filesystem in "df -h" and handover the server to requestor/Server Contact/ASM

********************************
STEPS TO INCREASE LVM BASED FS


Added a disk (/dev/sdf) of 57 GB (Thick provisioning eager zeroed) :
fdisk /dev/sdf
pvcreate /dev/sdf1
vgextend vg_apps /dev/sdf1
lvextend -L +57G /dev/mapper/vg_apps-lv_splunk
resize2fs /dev/mapper/vg_apps-lv_splunk

(or) instead of above 2 steps durectly:
lvextend -r -L +57G /dev/mapper/vg_apps-lv_splunk

********************************


LV REDUCE : (first we must shrink the filesystem before we reduce the logical volume's size)
(BEFORE resize2fs NEED TO RUN e2fsck)
umount
e2fsck
resize2fs
lvreduce
mount

ie:
umount /var/share
e2fsck -f /dev/fileserver/share
resize2fs /dev/fileserver/share 2G
lvreduce -L 2G /dev/fileserver/share
mount /dev/fileserver/share /var/share
*****************

Adding A Hard Drive And Removing Another One :
fdisk /dev/sdf
pvcreate /dev/sdf1
vgextend fileserver /dev/sdf1 (Add /dev/sdf1 to our fileserver volume group)
pvmove /dev/sdb1 /dev/sdf1
pvremove /dev/sdb1


****************************

Reduce "root" from 8.6G to 4G:
https://www.youtube.com/watch?v=sOrrbURtOtc

/ is initially 8.6G
no free space in resp VG
verify fdisk -l
rescue mode - linux rescue
make sure root LV is not mounted (df -h)
vgchange -ay (availability is yes)
e2fsck -f /dev/VolGroup00/Logvol00
resize2fs -f /dev/VolGroup00/LogVol00 4G
lvreduce -L 4G /dev/VolGroup00/LogVol00
reboot and verify once system is up ---> / is now 4G and we have free VG of 4.88G


****************************

Reduce root partition in lvm (remove 1 disk):
https://rbgeek.wordpress.com/2013/02/11/how-to-reduce-the-root-partition-in-lvm/

Two hard drives (66GB & 25GB). Due to some reasons, I want to remove the 25GB hard drive

In addition to above steps add the below 3 steps:
vgreduce vg_centos6 /dev/sdb1
pvremove /dev/sdb1
df -h

*****************************


13.5. Removing an Old Disk

Say you have an old IDE drive on /dev/hdb. You want to remove that old disk but a lot of files are on it.

Caution	Backup Your System
 	

You should always backup your system before attempting a pvmove operation.
13.5.1. Distributing Old Extents to Existing Disks in Volume Group

If you have enough free extents on the other disks in the volume group, you have it easy. Simply run


# pvmove /dev/hdb
pvmove -- moving physical extents in active volume group "dev"
pvmove -- WARNING: moving of active logical volumes may cause data loss!
pvmove -- do you want to continue? [y/n] y
pvmove -- 249 extents of physical volume "/dev/hdb" successfully moved
          

This will move the allocated physical extents from /dev/hdb onto the rest of the disks in the volume group.

Note	pvmove is Slow
 	

Be aware that pvmove is quite slow as it has to copy the contents of a disk block by block to one or more disks. If you want more steady status reports from pvmove, use the -v flag.
13.5.1.1. Remove the unused disk

We can now remove the old IDE disk from the volume group.


# vgreduce dev /dev/hdb
vgreduce -- doing automatic backup of volume group "dev"
vgreduce -- volume group "dev" successfully reduced by physical volume:
vgreduce -- /dev/hdb
            

""""The drive can now be either physically removed when the machine is next powered down or reallocated to other users"""".


13.5.2. Distributing Old Extents to a New Replacement Disk

If you do not have enough free physical extents to distribute the old physical extents to, you will have to add a disk to the volume group and move the extents to it.
13.5.2.1. Prepare the disk

First, you need to pvcreate the new disk to make it available to LVM. In this recipe we show that you don't need to partition a disk to be able to use it.


# pvcreate /dev/sdf
pvcreate -- physical volume "/dev/sdf" successfully created
            

13.5.2.2. Add it to the volume group

As developers use a lot of disk space this is a good volume group to add it into.


# vgextend dev /dev/sdf
vgextend -- INFO: maximum logical volume size is 255.99 Gigabyte
vgextend -- doing automatic backup of volume group "dev"
vgextend -- volume group "dev" successfully extended
            

13.5.2.3. Move the data

Next we move the data from the old disk onto the new one. Note that it is not necessary to unmount the file system before doing this. Although it is *highly* recommended that you do a full backup before attempting this operation in case of a power outage or some other problem that may interrupt it. The pvmove command can take a considerable amount of time to complete and it also exacts a performance hit on the two volumes so, although it isn't necessary, it is advisable to do this when the volumes are not too busy.


# pvmove /dev/hdb /dev/sdf
pvmove -- moving physical extents in active volume group "dev"
pvmove -- WARNING: moving of active logical volumes may cause data loss!
pvmove -- do you want to continue? [y/n] y
pvmove -- 249 extents of physical volume "/dev/hdb" successfully moved
            

13.5.2.4. Remove the unused disk

We can now remove the old IDE disk from the volume group.


# vgreduce dev /dev/hdb
vgreduce -- doing automatic backup of volume group "dev"
vgreduce -- volume group "dev" successfully reduced by physical volume:
vgreduce -- /dev/hdb
            

The drive can now be either physically removed when the machine is next powered down or reallocated to some other users. 

***************************************


NIC Bonding: (Bind both NIC so that it works as a single device)

Bonding is nothing but Linux kernel feature that allows to aggregate multiple like interfaces (such as eth0, eth1) into a single virtual link such as bond0

======> get higher data rates and as well as link failover (Redundancy) <======

Before Clustering softwares come in to existing this technique is widely used to provide redundancy to a high end server. After cluster software introduced its used to enhance the cluster redundancy.


Yes , we can bond more than 2 NIC, here is no limit but linux server do have limit to have total number of NIC cards.



Step #1: Create a Bond0 Configuration File (Red hat) :

First, you need to create a bond0 config file as follows:
 vi /etc/sysconfig/network-scripts/ifcfg-bond0

DEVICE=bond0
IPADDR=192.168.1.20
NETWORK=192.168.1.0
NETMASK=255.255.255.0
USERCTL=no
BOOTPROTO=none
ONBOOT=yes


Step #2: Modify eth0 and eth1 config files


 vi /etc/sysconfig/network-scripts/ifcfg-eth0

Modify/append directive as follows:
DEVICE=eth0
USERCTL=no
ONBOOT=yes
MASTER=bond0
SLAVE=yes
BOOTPROTO=none

vi /etc/sysconfig/network-scripts/ifcfg-eth1

Make sure file read as follows for eth1 interface:
DEVICE=eth1
USERCTL=no
ONBOOT=yes
MASTER=bond0
SLAVE=yes
BOOTPROTO=none


Step # 3: Load bond driver/module

Make sure bonding module is loaded when the channel-bonding interface (bond0) is brought up. You need to modify kernel modules configuration file:


 vi /etc/modprobe.conf

Append following two lines:
alias bond0 bonding                       (to load the bonding module in to kernel)
options bond0 mode=1 miimon=100 (miimon is nothing but to MONitor with MIItool to                                            check the availability of other interfaces,,,mode                                            is to specify whether the bond is configured as load balancing or fail over. Here its load-balancing which means data transmission will be shared between two interfaces. To know more about modes click here)


Step # 4: Test configuration

First, load the bonding module, enter:
# modprobe bonding

Restart the networking service in order to bring up bond0 interface, enter:
# service network restart


Make sure everything is working. Type the following cat command to query the current status of Linux kernel bounding driver, enter:
# cat /proc/net/bonding/bond0


***************************


Device mapper multipathing:

More than 1 path to reach to the shared storage, multiple paths to connect to the same Lun,



1) hba faulty, fibre cable faulty ( no conn with SAN)

iscsi target to the iscsi initiator using 3 networks in our env ( nic, cable fails exported lun will still be accessible thru other channels)

Device Mapper Multipathing (DM-Multipath) allows you to configure multiple I/O paths between server nodes and storage arrays into a single device. These I/O paths are physical SAN connections that can include separate cables, switches, and controllers. Multipathing aggregates the I/O paths, creating a new device that consists of the aggregated paths. 

https://www.centos.org/docs/5/html/5.1/DM_Multipath/MPIO_description.html

Basic procedure for multipath setup :
1) Install device-mapper multipath rpm
2)edit multipath.conf conf file:
comment out the default blacklist
change any of the existing defaults as needed
save the conf file
3)start the multipath daemons
4) Create the multipath device with the "multipath" command

The multipath conf file is divided into the following sections:
1)blacklist: list of specific devices that will notbe considered for multipath
2)blacklisted_exceptions
3)defaults: general default settings for DM-Multipath
multipaths: settings for the characteristics of individual multipath devices
devices: settings for individual storage controllers


Componenets of dm-multipath:

1)dm-multipath
2)multipath : started with /etc/rc.sysinit
3)multipathd:provides interactive changed to multipath devices
4)kpartx : creates device mapper devices for the partitions on a device


**************************
multipath:

http://www.learnitguide.net/2016/06/how-to-configure-multipathing-in-linux.html

More than 1 path to reach to the shared storage, multiple paths to connect to the same Lun,


Device Mapper Multipathing (DM-Multipath) is a native multipathing in Linux, Device Mapper Multipathing (DM-Multipath) can be used for Redundancy and to Improve the Performance. It aggregates or combines the multiple I/O paths between Servers and Storage, so it creates a single device at the OS Level.

For example, Lets say a server with two HBA card (with single ports on each HBA cards) attached to a storage controller. One lun assigned to the single server via two wwn number of both cards. So OS detects two devices: /dev/sdb and /dev/sdc. Once we installed the Device Mapper Multipathing. DM-Multipath creates a single device with a unique WWID that reroutes I/O to those four underlying devices according to the multipath configuration. So when there is a failure with any of this I/O paths, Data can be accessible using the available I/O Path.

fdisk -l|grep Disk
cat /proc/scsi/scsi (verify)


1. Install the  Device Mapper Multipath package.
Verify the device-mapper-multipath package has been installed or not.

    [root@linux1 ~]# rpm -q device-mapper-multipath

If it is not installed, Install the Device Mapper Multipath package using yum to avoid dependencies issue. if yum is not configured, please refer the link Yum Configuration on Linux

    [root@linux1 ~]# yum -y install device-mapper-multipath

2. Basic Configuration of Linux Device Mapper Multipathing
Configuration file is /etc/multipath.conf file, take a backup of it. Edit the configuration file to ensure you have the following entries uncommented out.

    defaults {
    user_friendly_names yes
    }
    blacklist {
    devnode “sda”
    }

The blacklist includes the devices which are not to be configured in Multipathing. For example, Lets say our OS installed disk is /dev/sda. So the first entry in the blacklist will exclude them. Same for IDE drives (hd).

3. Start and Enable the multipath daemons.
Start the multipath service if not started bydefault.

    [root@linux1 ~]# systemctl start multipathd

Enable the multipath service to start on boot.

    [root@linux1 ~]# systemctl enable multipathd

4. Check the multipathing status.
The multipath -ll command prints out multipathed paths that show which devices are multipathed. If the command does not print anything out, ensure that all SAN connections are set up properly and the system is multipathed.

    [root@linux1 ~]# multipath -ll
    mpathb (360014051f89d2bb3300470fa7d4baa10) dm-2 LIO-ORG ,lun0
    size=2.0G features='0' hwhandler='0' wp=rw
    |-+- policy='service-time 0' prio=0 status=active
    | `- 1:0:0:0 sdb 8:16 active active running
    `-+- policy='service-time 0' prio=0 status=enabled
      `- 2:0:0:0 sdc 8:32 active active running

The above output shows 1 LUN (mpathb) with 2 paths (sdb and sdc). Linux Multipathing basic configuration is done.

5. fdisk -l

*************************

1)Recover root password (if grub menu not displayed):


As unable to go to grub screen , mounted live CD and at prompt :
live
sudo su -

I checked fdisk|more and saw only one disk /dev/sda with one partition : /dev/sda1
mount /dev/sda1 /mnt
df -h
chroot /mnt
cat /etc/passwd 
check users that are created and make sure able to login and set passwd using passwd 
exit from chroot, umount /mnt disconnect iso then reboot and make sure you are able to login



2) Recover root password (if grub menu IS displayed)


When booting up press SHIFT (in systems 9.10 "karmic" or later) or ESC (in systems 9.04 "jaunty" or earlier) at the grub prompt and use the arrow keys to select the rescue mode option (single user mode) and press enter.
The file system may be read only (it is in all currently supported releases). Remount as read write

    mount -rw -o remount /

This will boot the system in rescue mode and you should arrive at a prompt that looks like this

    root@something

To reset your password type this in

    passwd <username>

    Press return, then you will be prompted to enter and confirm a new password. 

Once you are done resetting your password you can than switch back to the normal GUI mode by putting this in

    init 2


3)  If the "Standard Way" does not work for you and you recieve the "Give root password for maintenance" message, you can recover your password using the following steps

1. Reboot your computer

2. Press SHIFT or ESC at the grub prompt (as earlier).

3. Select your image.

4. Highlight the line that begins kernel and press 'e' to edit

5. Go to the very end of the line, change the (ro quiet splash)to rw and add (init=/bin/bash)

press enter, then press b to boot your system.

Your system will boot up to a passwordless root shell.

6. Type in passwd username

7. Set your password.

8. Type in reboot


********************************

RAID 0 - Blocks Striped , NO mirroring , No parity

    Minimum 2 disks.
    Excellent performance ( as blocks are striped ).
    No redundancy ( no mirror, no parity ).
    Don’t use this for any critical system.

RAID 1 - Blocks Mirrored , No stripe , No parity


    Minimum 2 disks.
    Good performance ( no striping. no parity ).
    Excellent redundancy ( as blocks are mirrored ).


RAID 5 - Blocks Striped , Distributed parity


    Minimum 3 disks.
    Good performance ( as blocks are striped ).
    Good redundancy ( distributed parity ).
    Best cost effective option providing both performance and redundancy. Use this for DB that is heavily read oriented. Write operations will be slow.


RAID 10 (RAID 1+0) - Blocks Mirrored (and Striped)


    Minimum 4 disks.
    This is also called as “stripe of mirrors”
    Excellent redundancy ( as blocks are mirrored )
    Excellent performance ( as blocks are striped )
    If you can afford the dollar, this is the BEST option for any mission critical applications (especially databases).


RAID 10 Vs RAID 01 (RAID 1+0 Vs RAID 0+1) :



    RAID 10 is also called as RAID 1+0
    It is also called as “stripe of mirrors”
    It requires minimum of 4 disks
    To understand this better, group the disks in pair of two (for mirror). For example, if you have a total of 6 disks in RAID 10, there will be three groups–Group 1, Group 2, Group 3 as shown in the above diagram.
    Within the group, the data is mirrored. In the above example, Disk 1 and Disk 2 belongs to Group 1. The data on Disk 1 will be exactly same as the data on Disk 2. So, block A written on Disk 1 will be mirroed on Disk 2. Block B written on Disk 3 will be mirrored on Disk 4.
    Across the group, the data is striped. i.e Block A is written to Group 1, Block B is written to Group 2, Block C is written to Group 3.
    This is why it is called “stripe of mirrors”. i.e the disks within the group are mirrored. But, the groups themselves are striped.



RAID 01


    RAID 01 is also called as RAID 0+1
    It is also called as “mirror of stripes”
    It requires minimum of 3 disks. But in most cases this will be implemented as minimum of 4 disks.
    To understand this better, create two groups. For example, if you have total of 6 disks, create two groups with 3 disks each as shown below. In the above example, Group 1 has 3 disks and Group 2 has 3 disks.
    Within the group, the data is striped. i.e In the Group 1 which contains three disks, the 1st block will be written to 1st disk, 2nd block to 2nd disk, and the 3rd block to 3rd disk. So, block A is written to Disk 1, block B to Disk 2, block C to Disk 3.
    Across the group, the data is mirrored. i.e The Group 1 and Group 2 will look exactly the same. i.e Disk 1 is mirrored to Disk 4, Disk 2 to Disk 5, Disk 3 to Disk 6.
    This is why it is called “mirror of stripes”. i.e the disks within the groups are striped. But, the groups are mirrored.

Main difference between RAID 10 vs RAID 01 ::::::

    Performance on both RAID 10 and RAID 01 will be the same.
    The storage capacity on these will be the same.
    The main difference is the fault tolerance level. On most implememntations of RAID controllers, RAID 01 fault tolerance is less. On RAID 01, since we have only two groups of RAID 0, if two drives (one in each group) fails, the entire RAID 01 will fail. In the above RAID 01 diagram, if Disk 1 and Disk 4 fails, both the groups will be down. So, the whole RAID 01 will fail.
    RAID 10 fault tolerance is more. On RAID 10, since there are many groups (as the individual group is only two disks), even if three disks fails (one in each group), the RAID 10 is still functional. In the above RAID 10 example, even if Disk 1, Disk 3, Disk 5 fails, the RAID 10 will still be functional.
    So, given a choice between RAID 10 and RAID 01, always choose RAID 10


******************************************

Linux Boot up process:


Bios
Mbr
Grub /boot/grub/grub.conf (/etc/grub.conf is a link to this,just loads and executes Kernel and initrd images)
Kernel
init 
runlevel


**********************************

vmware virtualisation : power consumption , cooling , less servers

Virtualisation COnverts h/w devices into s/w resources,, can seprate servers OS and apps from the underlying h/w by presenting virtual h/w that does the same job.

virtual h/w is actually just s/w running within something called hypervisor

hypervisor - resource traffic cop, job is to provide each vm with access to undelying physical resources

need for datacenter virtualisation:
Availability - minimise downtime caused by hardware failures, perform h/w maintainance during business hrs, h/w independent soln to replicate servers to another site.
Scalability - shared storage infrastructure not needed, add cpus and meory without downtime
optimisation - There are very large disk drives but most of the space is empty, need to make use of this space efficiently.
management


vm:

Virtual Hardware is really software
replace and upgrade h/w on the fly 
Add new h/w devices (network cards and processors) without rebooting the vm.
-----> hence reduce downtime


vsphere:
Using vsphere web client we can remotely control the vm,install s/w,cloning and hotspots

Vms can be used to host any kind of servers : File servers;db servers;email servers;app servers;


1)SNapshot:capture entire runnig state of A vm - CONTEST OF harddisks STATE OF cpu and memory 

In case of a crash using snapshot we can revert to original state.

TIMELINE --- SNAPSHOT --- PATCH INSTALL --- CRASH --- REVERT
                |                                        |
                |----------------------------------------| 


2)Image based Backup (bare metal restores)
3)hot add memory

Hypervisor :

Resource mgmt for vms
provides vms with virtual hardware.


2 Types of hypervisor:

a)Type1 :Baremetal: (Installed on the physical device as the OS) (eg: VMware ESXi)

Installed on a physical device without an os,performs functions of os as well as resource management features 

Benefits of ESXi: 
Not dependent on another os
Direct Hardware Access
Less overhead than hosted hypervisors
Flexible Installation Options 
nstalled directly on the harddrive,flash drive,sd card ,usb drives of the OS also can n/w boot

ESXi hosts common tasks::
create vms
Adjust VM Configurations
Monitoring Performance
Configuring and patching hosts


---> with only a single ESXi hosts, mgmt options are limited.
vCenter: to take advantage of the advance features of virtualisation, we need a central mgmt soln that can manage all the ESXi hosts in a single view,, for this we need a ----> Vcenter.

Vcenter:
Vcenter is scalable
Vcenter : 1 0r 1000 ESXi hosts can be managed,even 10000 Vms
Vcenter Components: Identify Mgmt server,db server,app server,web server,Vmware Vsphere@Web Client

Vcenter is a centralised platform for management features:
1)vMotion : MIgrate vms from one ESXi  host to another 
2)DRS: Automates vMotion,Distributed Resource Schedular:Load balancing for Vms across ESXi hosts.DRS leverages  vMotion to balance these workloads.monitors memory and cpu load on ESXi hosts , gather info on performance and hence we can make better decisions on where to place a VM.when we power on the VMs, DRS can tell us which hosts are better suited for the new workload, DRS maintains rules that match your business objectives,Defines that a particular VM  must run on a specific ESXi host.
Turn over control to DRS -- if one host becomes overloaded ,drs can step in and migrate vms to other ESXi host.
3)DPM (DIstributed Power Mgmt) Vms in demand, Cpus nearly Idle,but ESXi host remain at full power,hence power off unused ESXI hosts (may be during weekends when not in use); if threshold are met DPm will activate and begin consolidating Vms;then power off that hosts
To power back host again can use : A) wake-on-Lan Methods B) IPMI COntrols.
4)Storage vMotion : Migrate a running vm's Harddisk from one storage device to another
move a vm from a shared storage device to a local storage device on a ESXi host.
5)Storage DRS : Automates Storage vMotion ,Automates load balancing from storage perspective.
6)Storage I/O Control: Few production servers might use more storage and makes other vms slow, using storage i/o control this problem is addressed.Controls busy VMs so that they leave little room for the other VMs and still get appropriate amount of storage performance, this keeps all vms performing at normal level and ther will be no performance issues.
7)vSphere Data Protection (VDP): backup vms(Easier to make an image based backup of a VM,Then easily restore files from that image-based backup.)
VMware released an open set of APIs known as VADP that allows 3rd party backup vendors to create these image based backups of these VMs
8)HA : HA to restore vm to another host in case of a h/w problem (Each VM has a special set of software & drivers,HA can easily restart the VM for you,HA can also monitor services and applications)
9)FT : for server that simply cannot be offline ,uninterrupted availability for vms, easy to configure
Mirror your running vm to another ESXihost, if 1 ESXi host crashes,it continues to run on the another, kind of active-passive clustering, its transparent to ur app.
10)vSphere Replication (VR): Copy vms to another site like DR site or just another office building,for DR purposes.

----> Vmotion can option without shared storage (if the ESXi hosts do not have shared storage)

Shared Storage:
Visible to multiple ESXi hosts.
typical used to store VMs and ISO files


Features that use shared storage:

DRS
DPM
storage Drs
HA
FT


Storage types:
FIbre Channel
Fibre channel over Ethernet (FCoE)
iSCSI
NFS
Local

VSA: vSphere storage appliance: saves money, without the cost and mgmt of a full storage array, Its a componenet of vSphere that allows u to create  shared storage from the local storage devices in your Esxi hosts, provides mgmt and control to share these local storage devices so that u can take adv of the features like DRS, HA and FT, U designate disks within your ESXi hosts and VSA takes care of the rest


Vflash:(opeartes as read cache)
A pool of SSD in ESXi hosts.Leverage internal SSD drives to accelarate read performance of vm
Improves VMs disk read performance while reducing loads on the SAN.
Install atleast one SSD drive in your ESXi host and configure the vm to use the vflash storage.

Datastores: VMs are stored in containers called as Datastores, LVs that allow u to organise the storage of ESXi hosts and VM.
ex: drive to store VM files and ISo images
2 types:
VMFS DS are built on: local, ISCSI, FC storage
NFS Ds built from connections to NAS devices 


Virtual networking:
each vm in ESXihost has an address in the virtual n/w card, those virtual n/w cards are connected to virtual ethernet switches, (no need of virtual cables)
For each vm we cannot have n/w cards individually so ,, Create virtual network ethernet switch and software

VMware took the programming and logic of a physical ethernet switch and turned into a virtual device
its very similar to how a vm is built out of virtual motherboards,n/w cards and storage controllers,,, its just s/w that behaves like h/w 


Use virtual switches to attach vms to physical n/w or can create isolated networks to be used during testing and development
There are different types of virtual switches, we can use the ones needed as per business  requirement.

Virtual switch types (reduces n/w cables plugged in ESXi hosts):
a)Standard virtual switch architecture (Manages vm and networking at the host level)
b)distributed virtual switch architecture (Manages vm and networking at the Data Center level)

Reduces no of cables plugged into the ESXi hosts
Each ESXi host comes prebuilt with a standard switch that provides basic connectivity and mgmt features 


Netorking Features:
Vlans: logical separation of network traffic and often used to isolate diff sub networks such as a test or restore n/w.
Traffic Shaping : restricts the inbound and outbound n/w bandwidth of a group of vms ,hence reduce congestion in virtual n/w.
Port Mirroring : ability to monitor a VM's traffic for troubleshooting or intrusion prvention, allows to capture all traffic sent to/from a vm for later inspection. 
QoS,DSCP (Quality of service): N/w standards that all N/w switches to prioritise certain n/w traffic over others 
CDP/LLDP : Discovery protocols used to identify neighbouring physical n/w switches, can be used to help discover and troubleshoot misconfiguration

vCenter opeartions manager: monitor trends and avoids....
vCenter configuration manager : change discovery and correlation , configuration analysis
vCenter site recovery manager (srm): Automate and test the execution of a DR plan , restore servers and make sure all servers and apps are working 
Its job is to make ur DR failover and testing simple and painfree, simple automated recovery plans, captures recovery process in simple centralised manner that gives peace of mind during testing and recovery


b)Type2 :Hosted : (Installed as an App) (eg: Vmware workstation)

Operate as an app on top of a preexisting os, this separation of role can be  helpful 
if the underlying hardware is not supported by its type1 hypervisor
eg: VMware workstation.


***************************


1. Soft Mount

Suppose you have mounted a NFS filesystem using “soft mount” . When a program or application requests a file from the NFS filesystem, NFS client daemons will try to retrieve the data from the NFS server. But, if it doesn’t get any response from the NFS server (due to any crash or failure of NFS server), the NFS client will report an error to the process on the client machine requesting the file access. The advantage of this mechanism is “fast responsiveness” as it doesn’t wait for the NFS server to respond. But, the main disadvantage of this method is data corruption or loss of data. So, this is not a recommended option to use.
(A  so-called  "soft"  timeout can cause silent data
      corruption in certain  cases.  As  such,  use  the  soft
      option only when client responsiveness is more important
      than data integrity)

mount -o rw,soft host.nf_server.com/home /techhome
.
2. Hard Mount

If you have mounted the NFS filesystem using hard mount, it will repeatedly retry to contact the server. Once the server is back online the program will continue to execute undisturbed from the state where it was during server crash. We can use the mount option “intr” which allows NFS requests to be interrupted if the server goes down or cannot be reached. Hence the recommended settings are hard and intr options.

mount -o rw,hard,intr host.nf_server.com/home /techhome


************************

Difference Between Soft Link And Hard Link :

Hard Link acts like a mirror copy of the original file. These links share the same inodes. Changes made to the original or hard linked file will reflect the other. When you delete hard link nothing will happen to the other. Hard links can't cross file systems.


Soft Link is actual link to the original file. These Links will have a different Inodes value. Soft link points to the original file so If original file is deleted the soft link fails. If you delete the soft link, nothing will happen to file. The reason for this is, the actual file or directory’s inode is different from the "soft link" created file's inode, Hard links can cross file systems.

What are Hard Links

1. Hard Links have same inodes number.
 2. ls -l command shows all the links with the link column shows number of links.
 3. Links have actual file contents
 4. Removing any link, just reduces the link count, but doesn't affect other links.
 5. You cannot create a hard link for a directory.
 6 If original file is removed then the link will still show you the content of the file.

What are Soft Links

1. Soft Links have different inodes numbers.
 2. ls -l command shows all links with second column value 1 and the link points to original file.
 3. Soft Link contains the path for original file and not the contents.
 4. Removing soft link doesn't affect anything but removing original file, the link becomes "dangling" link which points to nonexistent file.
 5. A soft link can link to a directory.

Let us try to see some experimental differences.Make a new directory called Test and then move into it and create new file. Simply follow below steps.

Hard links


$ mkdir Test
 $ cd Test
 $ touch sample1
.
Now, create a hard link to sample1. Name the hard link sample2.


$ ln sample1 sample2
.
Display inodes for both files using ‘I’ argument of the ls command.


$ ls -il sample1 sample2
.
This is what you get:


1482256 -rw-r--r-- 2 bruno bruno 21 May 5 15:55 sample1
 1482256 -rw-r--r-- 2 bruno bruno 21 May 5 15:55 sample2
.
From the output you can notice that both sample1 and sample2 have the same inode number (1482256). Also both files have the same file permissions and the same size.

Now Remove the original sample1


$ rm sample1
.
After removing hard link just have a look at the content of the "link" sample2.


$ cat sample2
.
You will still be able to see the content of the file.

Symbolic links

Create soft link for the file sample2.


$ ln -s sample2 sample3
.
Display inodes for both using i argument of ls command.


$ ls -il sample2 sample3
.
This is what you'll get:


1482256 -rw-r--r-- 1 bruno bruno 21 May 5 15:55 FileB
 1482226 lrwxrwxrwx 1 bruno bruno 5 May 5 16:22 FileC -> FileB
.
From the output you can notice that the inodes are different and the symbolic link got a "l" before the rwxrwxrwx. The permissions are different for the link and the original file because it is just a symbolic link.

Now list the contents:


$ cat sample2
 $ cat sample3
.
Now remove the original file:


$ rm sample2
.
And then check the Test directory:


$ ls
.
It will still display symbolic link sample3 but if you try to list the contents It will tell you that there is no such file or directory.


$ cat sample3
.
Now you know about some of the key differences between hard links and soft links to make it easier to access files and run programs.


When to use Soft Link: 1.Link across filesystems: If you want to link files across the filesystems, you can only use symlinks/soft links.
2.Links to directory: If you want to link directories, then you must be using Soft links, as you can’t create a hard link to a directory.
When to use Hard Link: 
1.Storage Space: Hard links takes very negligible amount of space, as there are no new inodes created while creating hard links. In soft links we create a file which consumes space (usually 4KB, depending upon the filesystem)
2.Performance: Performance will be slightly better while accessing a hard link, as you are directly accessing the disk pointer instead of going through another file.
3.Moving file location: If you move the source file to some other location on the same filesystem, the hard link will still work, but soft link will fail.
4.Redundancy: If you want to make sure safety of your data, you should be using hard link, as in hard link, the data is safe, until all the links to the files are deleted, instead of that in soft link, you will lose the data if the master instance of the file is deleted.

The above points gives you a small idea where to use what, but doesn’t tell you that those are the only options. Everything depends on your setup.

How file is deleted having hard links:

So, as it’s pretty clear from the above article that hard links are just the reference to the main file location, and even if you delete one link, the data will still be intact. So, to remove a hard link, you need to remove all the links, which are referring to the file. Once the “link count” goes to “0”, then the inode is removed by the filesystem, and file is deleted.

**************

df -h hung :


Whenever there are network changes or hiccups on systems with NFS mounts the NFS mounts itself will get hung so when you are doing df it will never complete and your session will get hung also.

A easy solution to resolve the hung mounts is to look at /etc/mtab and look for nfs entries and identify which ones are having the problem and remove those entries from /etc/mtab.  Take note of the ones thats removed and do a umount -l <mountpoint> and after you got all of the hung nfs mounts you should be able to do df again.

We will need to troubleshoot a bit before remounting the nfs share.  run rpcinfo -p <remotehost> and look up all of the ports thats required for NFS to work.  You will need portmapper/nlockmgr/nfs/mountd make sure that you are able to reach each of these ports on the remote host before attempting to remount the nfs share.

After the ports have been verified you can safely remount your share and it should mount without problems unless you have a problem on the NFS server side with permissions or access list.


steps:
====
Step 1:
=========
Unmount in client side (manual)


Step 2:
====
troubleshoot a bit before remounting the nfs share.  run rpcinfo -p <remotehost> and look up all of the ports thats required for NFS to work.  You will need portmapper/nlockmgr/nfs/mountd make sure that you are able to reach each of these ports on the remote host before attempting to remount the nfs share
mount manually (in client) and restart network and check 

ie: rpcinfo -p <remotehost>
mount /mountpoint
restart n/w


Step 3: (in server) (server side troubleshoot):
====
Check if the directory is full

check in protocol entry (nfsv3,nfsv4)

permission issue

ACLS check


**************
GPT:

GUID Partition Table (GPT) is a partitioning scheme that is part of the Unified Extensible Firmware Interface specification; it uses a globally unique identifier for qualifying devices. It is the next generation partitioning scheme designed to succeed the Master Boot Record partitioning scheme method. It evolved to deal with several shortcomings of the MBR partitioning scheme method and offers additional advantages. 

Parted:

https://www.youtube.com/watch?v=GmAM7XroncA

parted /dev/sdb
print
mklabel msdos
mkpart
primary
ext4
start 1 
end  10000
print
quit
mkfs.ext4 /dev/sdb1
parted /dev/sdb
set 1 boot on (to set boot flag)
rm 1
print
rescue 
start 1 
end 10000
yes (recover parttion)

**********

Kdump:

1) install kdump-service:
yum install kexec-tools



Configuring the kdump Service:

There are 3 common means of configuring the kdump service: 
1)at the first boot
2)using the Kernel Dump Configuration graphical utility
3)doing so manually on the command line.



A) Configuring kdump at First Boot

When the system boots for the first time, the firstboot application is launched to guide the user through the initial configuration of the freshly installed system. To configure kdump, navigate to the Kdump section and follow the instructions below. 

?
1. Select the Enable kdump? check box to allow the kdump daemon to start at boot time. This will enable the service for runlevels 2, 3, 4, and 5, and start it for the current session. Similarly, unselecting the check box will disable it for all runlevels and stop the service immediately. 


2. Click the up and down arrow buttons next to the Kdump Memory field to increase or decrease the value to configure the amount of memory that is reserved for the kdump kernel. Notice that the Usable System Memory field changes accordingly showing you the remaining memory that will be available to the system. 

B)Using the Kernel Dump Configuration Utility :

type ----> system-config-kdump

The utility allows you to configure kdump as well as to enable or disable starting the service at boot time. When you are done, click Apply to save the changes. The system reboot will be requested


?Enabling the Service

To start the kdump daemon at boot time, click the Enable button on the toolbar. This will enable the service for runlevels 2, 3, 4, and 5, and start it for the current session
Basic Settings: - kdump memory
Target Settings: - target location for the vmcore dump (file in a local FS (/var/crash),written directly to hard drive(/dev/sda),sent over n/w using NFS and SSH)
Filtering Settings: tick - zero page, free page
Expert Settings:choose which kernel and initial RAM disk to use

C)Configuring kdump on the Command Line


Memory reserved for the kdump kernel is always reserved during system boot, which means that the amount of memory is specified in the system's boot loader configuration

To configure the amount of memory to be reserved for the kdump kernel, edit the /boot/grub/grub.conf file and add crashkernel=<size>M or crashkernel=auto

ie :

kernel /vmlinuz-2.6.32-220.el6.x86_64 ro root=/dev/sda3 crashkernel=128M


Configuring the Target Type:

the default option is to store the vmcore file in the /var/crash/ directory of the local file system. To change this, as root, open the /etc/kdump.conf configuration file in a text editor and edit the options as described below:


To change the local directory in which the core dump is to be saved, remove the hash sign (“#”) from the beginning of the #path /var/crash line, and replace the value with a desired directory path. Optionally, if you want to write the file to a different partition, follow the same procedure with the #ext4 /dev/sda3 line as well, and change both the file system type and the device (a device name, a file system label, and UUID are all supported) accordingly. For example

ext3 /dev/sda4
path /usr/local/cores

i)To write the dump directly to a device, remove the hash sign (“#”) from the beginning of the #raw /dev/sda5 line, and replace the value with a desired device name. For example: 
raw /dev/sdb1

ii)To store the dump to a remote machine using the NFS protocol, remove the hash sign (“#”) from the beginning of the #net my.server.com:/export/tmp line, and replace the value with a valid host name and directory path. For example: 
net penguin.example.com:/export/cores

iii)To store the dump to a remote machine using the SSH protocol, remove the hash sign (“#”) from the beginning of the #net user@my.server.com line, and replace the value with a valid user name and host name. For example: 
net john@penguin.example.com



Configuring the Core Collector

To reduce the size of the vmcore dump file, kdump allows you to specify an external application (that is, a core collector) to compress the data, and optionally leave out all irrelevant information. Currently, the only fully supported core collector is makedumpfile. 

To enable the core collector, as root, open the /etc/kdump.conf configuration file in a text editor, remove the hash sign (“#”) and to enable the dump file compression :core_collector makedumpfile -c



To remove certain pages from the dump, add the -d value parameter, where value is a sum of values of pages you want to omit as described in Table 31.2, “Supported filtering levels”. For example, to remove both zero and free pages, use the following: 
core_collector makedumpfile -d 17 -c

See the manual page for makedumpfile for a complete list of available options. 

?
Table 31.2. Supported filtering levels




Option 

Description 


1  Zero pages  
2  Cache pages  
4  Cache private  
8  User pages  
16  Free pages  


Changing the Default Action

By default, when kdump fails to create a core dump, the root file system is mounted and /sbin/init is run. To change this behavior, as root, open the /etc/kdump.conf configuration file in a text editor, remove the hash sign (“#”) from the beginning of the #default shell line, and replace the value with a desired action as described in Table 31.3, “Supported actions”. 

?
Table 31.3. Supported actions






Option 

Description 


reboot  Reboot the system, losing the core in the process.  
halt  Halt the system.  
poweroff  Power off the system.  
shell  Run the msh session from within the initramfs, allowing a user to record the core manually.  

For example: 
default halt

?Enabling the Service

To start the kdump daemon at boot time, type the following at a shell prompt as root: 
chkconfig kdump on

This will enable the service for runlevels 2, 3, 4, and 5. Similarly, typing chkconfig kdump off will disable it for all runlevels. To start the service in the current session, use the following command as root: 
service kdump start


--------> Testing the Configuration :

service kdump status
Kdump is operational

Then type the following commands at a shell prompt: 
echo 1 > /proc/sys/kernel/sysrq
echo c > /proc/sysrq-trigger

This will force the Linux kernel to crash, and the address-YYYY-MM-DD-HH:MM:SS/vmcore file will be copied to the location you have selected in the configuration (that is, to /var/crash/ by default). 

---------> Analyse the coredump:

To determine the cause of the system crash, you can use the crash utility, which provides an interactive prompt very similar to the GNU Debugger (GDB). This utility allows you to interactively analyze a running Linux system as well as a core dump created by netdump, diskdump, xendump, or kdump. 


To analyze the vmcore dump file, you must have the crash and kernel-debuginfo packages installed:
yum install crash
debuginfo-install kernel



----------> Running the crash Utility:

locate vmlinux
crash /usr/lib/debug/lib/modules/kernel/vmlinux /var/crash/timestamp/vmcore



----------> Displaying the Kernel Message Buffer:

crash> log



-----------> Displaying the kernel stack trace:
crash> bt


------------> Displaying status of processes in the system

crash> ps


------------> Displaying virtual memory information of the current context :

crash> vm

------------> Displaying information about open files of the current context :

crash> files

------------> Exiting the crash utility :
crash> exit



**************

------------
31.4. Additional Crash Dump Methods

Starting with Red Hat Enterprise Linux 6.8 an alternative dumping mechanism to kdump, the firmware-assisted dump (fadump), is available. The fadump feature is supported only on IBM Power Systems. The goal of fadump is to enable the dump of a crashed system, and to do so from a fully-reset system, and to minimize the total elapsed time until the system is back in production use. The fadump feature is integrated with kdump infrastructure present in the user space to seemlessly switch between kdump and fadump mechanisms.
------------



upload logs to redhat :

https://access.redhat.com/solutions/2112


**************

Configure SMTP Postfix mail:

update /etc/hosts :
192.168.5.159 mail.linuxhelp.com mail
::1  mail.linuxhelp.com mail     

yum install postfix
systemctl start postfix
systemctl enable postfix
firewall-cmd --permanent --add-service=smtp
vi /etc/postfix/main.cf
75th line : myhostname
83rd line : uncomment mydomain
99th line : uncomment myorigin
113 line: inet interfaces=all (uncomment)
164: mydestination
systemctl restart postfix
systemctl status postfix ----> should be active (running)

verify using following :

journalctl -xn (journallog)
postfix -n  (conf file)
mail -s "hi" user1@linuxhelp.com (s=subject)
Subject:invite
testing 
. (then hit enter to close mail)
su - user1
mail
vi /var/mail/user1 (check)


********************************

Samba:


Samba consists of two key programs, plus a bunch of other stuff that we'll get to later. The two key programs are smbd and nmbd. Their job is to implement the four basic modern-day CIFS services, which are: 
•File & print services 
•Authentication and Authorization 
•Name resolution 
•Service announcement (browsing)



Linux Server running samba, configured for file and printer sharing
1)Share files and printers with MS clients
2)Act as windows domain controller (auth)


Samba conf files:
smb.conf - conf file
smbpasswd - synchronises samba passwords with linux passwords



Global options:

1)Network Options
2)Logging Options
3)Standalone Server Options
4)Domain Members Options
5)Domain Controllers Options
6)Browser Control Options
7)Name Resolution Options
8)Printing Options
9)FS Options

Share Definitions:

- Default Shares
1)Homes
2)Printers
3)Netlogon
4)Profiles
5)Public

Authentication Mode — This corresponds to the security option. Select one of the following types of authentication.


?ADS — The Samba server acts as a domain member in an Active Directory Domain (ADS) realm. For this option, Kerberos must be installed and configured on the server, and Samba must become a member of the ADS realm using the net utility, which is part of the samba-client package. Refer to the net man page for details. This option does not configure Samba to be an ADS Controller.


?Domain — The Samba server relies on a Windows NT Primary or Backup Domain Controller to verify the user. The server passes the username and password to the Controller and waits for it to return. Specify the NetBIOS name of the Primary or Backup Domain Controller in the Authentication Server field.

The Encrypted Passwords option must be set to Yes if this is selected.


?Server — The Samba server tries to verify the username and password combination by passing them to another Samba server. If it can not, the server tries to verify using the user authentication mode. Specify the NetBIOS name of the other Samba server in the Authentication Server field.


?Share — Samba users do not have to enter a username and password combination on a per Samba server basis. They are not prompted for a username and password until they try to connect to a specific shared directory from a Samba server.


?User — (Default) Samba users must provide a valid username and password on a per Samba server basis. Select this option if you want the Windows Username option to work. Refer to Section 24.2.1.2 Managing Samba Users for details.




Steps to Configure Samba:
Modify /etc/samba/smb.conf  --->
workgroup=soundtraining (This should match the workgroup name on your windows clients) 
server string = inst-centos (This is the name that will appear in the n/w browse list of the windows clients)
hosts allow = 192.168.235. (subnet ip)
[smdbdemo]
comment= samba demo directory
path = /smbdemo
public=yes
writeable=yes

create smb users (Linux and windows user names must be the same):
smbpasswd -a jitesh (add the user)
pdbedit -L (list samba users & allows to work with password db)
(pdbedit -x -u username to delete samba users)
testparm (just for checking of conf of smb.conf)
service smb restart

Now go the windows machine : 192.168.235.135 and check: you will see : 
1)smbdemo (share)
2)jitesh home dir
to test : create another home dir here and go to linux end and su - jitesh and you will see new directory added


***************

https://www.youtube.com/watch?v=Q1EEj6VJaac

Kickstart Installation:

The Red Hat Kickstart installation method[1] is used primarily (but not exclusively) by the Red Hat Enterprise Linux os to automatically perform unattended operating system installation and configuration. Red Hat publishes Cobbler as a tool to automate the Kickstart configuration process.

Kickstart is normally used at sites with many such Linux systems, to allow easy installation[2] and consistent configuration of new computer systems:

To configure kickstart: 
1)can edit the file:anaconda-ks.cfg (default conf file) (or) install package system-config-kickstart 

mount /dev/sr0 /mnt
vi /etc/yum.repos.d :
gpgcheck=0
[ClassRoom]
baseurl=file:///mnt  (as resource available in same machine)


yum install system-config-kickstart -y
system-config-kickstart (create new kickstart conf file - kickstart configurator) --- 
1)Basic Conf (lang,keyb,timezone,type password,reboot after installation) 2)installation method (perform new inst, CD Rom)3)Boot loader options (install new BL)4)partition information(boot,root) 5) N/w conf (eth0,DHCP)
6)auth (nis,ldap,kerberos)7)firewall conf (selinux) 8)display conf 9)package selection 10)preinstallation script 11)postinstallation script (add user)

save the file under - root - ks.cfg
copy packages from anaconda-ks.cfg to ks.cfg
yum install vsftd -y 
cp ks.cfg /var/ftp/pub/
firewall-cmd --permanent --add-service=ftp (add ftp service in firewall)
firewall-cmd --reload
firewall-cmd --list-all
systemctl start vsftpd
systemctl enable vsftpd (enable at boot time)



boot client machine by DVD - press tab - ks=ftp://192.168.5.88/pub/ks.cfg .................
(machine boots from dvd but fetches all info from ftp server)

**************

Kernel tuning (/etc/sysctl.conf)
view current values:
sysctl -a
to load settings:
sysctl -p
Disable/Enable IP forwarding in Linux :

1. Current IP forwarding status
Read a current state of IP forwarding: # sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1

Currently, the output number 1 indicates that the IP forwarding is enabled. The above value is read from the Linux proc file system and more precisely from the actual file /proc/sys/net/ipv4/ip_forward file: # cat /proc/sys/net/ipv4/ip_forward
1


2. Disable IP forwarding
To disable IP forwarding on a running Linux system run: # sysctl -w net.ipv4.ip_forward=0
net.ipv4.ip_forward = 0

The above command actually writes number 0 into the above mentioned file /proc/sys/net/ipv4/ip_forward. If from some reason the above command fails you can attempt to disable the IP forwarding manually by: echo 0 > /proc/sys/net/ipv4/ip_forward

The above change is not reboot persistent. To permanently disable the IP forwarding on your Linux system edit /etc/sysctl.conf and add the following line: net.ipv4.ip_forward = 0


***************************


kernel upgrade:

1)upgrade kernel firmware (kernel-firmware-2.6.32...)
2)upgrade kernel (.rpm)


ls /lib/modules (will show both kernels)
ls /boot/vmlinuz* (check both linux images)


******************

YUM vs RPM


YUM(Yellow dog updater and modifier) is latest model/application in installing RPM packages on Redhat based machines. This YUM comes in to existence due to drawbacks of RPM package management.

So whats the drawbacks of this RPM package management and what are the advantage of YUM



Sl No.	RPM	YUM
1	If we want to install an application(Ex: apache), rpm need to install all the packages required for this application, these packages may vary from 1 rpm to several rpm’s depending on shared rpm packages.	Install an application with single command
Ex: yum install httpd
2	RPM package dependencies is bit tough	YUM resolves dependencies with ease
3	Batch installation of applications is possible with one command	YUM command can install number of applications in one single command
Ex: yum install httpd vsftpd
4	RPM can not handle updated software installation automatically	Does YUM install updates of the existing packages by using
yum install upgrade
5	Can not connect to online repositories	Can connect to on-line repositories to get latest software before installing the applications


*****************************

NAS Vs SAN :

A NAS is a single storage device that operate on data files, while a SAN is a local network of multiple devices that operate on disk blocks. 

A SAN commonly utilizes Fibre Channel interconnects. A NAS typically makes Ethernet and TCP/IP connections. 

Network Attached Storage (NAS)
Network Attached Storage (NAS) devices are storage arrays or gateways that support file-based storage protocols such as NFS and CiFS, and are typically connected via an IP network. These file-based protocols provide clients shared access to storage resources. This centralization of shared storage resources reduces management complexity, minimizes stranded disk capacity, improves storage utilization rates and eliminates file server sprawl.

NAS vs SAN
The primary difference between NAS and SAN solutions is the type of access protocol. NAS protocols such as NFS and CiFS provide shared file level access to storage resources. The management of the file system resides with the NAS device. SAN protocols such as iSCSI and fibre channel provide block level access to storage resources. Block level devices are accessed by servers via the SAN, and the servers manage the file system.

Despite their differences, SAN and NAS are not mutually exclusive, and may be combined in multi-protocol or unified storage arrays, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. The best of both worlds!

Benefits of NAS


•NAS devices typically leverage existing IP networks for connectivity, enabling companies to reduce the price of entry for access to shared storage.
•The RAID and clustering capabilities inherent to modern enterprise NAS devices offer greatly improved availability when compared with traditional direct attached storage.
•Because NAS devices control the file system, they offer increased flexibility when using advanced storage functionality such as snapshots.
•With 10GE connectivity, NAS devices can offer performance on par with many currently installed fibre channel SANs

Key NAS Uses
Traditional use cases for NAS devices include file shares, home directories and centralized logging. Recently, as the performance and availability of NAS devices has improved, many customers are expanding the use of NAS to include storage for relational databases such as Oracle and MySQL, server virtualization environments such as VMWare VSphere, and virtual desktop solutions such as VMWare VDI.

****************

SAN (Storage Area Network) and NAS(Network Attached Storage)
 

The main things that differentiate each of these technologies are mentioned below.

 

•How a storage is connected to a system. In short how the connection is made between the accessing system and the storage component (directly attached or network attached)

•Type of cabling used to connect. In short this is the type of cabling done to connect a system to the storage component (eg. Ethernet & Fiber channel)

•How are input and output requests done. In short this is the protocol used to conduct input and output requests (eg. SCSI, NFS, CIFS etc)

 

Related: How to monitor IO on linux

 

Let's discuss SAN first and then NAS, and at the end, let's compare each of these technologies to clear the differences between them.

 

SAN(Storage Area Network)
 

Today's applications are very much resource intensive, due to the kind of requests that needs to be processed simultaneously per second. Take example of an e-commerce website, where thousands of people are making orders per second, and all needs to be stored properly in the database for later retrieval. The storage technology used to store such high traffic data bases must be fast in request serving and response(in short it should be fast in Input and Output). 

 

Related: Web server Performance test

 

In such cases(where you need high performance, and fast I/O ) we can use SAN. 

 

SAN is nothing but a high speed network that makes connections between storage devices and servers. 

 

Traditionally application servers used to have their own storage devices attached to them. Server's talk to these devices by a protocol known as SCSI(Small Computer System Interface). SCSI is nothing but a standard used to communicate between servers and storage devices. All normal hard disks, tape drives etc uses SCSI. In the beginning the storage needs of a server was fulfilled by a storage devices that was included inside the server(the server used to talk to those internal storage device, using SCSI. This is very much similar to how a normal desktop talks to its internal hard disk.).

 

Devices like Compact Disk drives are attached to the server(which are part of the server) using SCSI. The main advantage of SCSI for connecting devices to a server was its high throughput. Although this architecture is sufficient for low end requirements, there are few limitations like the below mentioned ones. 

 

•The server can only access data on the devices, which are directly attached to it.
•If something happens to the server, access to data will fail (because the storage device is part of the server and is attached to it using SCSI)
•There is a limit in the number of storage devices the server can access. In case the server needs more storage space, there will be no more space that can be attached, as the SCSI bus can accommodate only a finite number of devices.  
•Also the server using the SCSI storage has to be near the storage device(because parallel SCSI, which is the normal implementation in most computer's and servers, has some distance limitations. It can work up to 25 meters.)
 

Some of these limitations can be overcame using DAS (Directly Attached Storage). The media used to directly connect storage to the server can be any one of SCSI, Ethernet, Fiber channel etc.). Low complexity, Low investment, Simplicity in deployment caused DAS to be adopted by many for normal requirement's. The solution was good even performance wise, if used with faster mediums like fiber channel. 

 

Even an external USB drive attached to a server is also a DAS(well conceptually its DAS, as its directly attached to the server's USB bus). But USB drives are normally not used due to the speed limitation of USB bus. Normally for heavy and large DAS storage solutions, the media used are SAS(Serially attached SCSI). Internally the storage device can use RAID(which normally is the case) or anything to provide storage volumes to servers. SAS storage options provide 6Gb/s speed these days.


*********


what is SAS(Serially Attached SCSI), FC(Fibre Channel), and iSCSI (Internet Small Computer System Interface)?
 

Traditionally the SCSI devices like the internal hard disk's are connected to a shared parallel SCSI bus. This means all devices attached, will be using the same bus to send/receive data.  But shared parallel connections are not good for high accuracy, and create issues during high speed transfers. However a serial connection between the device and the server can increase the overall throughput of the data transfer. SAS connections between storage devices and servers uses a dedicated 300 MB/Sec per disk. Think of SCSI bus that shares the same speed for all devices connected. 

 

SAS uses the same SCSI commands to send and receive data from a device.  Also please do not think that SCSI is only used for internal storage. It is also used for external storage device to be connected to the server. 

 

If data transfer performance and reliability is the choice, then using SAS is the best solution. In terms of reliability and error rate SAS disks are much better compared to the old SATA disks. SAS was designed by keeping performance in mind, due to which it is full-duplex. This means, data can be send and received simultaniously from a device using SAS. Also a single SAS host port can connect to multiple SAS drives using expanders. SAS uses point to point data transfer by using serial communication between devices (storage device, like disk drives & disk array's) and hosts. 

 

The first generation of SAS provided around 3Gb/s of speed. The second generation of SAS improved this to 6Gb/s. And the third generation (which is currently used by many organization's for extremly high throughput) improved this to 12Gb/s.

 

Fiber Channel Protocol
Fiber Channel is a relatively new interconnection technology used for fast data transfer. The main purpose of its design is to enable transport of data at faster rates with a very less/negligible delay. It can be used to interconnect workstations, peripherals, storage array's etc.

The major factor that distinguishes fiber channel from other interconnecting method is that, it can manage both networking and I/O communication over a single channel using the same adapters.

ANSI (American National Standards Institute) standardized Fiber channel during 1988. When we say Fiber (in Fiber channel) do not think that it only supports optical fiber medium. Fiber is a term used for any medium used to interconnect in fiber channel protocol. You can even use copper wire for lower cost.

 

Please note the fact that fiber channel standard from ANSI supports networking, storage and data transfer. Fiber channel is not aware of the type of data that you transfer. It can send SCSI commands encapsulated inside a fiber channel frame(it does not have its own I/O commands to send and receive storage). The main advantage is that it can incorporate widely adopted protocols like SCSI and IP inside.  

 

The components of making a fiber channel connection are mentioned below. The below requirement is very minimal to achieve a point to point connection. Typically this can be used for a direct connection between a storage array and a host.

•An HBA (Host Bus Adapter) with Fiber channel port

•Driver for the HBA card

•Cables to interconnect devices in HBA fiber channel port

 

As mentioned earlier, SCSI protocol is encapsulated inside fiber channel. So normally SCSI data has to be modified to a different format that fiber channel can deliver to the destination. And when the destination receives the data it then retranslates it to SCSI. 

 

You might be thinking that why do we need this mapping and re-mapping, why cant we directly use SCSI to deliver data. Its because SCSI cannot deliver data to greater distances to large number of devices (or large number of hosts). 

 

Fiber cannel can be used to interconnect systems as far as 10KM (if used with optical fibers. You can increase this distance by having repeaters in between). And you can also transfer data to an extent of 30m using a copper wire for lower cost in fiber cannel.

 

With the emergence of fiber channel switches from variety of major vendors, connecting many large number of storage devices and servers have now become an easy task(provided you have the budget to invest). The networking ability of fiber channel led to the advanced adoption of SAN(Storage Area Networks) for faster, long distance, and reliable data access. Most of the high computing environment's(which requires fast and large volume data transfers) uses fiber channel SAN with optical fiber cables.

 

The current fiber channel standard (called as 16GFC) can transmit data  at the rate of 1600MB/s(dont forget the fact that this standard was released in 2011). The upcoming standards in the coming years are expected to provide  3200MB/s and 6400MB/s speed. 

 

iSCSI(Internet Small Computer System Interface )
 

iSCSI is nothing but an IP based standard for interconnecting storage arrays and hosts. It is used to carry SCSI traffic over IP networks. This is the simplest and cheap solution(although not the best) to connect to a storage device.

This is a nice technology for location independent storage. Because it can establish connection to a storage device using local area networks, Wide area network. Its a Storage Area Network interconnection standard. It does not require special cabling and equipments like the case of a fiber channel network.

To the system using a storage array with iSCSI, the storage appears as a locally attached disk. This technology came after fiber channel and was widely adopted due to it low cost.

 

Its a networking protocol which is made on top of TCP/IP. You can guess that its not at all good performance wise, when compared with fiber channel(simply because everything is running over TCP with no special hardware and modifications to your architecture.)

iSCSI introduces a little bit of CPU load on the server, because the server has to do the extra processing for all storage requests over the network, with the regular TCP.  

 
**********

iSCSI has the following disadvantages, compared to fiber channel
 

•iSCSI introduces a little bit more latency compared to fiber channel, due to the overhead of IP headers
•Database applications have small read and write operations, which when done on iSCSI will introduce more latency
•iSCSI when done on the same LAN, which contains other normal traffic (other infrastructure traffic other than iSCSI), it will introduce a read/write lag or say low performance. 
•The maximum speed/bandwidth is limited to your ethernet and network speed. Even if you aggregate multiple links, it does not scal to the level of a fiber channel.
 

NAS(Network Attached Storage)
 

The simplest definition of NAS is "Any server that shares its own storage with others on the network and acts as a file server is the simplest form NAS". 

Please make a note of the fact that Network Attached Storage shares files over the network. Not storage device over the network. 

 

NAS will be using an ethernet connection for sharing files over the network. The NAS device will have an IP address, and then will be accessible over the network through that IP address. When you access files on a file server on your windows system, its basically NAS.

The main difference is in how your computer or the server treats a particular storage. If the computer treats a storage as part of itself(similar to how you attach a DAS to your server), in other words, if the server's processor is responsible for managing the storage attached, it will be some sort of DAS. And if the computer/server treats the storage attached as another computer, which is sharing its data through the network, then its a NAS. 

 

Directly attached storage(DAS) can be viewed as any other peripheral device like mouse keyboard etc. Because to the server/computer, its a directly attached storage device. However NAS is another server, or say an equipment having its own computing features that can share its own storage with others. 

 

Even SAN storage can also be considered as an equipment that has its own processing/computing power. So the main difference between NAS, SAN and DAS is how the server/computer accessing it sees. A DAS storage device appears to the server as part of itself. The server sees it as its own physical part. Although the DAS storage device might not be inside the server(its normally another device with its own storage array), the server sees it as its own internal part(DAS storage appears to the server as its own internal storage)

 

When we talk about NAS, we need to call them shares rather than storage devices. Because NAS appears to a server as a shared folder instead of a shared device over the network. Please do not forget the fact that NAS devices are computers in themselves, who can share their storage space with others. When you share a folder with access control using SAMBA, its NAS. 

 

Although NAS is a cheaper option for your storage needs. It really does not suit for an enterprise level high performance application. Never ever think of using a database storage (which needs to be high performing) with a NAS. The main downside of using NAS is its performance issue, and dependency on network(most of the times, the LAN which is used for normal traffic is also used for sharing storage with NAS, which makes it more congested)

 

Related: Linux Network Performance Tuning

 

When you share an export with NFS over the network, its also a form of NAS.


 

Related: NFS Tutorial in Linux

 

A NAS is nothing but a device/equipmet/server attached to TCP/IP network, that shares its own storage with other's. If you dig a little deeper, when a file read/write request is send to a NAS share attached to a server, the request is sent in the form of a CIFS(Common internet file system) or NFS(Network File system) requests over the network. The receiving end(NAS device), on receiving the NFS, CIFS request, will then convert it into the local storage I/O command set. This is the reason, why a NAS device has its own processing and computing power. 

 

So NAS is file level storage(Because its basically a file sharing technology). This is because it hides the actual file system under the hood. It gives the users an interface to access its shared storage space using NFS, or CIFS.

 

Related: How to do NFS Performance Tuning in Linux

 

A common use of NAS you can find is to provide each user with a home directory. These home directories are stored in a NAS device, and mounted to the computer, where the user logs in. As the home directory is networkly accessible, the user can log in from any computer on the network. 

 

Advantages of NAS
 
•NAS has a less complex architecture compared to SAN
•Its cheaper to deploy in an existing architecture.
•No modification is required on your architecture, as a normal TCP/IP network is the only requirement
 

Disadvantages of NAS
•NAS is slow
•Lowever throughput and high latency, due to which it cannot be used for high performance applications
 

Getting Back to SAN
 

Now let's get back to our discussion of SAN(Storage area network) which we started earlier in the beginning. 

 

The first and foremost thing to understand about SAN (apart from the things we already discussed in the beginning) is the fact that its a block level storage solution.  And SAN is optimized for high volume of block level data transfer. SAN is performs best when used with fiber channel medium (optical fibers, and a fiber channel switch )

 

Both NAS and SAN solves the problem of keeping the storage device nearer to the server accessing it(which was the case with DAS). A SAN storage can be alloted to a server, which in tern can share it with other's using NAS.  Please do not forget the fact that the underlying disks on a DAS, NAS and a SAN can be in any form of a RAID (what makes the real difference is how the server access these storage  devices, using which protocol and media).

 

The name Storage Area Network itself implies that the storage resides in its own dedicated network. Hosts can attach the storage device to itself using either Fiber channel, TCP/IP network (SAN uses iSCSI when used over tcp/ip network). 

 

SAN can be considered as a technology that combines the best features of both DAS and NAS. If you remember, DAS appears to the computer as its own storage device, and is known for good speed, DAS is also a block level storage solution(if you remember, we never talked of CIFS or NFS during DAS). NAS is known for its flexibility, primary access through network, access control etc. SAN combines the best features of both of these worlds together because....

 

•SAN storage also appears to the server as its own storage device
•Its a block level storage solution
•Good performance/speed
•Networking features using iSCSI
 

SAN and NAS are not competing technologies, but were designed for different needs and purposes. As SAN is a block level storage solution, its best suited for high performance data base storage, email storage etc. Most modern SAN solutions provide, disk mirroring, archiving backup and replication features as well. 

 

SAN is a dedicated network of storage devices(can include tape drives storages, raid disk arrays etc) all working together to provide an excellent block level storage. While NAS is a single device/server/computing appliance, sharing its own storage over the network.

 

Major Differences between SAN and NAS
 

SAN NAS 
Block level data access File Level Data access 
Fiber channel is the primary media used with SAN.  Ethernet is the primary media used with NAS 
SCSI is the main I/O protocol NFS/CIFS is used as the main I/O protocol in NAS 
SAN storage appears to the computer as its own storage NAS appers as a shared folder to the computer 
It can have excellent speeds and performance when used with fiber channel media It can sometimes worsen the performance, if the network is being used for other things as well(which normally is the case)
 
Used primarily for higher performance block level data storage Is used for long distance small read and write operations 



*********

NAS 
  
SAN 

Almost any machine that can connect to the LAN (or is interconnected to the LAN through a WAN) can use NFS, CIFS or HTTP protocol to connect to a NAS and share files.   Only server class devices with SCSI Fibre Channel can connect to the SAN. The Fibre Channel of the SAN has a limit of around 10km at best 
A NAS identifies data by file name and byte offsets, transfers file data or file meta-data (file's owner, permissions, creation data, etc.), and handles security, user authentication, file locking   A SAN addresses data by disk block number and transfers raw disk blocks. 
A NAS allows greater sharing of information especially between disparate operating systems such as Unix and NT.   File Sharing is operating system dependent and does not exist in many operating systems. 
File System managed by NAS head unit   File System managed by servers 
Backups and mirrors (utilizing features like NetApp's Snapshots) are done on files, not blocks, for a savings in bandwidth and time. A Snapshot can be tiny compared to its source volume.   Backups and mirrors require a block by block copy, even if blocks are empty. A mirror machine must be equal to or greater in capacity compared to the source volume.  


******************************


NAS (NFS,CIFS)

Network Attached Storage (NAS)

Network Attached Storage (NAS) devices are storage arrays or gateways that support file-based storage protocols such as NFS and CiFS, and are typically connected via an IP network. These file-based protocols provide clients shared access to storage resources. This centralization of shared storage resources reduces management complexity, minimizes stranded disk capacity, improves storage utilization rates and eliminates file server sprawl.

NAS vs SAN  <---------------------
 The primary difference between NAS and SAN solutions is the type of access protocol. NAS protocols such as NFS and CiFS provide shared file level access to storage resources. The management of the file system resides with the NAS device. SAN protocols such as iSCSI and fibre channel provide block level access to storage resources. Block level devices are accessed by servers via the SAN, and the servers manage the file system.

Despite their differences, SAN and NAS are not mutually exclusive, and may be combined in multi-protocol or unified storage arrays, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. The best of both worlds!

Benefits of NAS

•NAS devices typically leverage existing IP networks for connectivity, enabling companies to reduce the price of entry for access to shared storage.
• The RAID and clustering capabilities inherent to modern enterprise NAS devices offer greatly improved availability when compared with traditional direct attached storage.
• Because NAS devices control the file system, they offer increased flexibility when using advanced storage functionality such as snapshots.
• With 10GE connectivity, NAS devices can offer performance on par with many currently installed fibre channel SANs


(iscsi uses existing n/w to connect to SAN , Lower performance,cost)
Fiber Channel (better performance, requires dedicated card (hba) , special switch)


**************

Well known ports :

20 – FTP Data (For transferring FTP data)

21 – FTP Control (For starting FTP connection)

22 – SSH(For secure remote administration which uses SSL to encrypt the transmission)

23 – Telnet (For insecure remote administration

25 – SMTP(Mail Transfer Agent for e-mail server such as SEND mail)

53 – DNS(Special service which uses both TCP and UDP)

67 – Bootp

68 – DHCP

69 – TFTP(Trivial file transfer protocol uses udp protocol for connection less transmission of data)

80 – HTTP/WWW(apache)

88 – Kerberos

110 – POP3(Mail delivery Agent)

123 – NTP(Network time protocol used for time syncing uses UDP protocol)

137 – NetBIOS(nmbd)

139 – SMB-Samba(smbd)

143 – IMAP

161 – SNMP(For network monitoring)

389 – LDAP(For centralized administration)

443 – HTTPS(HTTP+SSL for secure web access)

514 – Syslogd(udp port)

636 – ldaps(both tcp and udp)

873 – rsync

989 – FTPS-data

990 – FTPS

993 – IMAPS

1194 – openVPN

1812 – RADIUS

995 – POP3s

2049 – NFS(nfsd, rpc.nfsd, rpc, portmap)

2401 – CVS server

3306 – MySql

3690 – SVN

6000-6063-X11

Note1:If protocol(TCP or UDP) is not mention then the above port are solely for TCP. Some service use UDP as mention in above list.

Note2:X11 use 6000 to 6063.. ports for connecting X11 from remote server.




********

The following is a very basic GRUB menu configuration file designed to boot either Red Hat Enterprise Linux or Microsoft Windows 2000:

default=0 
timeout=10 
splashimage=(hd0,0)/grub/splash.xpm.gz 
hiddenmenu 
title Red Hat Enterprise Linux Server (2.6.18-2.el5PAE)         
root (hd0,0)         
kernel /boot/vmlinuz-2.6.18-2.el5PAE ro root=LABEL=/1 rhgb quiet       
initrd /boot/initrd-2.6.18-2.el5PAE.img

# section to load Windows 
title Windows         
rootnoverify (hd0,0)         
chainloader +1



************

@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ :

1)
Procedure 1:
 Removing known_hosts file :
# hagar:/home/xxxx -> cd .ssh
# hagar:/home/xxxx -> mv known_hosts known_hosts.old

2)
Procedure 2:
Removing only offending key entry

From the above "Offending key in /home/<hagarid/.ssh/known_hosts:1421"
Here 1421 line is the offending key

# hagar:/home/xxxx -> vi +1421 .ssh/known_hosts 


*****************************************************************

Xen (Virtual Machine Monitor (VMM) also known as a hypervisor)  ::

is a software system that allows the execution of multiple virtual guest operating systems simultaneously on a single physical machine
Xen is known as a Type 1 or “bare-metal” hypervisor, meaning that it runs directly on top of the physical machine as opposed to within an operating system. 

Guest virtual machines running on Xen are known as “domains” and a special domain known as dom0 is responsible for controlling the hypervisor and starting other guest operating systems
These other guest operating systems are called domUs, this is because these domains are “unprivileged” in the sense they cannot control Xen or start/stop other domains. 

Xen supports 2 primary types of virtualization, para-virtualization and hardware virtual machine (HVM) also known as “full virtualization”:::

1) Para-virtualization uses modified guest operating systems that we refer to as enlightened guests. These operating systems are aware that they are being virtualized and as such don’t require virtual “hardware” devices, instead they make special calls to Xen that allow them to access CPUs, storage and network resources

Paravirtualization (OS Assisted) is virtualization in which the guest operating system (the one being virtualized) is aware that it is a guest and accordingly has drivers that, instead of issuing hardware commands, simply issue commands directly to the host operating system. This also includes memory and thread management as well, which usually require unavailable privileged instructions in the processor.

2)In contrast HVM guests need not be modified as Xen will create a fully virtual set of hardware devices for this machine that resemble a physical x86 computer. This emulation requires much more overhead than the paravirtualisation approach but allows unmodified guest operating systems like Microsoft Windows to run on top of Xen. HVM support requires special CPU extensions - VT-x for Intel processors and AMD-V for AMD based machines. This technology is now prevalent and all recent servers and desktop systems should be equipped with them. 


•Full Virtualization (using Binary Translation)is virtualization in which the guest operating system is unaware that it is in a virtualized environment, and therefore hardware is virtualized by the host operating system so that the guest can issue commands to what it thinks is actual hardware, but really are just simulated hardware devices created by the host. 

3)A third type of virtualization though not discussed in this guide is called PVHVM or “Para-virtualisation on HVM” which is a HVM domain with paravirtualized storage, network and other devices. This provides the best of both worlds by reducing expensive emulation but providing hardware accelerated CPU and memory access


The dom0 forms the interface to the hypervisor, through special instructions the dom0 communicates to Xen and changes the configuration of the hypervisor. This includes instantiating new domains and related tasks.

Another crucial part of the dom0’s role is that it is the primary interface to the hardware. Xen doesn’t contain device drivers, instead the devices are attached to dom0 and you can use standard Linux drivers. Dom0 then shares these resources with guest operating systems through a number of “backend” deamons. 


•There is also a combination of Para Virtualization and Full Virtualization called Hybrid Virtualization where parts of the guest operating system use paravirtualization for certain hardware drivers, and the host uses full virtualization for other features. This often produces superior performance on the guest without the need for the guest to be completely paravirtualized. An example of this: The guest uses full virtualization for privileged instructions in the kernel but paravirtualization for IO requests using a special driver in the guest. This way the guest operating system does not need to be fully paravirtualized, since this is sometimes not available, but can still enjoy some paravirtualized features by implementing special drivers for the guest.

So total 3:

1) Full virtualisation using Binary Translation
2) Para Virtualisation (OS Assisted)
3) H/w Assisted Virtualisation

************************************************************




Types of Hypervisors :

What is Hypervisor?
A Hypervisor also known as Virtual Machine Monitor (VMM) can be a piece of software, firmware or hardware that gives an impression to the guest machines(virtual machines) as if they were operating on a physical hardware. It allows multiple operating system to share a single host and its hardware. The hypervisor manages requests by virtual machines to access to the hardware resources (RAM, CPU, NIC etc) acting as an independent machine.

Now the Hypervisor is mainly divided into two types namely
 Type 1/Native/Bare Metal Hypervisor
 Type 2/Hosted Hypervisor

 Let us try to understand about them in detail


Type 1 Hypervisor
•This is also known as Bare Metal or Embedded or Native Hypervisor.
•It works directly on the hardware of the host and can monitor operating systems that run above the hypervisor.
•It is completely independent from the Operating System. 
•The hypervisor is small as its main task is sharing and managing hardware resources between different operating systems.
•A major advantage is that any problems in one virtual machine or guest operating system do not affect the other guest operating systems running on the hypervisor.


OS1 OS2 OS3
 |
HYPERVISOR
 |  
H/W 

          Examples:
          VMware ESXi Server
           Microsoft Hyper-V
           Citrix/Xen Server


Type 2 Hypervisor
•This is also known as Hosted Hypervisor.
•In this case, the hypervisor is installed on an operating system and then supports other operating systems above it.
•It is completely dependent on host Operating System for its operations
•While having a base operating system allows better specification of policies, any problems in the base operating system a ffects the entire system as well even if the hypervisor running above the base OS is secure.


GUESTOS1 GUESTOS2 GUESTOS3
 |
HYPERVISOR
 |
OS (HOST)
 |  
H/W 

         Examples:
          VMware Workstation
          Microsoft Virtual PC
          Oracle Virtual Box

**********************************************************






More Differences 



*******************
Cloud Computing:

Cloud computing is the delivery of computing as a service rather than a product, whereby shared resources, software, and information are provided to computers and other devices as a utility (like the electricity grid) over a network (typically the Internet). 

Clouds can be classified as public, private or hybrid

Cloud focuses on maximizing the effectiveness of the shared resources,not only shared by multiple users but are also dynamically reallocated per demand.

With cloud computing, multiple users can access a single server to retrieve and update their data without purchasing licenses for different applications.
Cloud providers typically use a "pay as you go" model

******************************
SaaS Examples: Google Apps (gmail), Salesforce, Workday, Concur, Citrix GoToMeeting, Cisco WebEx
IaaS Examples: Amazon Web Services (AWS), Cisco Metapod, Microsoft Azure, Google Compute Engine (GCE), Joyent
Paas Examples: Microsoft Azure


*******************

In computer networking, xinetd (extended Internet daemon) is an open-source super-server daemon which runs on many Unix-like systems and manages Internet-based connectivity.

It offers a more secure alternative to the older inetd ("the Internet daemon"), so most modern Linux distributions use it instead of that.[3]

---> It listens to multiple ports, and invokes only requested services.


xinetd listens for incoming requests over a network and launches the appropriate service for that request.[4] Requests are made using port numbers as identifiers and xinetd usually launches another daemon to handle the request. It can be used to start services with both privileged and non-privileged port numbers.

xinetd features access control mechanisms such as TCP Wrapper ACLs, extensive logging capabilities, and the ability to make services available based on time. It can place limits on the number of servers that the system can start, and has deployable defense mechanisms to protect against port scanners, among other things.

On some implementations of Mac OS X, this daemon starts and maintains various Internet-related services, including FTP and telnet. As an extended form of inetd, it offers enhanced security. It replaced inetd in Mac OS X v10.3, and subsequently launchd replaced it in Mac OS X v10.4. However, Apple has retained inetd for compatibility purposes.


The configuration files for xinetd are as follows: 


•/etc/xinetd.conf — The global xinetd configuration file. 


•/etc/xinetd.d/ directory — The directory containing all service-specific files. 

	

1) /etc/xinetd.conf :

	The /etc/xinetd.conf file contains general configuration settings which effect every service under xinetd's control. It is read once when the xinetd service is started, so for configuration changes to take effect, the administrator must restart the xinetd service. Below is a sample /etc/xinetd.conf file:

defaults
{
        instances               = 60
        log_type                = SYSLOG authpriv
        log_on_success          = HOST PID
        log_on_failure          = HOST
        cps                     = 25 30
}
includedir /etc/xinetd.d

2) /etc/xinetd.d directory :

service telnet
{
        flags           = REUSE
        socket_type     = stream
        wait            = no
        user            = root
        server          = /usr/sbin/in.telnetd
        log_on_failure  += USERID
        disable         = yes


*******************************

Inetd and xinetd both help to rstart and reload services but
to restrt service in inetd we use---
#service dhcpd start
but in xinetd we use----
#service xinetd start


The inetd daemon is best described as a super-server, created to manage many daemons or services. It listens to multiple ports, and invokes only requested services.
This reduces the load that services place on a system, because it means that network services – such as telnet, File Transfer Protocol (FTP), and Simple Mail Transfer Protocol (SMTP) – can be activated on demand rather than having to run continuously.
When a system initializes, the inetd daemon needs to determine its configuration information. To do this, the daemon accesses two files – /etc/services and /etc/inetd.conf.

xinetd (extended Internet daemon) is an open-source super-server daemon 

In linux, the extended Internet services daemon (xinetd) replaced inetd. It performs the same function as inetd in that it listens to multiple ports and invokes a requested service. However, it is more secure. Typical xinetd services include Remote Shell (RSH), FTP, telnet, and Post Office Protocol 3 (POP3).

xinetd listens for incoming requests over a network and launches the appropriate service for that request.[4] Requests are made using port numbers as identifiers and xinetd usually launches another daemon to handle the request. It can be used to start services with both privileged and non-privileged port numbers.

xinetd features access control mechanisms such as TCP Wrapper ACLs, extensive logging capabilities, and the ability to make services available based on time. It can place limits on the number of servers that the system can start, and has deployable defense mechanisms to protect against port scanners, among other things.

When you run “chkconfig -list”, you would see 2 set of service list.  One portion is termed as “Standard” service and other one as “xinetd” (Extended Inetd) based services:

Xinetd - Doesn’t depend on run levels - Xinetd dameon performs the job of continuously monitoring the service request under its control and activate it only when it is required.

ls -l /etc/xinetd.d  ---> #  location of xinetd config files

***********************


  xinetd  performs  the  same  function as inetd: it starts programs that
       provide Internet services.  Instead of having such servers  started  at
       system  initialization  time, and be dormant until a connection request
       arrives, xinetd is the only daemon process started and  it  listens  on
       all  service  ports  for the services listed in its configuration file.
       When a request comes in, xinetd starts the appropriate server.  Because
       of  the  way it operates, xinetd (as well as inetd) is also referred to
       as a super-server.

*************

xinetd (extended Internet daemon) is an open-source super-server daemon which runs on many Unix-like systems and manages Internet-based connectivity. It offers a more secure extension to or version of inetd, the Internet daemon, thus most modern Linux distributions have switched to it.

xinetd listens for incoming requests over a network and launches the appropriate service for that request.[2] Requests are made using port numbers as identifiers and xinetd usually launches another daemon to handle the request

There are three types of services. The type is INTERNAL if the service is provided by xinetd, RPC when it based on Remote procedure call, they are commonly listed in the /etc/rpc file, or it can be UNLISTED when the service is neither in the /etc/services nor in the /etc/rpc files.

*********************


WWN :


A World Wide Name (WWN) or World Wide Identifier (WWID) is a unique identifier used in storage technologies including Fibre Channel, Advanced Technology Attachment (ATA) or Serial Attached SCSI (SAS).

WWNs are important when setting up a storage area network (SAN). Each device has to be registered with the SAN by its WWN before the SAN will recognize it. (In fact, if the SAN has trouble recognizing a device, the WWN registration is usually one of the first things an administrator will check.) The names are usually 128 binary digits (bits) long, but could be 64 bits if the device is older.

A WWN is similar in concept to a network card's media access control (MAC) address in an Internet protocol (IP) network, but is formatted differently.

IDENTIFY WWN:

How to identify the HBA cards/ports and WWN in Linux

By Sandeep

There are several commands to determine the WWN of a Fibre Channel (FC) HBA and their status (online/offline). The post discusses few of the most commonly used methods.

Method 1
 To find the HBA cards installed on your system use :

# lspci -nn | grep -i hba
07:00.0 Fibre Channel [0c04]: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA [1077:2532] (rev 02)
07:00.1 Fibre Channel [0c04]: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA [1077:2532] (rev 02)

To check the available HBA ports :


# ls -l /sys/class/fc_host
total 0
drwxr-xr-x 3 root root 0 Feb  3  2015 host2
drwxr-xr-x 3 root root 0 Feb  3  2015 host3

To find the state of HBA ports (online/offline) :

# more /sys/class/fc_host/host?/port_state
::::::::::::::
/sys/class/fc_host/host2/port_state
::::::::::::::
Online
::::::::::::::
/sys/class/fc_host/host3/port_state
::::::::::::::
Online

To find the WWN numbers of the above ports :

# more /sys/class/fc_host/host?/port_name
::::::::::::::
/sys/class/fc_host/host2/port_name
::::::::::::::
0x500143802426baf4
::::::::::::::
/sys/class/fc_host/host3/port_name
::::::::::::::
0x500143802426baf6

Method 2 : Using systool
 Another useful command to find the information about HBAs is systool. If not already install, you may need to install the sysfsutils package.

# yum install sysfsutils

To check the available HBA ports :

# systool -c fc_host
Class = "fc_host"

  Class Device = "host2"
    Device = "host2"

  Class Device = "host3"
    Device = "host3"

To find the WWNs for the HBA ports :

# systool -c fc_host -v | grep port_name
    port_name           = "0x500143802426baf4"
    port_name           = "0x500143802426baf6"

To check the state of the HBA ports (online/offline) :

# systool -c fc_host -v | grep port_state
    port_state          = "Online"
    port_state          = "Online"


**********************************


***********************

A split-brain condition is the result of a Cluster Partition, where each side believes the other is dead, and then proceeds to take over resources as though the other side no longer owned any resources.

After this, a variety of Bad Things Will Happen - including destroying shared disk data.

This is the result of acting on incomplete information - neglecting Dunns Law. That is, when a node is declared "dead", its status is, by definition, not known. Perhaps it is dead, perhaps it is merely incommunicado. The only thing that is known is that its status is not known.

The ultimate cure to this is to use Fencing and lock the other side out.

The problem with merely using quorum without fencing, is that the loss of quorum can take an unbounded amount time to detect and react to in the worst case.

Fencing does not require knowledge of the timing or behavior of the "errant" nodes, nor does it require the cooperation or sanity of errant nodes. In addition, fencing operations receive positive confirmation. Hence, fencing has a high degree of certainty.

A good way of avoiding split brain conditions in most cases without having to resort to fencing is to configure redundant and independent cluster communications paths - so that loss of a single interface or path does not break communication between the nodes - that is the communications should not have a single point of failure (SPOF).

Using both redundant communications and fencing is a good way to go. We highly recommend both. 


******
Fencing is the process of locking resources away from a node whose status is uncertain.

There are a variety of fencing techniques available.

One can either fence nodes - using Node Fencing, or fence resources using Resource Fencing. Some types of resources are Self Fencing Resources, and some aren't damaged by simultaneous use, and don't require fencing at all. 

****

STONITH is a technique for NodeFencing, where the errant node which might have run amok with cluster resources is simply shot in the head. Normally, when an HA system declares a node as dead, it is merely speculating that it is dead. STONITH takes that speculation and makes it reality. "Make it so, Number One".

Reluctantly setting whimsy and humor aside...

There are a few properties a STONITH plugin must have for it to be usable:

    It must never report false positives for reset. If a STONITH plugin reports that the node is down, it had better be down.
    It must support the RESET command (on and off are optional)
    When given a RESET or OFF command it must not return control to its caller until the node is no longer running. Waiting until it comes up again for RESET is optional.
    All commands should work in all circumstances:
        RESET when node is ON or OFF should succeed and bring the node up (or at least attempt to bring it up - it may not boot for other reasons).
        OFF when node is OFF should succeed.
        ON when node is ON should succeed. 

If you don't follow these rules, Bad Things Will Happen - if not sooner, then later. 

******



********

ports:

 20 – FTP Data (For transferring FTP data)

21 – FTP Control (For starting FTP connection)

22 – SSH(For secure remote administration which uses SSL to encrypt the transmission)

23 – Telnet (For insecure remote administration

25 – SMTP(Mail Transfer Agent for e-mail server such as SEND mail)

53 – DNS(Special service which uses both TCP and UDP)

67 – Bootp

68 – DHCP

69 – TFTP(Trivial file transfer protocol uses udp protocol for connection less transmission of data) 

 80 – HTTP/WWW(apache)

88 – Kerberos

110 – POP3(Mail delivery Agent)

123 – NTP(Network time protocol used for time syncing uses UDP protocol)

137 – NetBIOS(nmbd)

139 – SMB-Samba(smbd)

143 – IMAP

161 – SNMP(For network monitoring)

389 – LDAP(For centralized administration)

443 – HTTPS(HTTP+SSL for secure web access)

514 – Syslogd(udp port)

636 – ldaps(both tcp and udp)

873 – rsync

989 – FTPS-data

990 – FTPS

993 – IMAPS

1194 – openVPN

1812 – RADIUS

995 – POP3s

2049 – NFS(nfsd, rpc.nfsd, rpc, portmap)

2401 – CVS server

3306 – MySql

3690 – SVN

6000-6063-X11 

/xinet.d
************



****************

  988  grep "^Apr 29" /var/log/syslog|less
  989  grep "^Apr 29" /var/log/syslog > syslog.20150429
  990  grep "^Apr 28" /var/log/syslog > syslog.20150428
  991  du -sh syslog.2015042*
  992  zip -m syslog syslog.2015042*
  993  du -sh syslog.zip
  994  ls
  995  mv syslog.zip ~adm_narayji2


sync; echo 3 > /proc/sys/vm/drop_caches





******************

File creation time (possible in ext4) :

debugfs -R 'stat <151200>' /dev/mapper/vg_root-lv_root

1. atime : The last time file was opened or executed,also when a file is used for other operations like grep, sort, cat, head, tail and so on.
2. ctime : The time the inode information was updated (chmod, chown etc does that). ctime also gets updated when file is modified
3. mtime: The time file was closed after it was opened for write

Command to see atime # ls -lu
Command to see ctime # ls -lc
Command to see mtime # ls -l


Show atime, ctime and mtime with "stat" command :

In Linux distributions, you will probably find a stat command, which can be used to show all of the times in a more convenient way, and among plenty of other useful information about your file:
ubuntu# stat /tmp/file1
File: `/tmp/file1'
Size: 9             Blocks: 8          IO Block: 4096   regular file
Device: 811h/2065d    Inode: 179420      Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2008-04-05 07:27:51.000000000 +0100
Modify: 2008-04-05 07:10:14.000000000 +0100
Change: 2008-04-05 07:35:22.000000000 +0100


************
Automatically logout inactive ssh sessions : (say 1 hour)

 Enable the following directives in SSH config file (/etc/ssh/sshd_config) and reload the 'sshd' service.

ClientAliveInterval 3600
ClientAliveCountMax 0

# service sshd reload  (or)  # service sshd restart

*************************************
grep (grep "some text") will grep some text. 

egrep (grep -E in linux) is extended grep where additional regular expression metacharacters have been added like +, ?, | and () 

fgrep (grep -F in linux) is fixed or fast grep and behaves as grep but does not recognise any regular expression metacharacters as being special. 


grep -i "string" FILE\
grep -c "This" demo_file
grep -n "This" demo_file

grep is an acronym that stands for "Global Regular Expressions Print"

The standard grep command looks like:
grep <flags> '<regular expression>' <filename>

Checking for full words, not for sub-strings using grep -w :

grep -iw "is" demo_file (it will show only is files and not this , his)


egrep: (same as grep -E)

egrep is an acronym that stands for "Extended Global Regular Expressions Print".
The 'E' in egrep means treat the pattern as a regular expression

In basic regular expressions (with grep), the meta-characters ?, +, {, |, (, and ) lose their special meaning. If you want grep to treat these characters as meta-characters, escape them \?, \+, \{, \|, \(, and \).

For example, here grep uses basic regular expressions where the plus is treated literally, any line with a plus in it is returned.

grep "+" myfile.txt

egrep on the other hand treats the "+" as a meta character and returns every line because plus is interpreted as "one or more times".

egrep "+" myfile.txt



fgrep: (same as grep -F) (Fixed and fast grep)

fgrep is an acronym that stands for "Fixed-string Global Regular Expressions Print".

It behaves as grep but does NOT recognize any regular expression meta-characters as being special

For example, if I wanted to search my .bash_profile for a literal dot (.) then using grep would be difficult because I would have to escape the dot because dot is a meta character that means 'wild-card, any single character':

grep "." myfile.txt
The above command returns every line of myfile.txt. Do this instead:

fgrep "." myfile.txt
Then only the lines that have a literal '.' in them are returned. fgrep helps us not bother escaping our meta characters.


pgrep : 

pgrep is an acronym that stands for "Process-ID Global Regular Expressions Print".

pgreptop ----> 7312



zgrep

Many a times we gzip the old file to reduce size and later wants to look or find something on those file

zgrep -i Error *.gz


*************


   Repetition
       A regular expression may be followed by one of several repetition operators:
       ?      The preceding item is optional and matched at most once.
       *      The preceding item will be matched zero or more times.
       +      The preceding item will be matched one or more times.
       {n}    The preceding item is matched exactly n times.
       {n,}   The preceding item is matched n or more times.
       {,m}   The preceding item is matched at most m times.
       {n,m}  The preceding item is matched at least n times, but not more than m times.


***************************************

Grep Options:

we want to search all lines in file example.txt which contains word UNIX but same time doesn't contain world Linux.

grep UNIX example.txt | grep -v Linux
UNIX operating system


How to count occurrence of a word in a file using grep command

If you want to count of a particular word in log file you can use grep -c option to count the word. Below example of command will print how many times word "Error" has appeared in logfile.txt.

Prints two lines of context around each matching line.

grep -C 2 'hello' *

You can use grep -w command in UNIX to find whole word instead of just pattern, as shown in following example. This example will only print lines from logfile.txt which contains full word ERROR.

grep -w ERROR logfile.txt
Above grep command in UNIX searches only for instances of 'ERROR' that are entire words; it does not match `SysERROR'.

If you want to see line number of matching lines you can use option "grep -n" below command will show on which lines Error has appeared.

grep -n ERROR log file.


to see matching pattern in color use below command.

grep Exception today.log --color

***************************

FIND:

Basic Find Commands for Finding Files with Names :

1. Find Files Using Name in Current Directory

Find all the files whose name is tecmint.txt in a current working directory.

# find . -name tecmint.txt

./tecmint.txt

2. Find Files Under Home Directory

Find all the files under /home directory with name tecmint.txt.

# find /home -name tecmint.txt

/home/tecmint.txt

3. Find Files Using Name and Ignoring Case

Find all the files whose name is tecmint.txt and contains both capital and small letters in /home directory.

# find /home -iname tecmint.txt

./tecmint.txt
./Tecmint.txt

4. Find Directories Using Name

Find all directories whose name is Tecmint in / directory.

# find / -type d -name Tecmint

/Tecmint

5. Find PHP Files Using Name

Find all php files whose name is tecmint.php in a current working directory.

# find . -type f -name tecmint.php

./tecmint.php

6. Find all PHP Files in Directory

Find all php files in a directory.

# find . -type f -name "*.php"

./tecmint.php
./login.php
./index.php


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

sticky bit (only dir usually) (tmp)(t) (1):
chmod o+t dir1   ------ ls -ld /tmp
chmod 1777 dir1


suid (only files) (passwd) (u+s) (4):

cchmod u+s hello.sh
(or)
chmod 4744 hello.sh  --- ls -l /usr/bin/passwd

sgid (files & dir) (g+s) (2)

chmod g+s /javaproject

chmod 2775 /shared/


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


Sticky Bit (only directories usually) (tmp)(t) (1):-

 ls -ld /tmp/
drwxrwxrwt 4 root root 4096 Aug 19 02:29 /tmp/   <------------------------

The sticky bit is used to indicate special permissions for files and directories. If a directory with sticky bit enabled, will restricts deletion of file inside it. It can be removed by root, owner of file or who have write permission on it. This is usefull for publically accessible directories like /tmp.
Implementation of Sticky bit on file:


The sticky bit works on directories only. If a user wants to create or delete a file/directory in some directory, he needs write permission on that directory. The write permission on a directory gives a user the privilege to create a file as well as the privilege to remove it.

The /tmp directory is the directory for temporary files/directories. This directory has all the rights on all the three levels, because all the users need to create/delete their temporary files. But as the users have write permission on this directory, they can delete any file in this directory. The permissions of that file do not have any effect on deletion.

But with sticky bit set on a directory, anyone can create a file/directory in it, but can delete his own files only. Files owned by other users cannot be delseted.



    $ chmod o-t dir1
    $ ls -l
    total 8
    drwxr-xr-x 2 root root 4096 Aug 19 03:08 dir1
    $ chmod o+t dir1
    $ ls -l
    total 8
    drwxr-xr-t 2 root root 4096 Aug 19 03:08 dir1

Alternatively

    $ chmod 1777 dir1/
    $ ls -l
    total 8
    drwxrwxrwt 2 root root 4096 Aug 19 03:08 dir1


*********************************


SUID ( setuid ) (only files) (passwd) (u+s) (4) :-

This is to be noted that SUID bit works on files only

If SUID bit is set on a file and a user executed it. The process will have the same rights as the owner of the file being executed.

For example: passwd command have SUID bit enabled. When a normal user change his password this script update few system files like /etc/passwd and /etc/shadow which can’t be update by non root account. So that passwd command process always run with root user rights.
Implementation of SUID on file:

ls -l /usr/bin/passwd
-rwsr-xr-x 1 r oot root 23420 Aug 3 2010 /usr/bin/passwd  <------------

Mehtod 1:

# chmod u+s tecadmin.txt
# ls -l tecadmin.txt
-rwsr-xr-x 1 root root 0 Mar  8 02:06 tecadmin.txt

Method 2:

# chmod 4655 tecadmin.txt
# ls -l tecadmin.txt
-rwSr-xr-x 1 root root 0 Mar  8 02:06 tecadmin.txt


*******************************

SGID ( setgid) (files & dir) (g+s) (2) :-

Same as SUID, The process will have the same group rights of the file being executed. If SGID bit is set on any directory, all sub directories and files created inside will get same group ownership as main directory, it doesn’t matter who is creating.

When SGID bit is set on a directory, all the files and directory created within it has the group ownership of the group associated with that directory. It means that after setting SGID bit on /javaproject directory, all the files and directories being created in this directory will have the group ownership of "javaproject" group. Moreover, this behavior is recursive, i.e. the directories created in this directory will have SGID bit set as well. The permissions for the new directory will also be same as that of /javaproject directory.

Implementation of SGID on directory:

# chmod g+s /test/
# ls -ld /test
drwxrwsrwx 2 root root 4096 Mar  8 03:12 /test

Now swich to other user and create a file in /test directory.

# su - tecadmin
$ cd /test/
$ touch tecadmin.net.txt
$ ls -l tecadmin.net.txt
-rw-rw-r-- 1 tecadmin root 0 Mar  8 03:13 tecadmin.net.txt

In above example tecadmin.net.txt is created with root group ownership.

(or)

$ ls -ld /shared/
drwxrwxr-x 2 root adm 4096 Aug 19 02:47 /shared/
$ chmod 2775 /shared/
$ ls -ld /shared/
drwxrwsr-x 2 root adm 4096 Aug 19 02:47 /shared/



*****************************************
GPT (Globaly unique identifier partition table) (replacement of msdos MBR)
MBR = 512 bytes = Boot loader(446 bytes) + partition table (64 bytes) + boot signature (2 bytes)
The MBR is limited to four primary partitions, and a single primary partition can hold an extended partition which can then be divided into logical partitions.
GPT is part of the Unified Extensible Firmware Interface (UEFI) specification
GPT does not have primary and logical partitions, but just partitions as GParted
The default maximum number of partitions is 128
The GPT GUIDs (Globally unique identifiers) and our familiar Linux UUIDs (Universally Unique Identifiers) are not the same thing, though they serve the same useful purpose: giving block devices unique names.
Linux UUIDs are a function of filesystems, and are created when the filesystem is created. - blkidthe MBR stores data about four partitions, known as primary partitions. Each partition is described in two ways: using cylinder/head/sector (CHS) notation and using logical block addressing (LBA) notation. The CHS notation is almost a historical footnote today, because it's a 24-bit number. This means that it's limited to describing areas of about 8GB in size. The 32-bit LBA values permit 2TiB sizes, assuming a sector size of 512 bytes. This 2TiB ceiling is not easily overcome; there simply aren't any unallocated fields left in the MBR that could be used to add more bits to the LBA addresses. 

Whether your computer uses a legacy BIOS or an EFI, GPT fixes many of the MBR's limitations: 
    GPT uses LBA exclusively, so CHS headaches are gone.
    Disk pointers are 64 bits in size, meaning that GPT can handle disks of up to 512 x 264 bytes (8 zebibytes, or 8.6 billion TiB), assuming 512-byte sectors.
    GPT data structures are stored twice on the disk: once at the start and again at the end. This duplication improves the odds of successful recovery in case of damage from an accident or a bad sector.
    Cyclic redundancy check values are computed for critical data structures, improving the odds of detection of data corruption.
    GPT stores all partitions in a single partition table (with backup), so there's no need for extended or logical partitions. By default, 128 partitions are supported, although you can change the partition table size if the partitioning software supports such changes.
    Whereas MBR provides a 1-byte partition type code, GPT uses a 16-byte globally unique identifier (GUID) value to identify partition types. This makes partition-type collisions less likely.
    GPT enables storing a human-readable partition name. You can use this field to name your Linux® /home, /usr, /var, and other partitions for easier identification within partitioning software. 

The first sector of the disk is reserved for a protective MBR, which is a legal MBR data structure that defines a single partition of type 0xEE (EFI GPT). On sub-2TiB disks, this partition should span the entire disk; on larger disks, it should be 2TiB in size. The idea is to protect the GPT disk from damage by GPT-unaware disk utilities. If such tools look at the disk, they'll see an MBR disk with no free space. (Some disk utilities can create a hybrid MBR, which defines up to three MBR partitions in addition to the EFI GPT partition. The idea is to enable a GPT-unaware operating system, such as most pre-Windows Vista® versions of Windows®, to coexist on a disk along with GPT partitions. This configuration is decidedly non-standard and kludgy, though.)

Because GPT incorporates a protective MBR, a BIOS-based computer can boot from a GPT disk using a boot loader stored in the protective MBR's code area, but the boot loader and operating system must both be GPT aware. (Some buggy BIOSes have problems booting from GPT disks, though.) EFI provides its own boot methods, so you can boot from a GPT disk on an EFI-based system.

The main problem with GPT is one of compatibility: Low-level disk utilities and operating systems must all support GPT. Such support is fairly common for Linux, although you may need to attend to some of these details and change some of the tools you use for low-level disk maintenance. If you multi-boot a computer, you'll have to look into GPT support for all of your operating systems.

If you administer many Linux systems, or if you anticipate adding an over-2TiB disk in the not-too-distant future, you may want to consider doing a test installation with GPT. Doing so before you're forced to do it will give you first-hand experience with GPT's features as well as with the quirks of some of the GPT-aware Linux utilities.

It's possible to run a system with a mixture of MBR and GPT disks. For instance, you can boot from an MBR disk but still use GPT for a data disk. Such a configuration is most useful for Windows on BIOS-based systems, because Windows can't boot from GPT using BIOS, but Windows Vista and later Microsoft operating systems can use a GPT data disk. 


************

Centrify :  robust technology for integration and single sign-on to Active Directory 



Centrify would provide organizations with the best solution to centrally manage identities for all quantities of UNIX servers within their existing Active Directory infrastructure



Centrify Express for UNIX quickly and easily integrates UNIX systems with Active Directory from an authentication and single sign-on perspective. Centrify Express for Linux and UNIX is comprised of the following components:

    DirectControl Express secures your non-Windows systems using the same authentication services deployed in your Windows environment.
    DirectManage Express centrally discovers your systems, checks their ability to integrate with Active Directory, downloads the required software packages and automatically deploys to your systems. DirectManage Express also provides you with a single pane of glass to quickly and remotely access any of your systems securely using your Active Directory credentials.

    Centrify-enabled open source tools including OpenSSH, PuTTY and Samba that have been enhanced and tested to seamlessly work with Active Directory and support Kerberos while delivered as easy to install pre-compiled packages.

Centrify Express for Linux and UNIX is free to use, non-intrusive, easy to deploy, robust in operation, and delivers more functionality and more to upgrade to than any other free Active Directory integration solution.


Benefits :

Users login as themselves using their AD indentity ,great way to break the habit of sharing accounts and to reduce forgotten passwords.


Difference between Centrify Express & Centrify Server Suite:


    Centrify Express for Linux and UNIX is primarily useful when you need proven Active Directory authentication for a small number of systems. You have access to peer support through our Centrify Express Community.

    Centrify Server Suite is required in larger environments that need automated, centralized management of user identities, access controls, and privilege levels. You will also need the Centrify Server Suite for advanced features such as granular Zone-based access controls, cross-platform policy enforcement through Group Policy, privilege management, detailed auditing and reporting on user activity, server isolation, encryption of data-in-motion and single sign-on to SAP and web applications — to name just a few. In addition to peer support, Centrify Server Suite customers have guaranteed service-level agreements, plus 24x7 access to technical support representatives and an extensive online knowledge base.


http://www.centrify.com/express/linux-unix/reasons-to-upgrade/ *


******************************

http://web.global.nibr.novartis.net/apps/confluence/display/scicomp/USEM+-+cfengine



[adm_narayji2@nrusem-slp9992 emv]$ pwd
/home/emv-sa/repo/cfengine/inputs/emv

        phusem_s3701::
                { /etc/sudoers
                  Backup "timestamp"
                  DefineClasses "check_sudoers"
                  BeginGroupIfNoSuchLine "%%admins_nibr_mms_srv_gbl_a ALL=(ALL) /bin/su - oracle, (oracle) ALL"
                        DeleteLinesStarting "%admins_nibr_mms_srv_gbl_a"
                        InsertLine "%admins_nibr_mms_srv_gbl_a ALL=(ALL) /bin/su - oracle, (oracle) ALL"
                  EndGroup
                }

*****************************************

[adm_narayji2@nrusem-slp9992 emv]$ cat cf.passwd
## this file was not refered by any other cf config, probably never went live in barry's time.

## yeap, use cf.base.passwd

editfiles:
#       pollux|sys_cluster_nodes|sys_cluster_head|phusem_builder|ls4x2a|ls4x2b::
#               { /etc/shadow
#                 BeginGroupIfNoSuchLine "root:$1$mop6S2zz$4wyT/Ht.aOZelMb5oZTjz0:14083:0:10000::::"
#                       ReplaceAll "^root.*" With "root:$1$mop6S2zz$4wyT/Ht.aOZelMb5oZTjz0:14083:0:10000::::"
#                 EndGroup
#               }
        ##phusem_vml_knime1|phusem_vml_knime2:
        phusem_vml_knime1:
                { /etc/shadow
                  BeginGroupIfNoSuchLine                "root:$1$XMEth8Fo$bM9PbHCJd0GBKsITB.o171:15876:0:99999:7:::"
                        ReplaceAll "^root.*" With       "root:$1$XMEth8Fo$bM9PbHCJd0GBKsITB.o171:15876:0:99999:7:::"
                        # r0 N
                  EndGroup
                }
                {
                  /etc/passwd
                  BeginGroupIfNoSuchLine        "adm_hoti10:x:1506381530:0:tin:/tmp:/bin/bash"
                        DeleteLinesStarting     "adm_hoti1"
                        InsertLine              "adm_hoti10:x:1506381530:0:tin:/tmp:/bin/bash"
                  EndGroup
                }

************************************************

packages:
        os_redhat::
                compat-libstdc++-33 action=install
                conky define=install_conky
                glew elsedefine=install_glew
                kernel-headers action=install
                libgfortran action=install
                libicu action=install
                logwatch define=remove_logwatch
                lua action=install
                mesa-libGLU action=install
                nedit action=install
                rcs action=install
                ntp action=install
                postfix action=install
                redhat-lsb action=install
                sysstat action=install
                tcllib action=install
                tklib action=install
                tix action=install
                wireshark action=install
                wget action=install
                yum-utils action=install

        os_debian::
                cfengine2 action=install
                postfix action=install
                ntp action=install
                sudo action=install


************************************************

Cfengine 2 is a configuration management framework.  It was implemented in Emeryville by Barry King in 2008 to more efficiently manage the increasing number of Unix and Linux systems at the site.  At that time, CFEngine or BConfig was run independently at other site such as Basel and Cambridge.

In 2014, most machines are now build using Puppet.  But the large number of legacy machines have not been rebuild or migrated to puppet yet, as such, some 125 machines are still managed by CFEngine in Emeryville.   These include physical servers, VM and workstations. 

Hosts that are managed by cfengine have hourly cronjob run by root.  FIles that CFEngine edits have copies saved with timestamp and .cfsaved appended to them.  Most, but not all, files are edited using sed rather than whole file replacement from a standard template.  The content of the edit are programmed into the cfengine policies. 



**************************************


Using puppet, we can create Filesystems,install packages, create users,groups,sudo access,init.d scripts during boot


[root@nrusca-slp9996 ~]# puppet --version
3.7.4

[root@nrusca-slp9996 puppet]# ruby --version
ruby 1.8.7 (2013-06-27 patchlevel 374) [x86_64-linux]


Example :

[adm_narayji2@nrusca-slp9996 hosts]$ cat nrusca-slt0008.nibr.novartis.net.yaml

---
mountfs_mountpoints:
  nrusca-slt0151_apps:
    mountpoint: '/apps'
    source: 'ext4:///dev/vg_apps/lv_apps'
    size: '10'

---
mountfs_mountpoints:
  nrusca-slt0008_u01:
    mountpoint: '/u01'
    type: 'static'
    source: 'ext4:///dev/mapper/vg_apps-lv_oud'
    ownership: 'oracle:'
    ensure_mounted: 'true'
    options: 'defaults'


---
mountfs_mountpoints:
  nrusca-slt0061_usr_prog2:
    mountpoint: '/usr/prog2'
    type: 'static'
    source: 'nfs://isiusca06.nibr.novartis.net/ifs/usca/prog2'



core_local_users:
 - oracle

core_local_groups:
 - dba

sudo_rules:
 - '%admins_nibr_oracle_srv_gbl_a ALL= NOPASSWD: /bin/su - oracle'
 - '%admins_nibr_mms_iam_srv_gbl_a ALL= NOPASSWD: /bin/su - oracle'
 - 'oracle ALL = (ALL) NOPASSWD: /usr/local/bin/acs-oracle-run-root.sh'

core_packages_extra:
  - 'binutils'
  - 'compat-libstdc++-33'
  - 'elfutils-libelf'
  - 'elfutils-libelf-devel'
  - 'gcc'
  - 'gcc-c++'
  - 'glibc'
  - 'glibc-common'
  - 'glibc-devel'

makes sure some services are running and enabled:

core_services_extra:
  - 'httpd_mw'


ulimits_rules:
  - '* - memlock unlimited'
  - 'oracle - core unlimited'
  - 'oracle - nproc 16384'
  - 'oracle - stack 10240'
  - 'oracle - nofile 131072'

sysctl_rules:
  - 'kernel.shmmax = 6442450944'
  - 'kernel.shmall = 1572864'
  - 'fs.file-max = 6815744'
  - 'fs.aio-max-nr = 1048576'
  - 'kernel.sem = 256 32000 100 128'
  - 'kernel.shmmni = 4096'
  - 'net.ipv4.ip_local_port_range = 9000 65500'


sudo_rules:
  'adm_flemmda1 ALL=(ALL) NOPASSWD: /bin/vi /etc/httpd/conf/httpd.conf, /etc/init.d/httpd restart, /usr/bin/crontab -e, /usr/bin/crontab -l, /opt/apache-tomcat*/bin/sta


core::add_cronjob {'check-git-repos-state':
  requestor => $module_name,
  time      => '0 10,12,14,16,18,19 * * 1-5',
  command   => '/usr/local/bin/check-git-repos-state.sh',


Puppet works in a classic client-server architecture. The client machine has the Puppet client installed and is configured to contact a server, called a Puppetmaster, to retrieve the description of what its configuration should be (called the catalog). The local Puppet client then “applies” the configuration described in this catalog, making local changes if necessary.
As the wanted configuration can depend on criterias like the OS release, the number of CPUs, etc.. the client locally computes so called facts which are passed to the puppetmaster together with the catalog request. There are a number of facts which are computed by facter out of the box, and it’s possible to develop so called custom-facts (i.e the datacenter name, depending on the machine’s IP address). All these facts are available to the puppetmaster when it compiles the client’s catalog.
There is a Puppetmaster per site, to manage the DEV/TEST/PROD servers on that site. There can be any number of DEV Puppetmaster servers, to which a number of experimental client machines are attached to.
The Puppet code is written in Puppet’s own language and stored in files called manifests on the Puppet master server. The catalog requested by client machines when they contact the Puppetmaster is compiled from these manifests.
The data (the values populated in the catalog and applied on the host) is written in YAML and stored in a hierarchical tree, called hiera in the rest of this document.


Most of the files managed by Puppet are so-called concat files. Some modules support the notion of external fragment. This allows configuration NOT managed by Puppet to be integrated in the files Puppet manages. The tool/process/person putting configuration in these external fragments is accountable for what it does or breaks. Puppet uses it as-is.
External fragments are disabled by default. They can be enabled on a server-basis (in Hiera), or at the server-type level (i.e srv_middleware servers)
Modules supporting external fragments:
Module	Managed file	External fragment path
sysctl
/etc/sysctl.conf	/etc/sysctl.conf.ext
ulimits
/etc/security/limits.conf	/etc/security/limits.conf.ext
mountfs
/etc/fstab	/etc/fstab.ext
autofs
/etc/auto.direct	/etc/auto.direct.ext
hosts_access
/etc/hosts.allow
/etc/hosts.deny	/etc/hosts.allow.ext
/etc/hosts.deny.ext

You can run the Puppet client manually via:
puppet agent --no-daemonize --onetime [--verbose|--debug]
For debugging it may be useful to run
puppet agent –test




******************************

Bring server from unmanaged to managed :

crm_attribute -n is-managed-default -v true

Failed actions and count :

crm_resource –C –H vhst02#### -r vsrv02
crm_resource –C –H vht01#### -r vsrv01
crm_failcount -G -r vsrv01
crm_failcount -v 0 -r vsrv01

Reflection X :


xm con vmname (linux Vms)(eg: console of vm after shutting down)
./get_vm_location.pl saps01 (Check vm location)
./start_xen_guest.pl (if its stopped)
To come out :
Ctrl ] (or) ctrl 5

Vhst01 offline issue :(2274.us)
meatclient -c vhst01xxxx (as we cannot move vms for stonith issues)
move vms on good node
then ask sst to reboot



crm resource stop saps01
cd /opt/wmxenadm/
./get_vm_location.pl saps01
crm resource start saps01
/sbin/rcdrbd status
show cli-prefer-saps01
crm confiure 



xm info | grep memory
xm mem-set xx 8192 



TO send Dump:
cd /var/lib/xen
ls -ltr
mv dump dump.orig
ls -ltr
ulimit -a 
ulimit -f unlimited
ln -s /usr/sys/inst.images/dump
xm dump-core -L <vmname>


DRBD recovered after power outage :

crm configure show cli-prefer-VM name
crm resource cleanup <vm name> (go to that VHst and clear failed action on relevant vm)


DRBD:
rcdrbd status

Example to check stonith occurences:

cat /var/log/messages|grep -i sto


************************************************

NIS :(YP - Yellow pages)

NIS is a lookup service for set of databases. The databases in this cases can be a passwd file, group file, hosts file, etc. This is primarily used as a central repository to hold all username and passwords (i.e /etc/passwd), and different servers can authenticate against this server for the username and password

This is very helpful for system administrators who has to manage several servers. Instead of creating useraccount for your users on each and every Linux servers, you can just create the account on one server that is configured to run NIS server. All other servers can be configured as NIS client, which will authenticate against this central NIS server repository.


This is a step-by-step tutorial that explains the installation and configuration of ypserv NIS server and client.


NIS Server Configuration :: (install NIS server on a servername called prod-db)

1. Verify Portmap

Portmap server maps DARPA port to RPC program number. For a NIS client that makes RPC calls to talk to the NIS Server (which is a RPC server), portmapper should be running.

When the NIS server starts, it informs the portmapper on what port it is listening. When NIS client contacts a NIS server, it will first check with the portmapper and get the portnumber where the NIS servers is running, and will send the RPC calls to that port number.

On most Linux distributions, portmap will be running by default. Make sure it is running, and configured to be started when the system is rebooted.


# ps -ef | grep -i portmap
rpc       3624     1  0 Feb23 ?        00:00:00 portmap
root     16908  8658  0 10:35 pts/0    00:00:00 grep -i portmap

# chkconfig --list | grep portmap
portmap         0:off   1:off   2:off   3:on    4:on    5:on    6:off



2. Install YPServ


 rpm -ivh ypserv-2.19-5.el5.i386.rpm

# whereis ypserv

3. Start ypserv

Check to see whether the ypserv is registered with the portmap as shown below.

# rpcinfo -u localhost ypserv
rpcinfo: RPC: Program not registered
program 100004 is not available


The above output indicates either ypserv is not installed, or ypserv is installed but not started yet. The following quick check indicates that the ypserv is not started yet.

# chkconfig --list | grep yp
ypbind          0:off   1:off   2:off   3:off   4:off   5:off   6:off
yppasswdd       0:off   1:off   2:off   3:off   4:off   5:off   6:off
ypserv          0:off   1:off   2:off   3:off   4:off   5:off   6:off
ypxfrd          0:off   1:off   2:off   3:off   4:off   5:off   6:off

# service ypserv status
ypserv is stopped

Set the NISDOMAIN in the /etc/sysconfig/network file as shown below.

# vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=prod-db
GATEWAY=192.168.1.1
NISDOMAIN=thegeekstuff.com

Start the ypserv as shown below.

# service ypserv start
Setting NIS domain name thegeekstuff.com: [  OK  ]
Starting YP server services:              [  OK  ]

There are some NIS server configuration parameters set in the /etc/ypserv.conf file. But, you don’t need to modify the default values in this file.


4. Generate NIS Database

Once the ypserv is installed and started, it is time to generate the NIS database. All the NIS database are stored under /var/yp directory. Before you generate the database you will not see the directory for your domain name under the /var/yp.

# ls -l /var/yp
total 36
drwxr-xr-x 2 root root  4096 May 18  2010 binding
-rw-r--r-- 1 root root 16669 Oct 31  2008 Makefile
-rw-r--r-- 1 root root   185 Jun  6  2007 nicknames

Generate the NIS database using ypinit program as shown below. You just have to enter the hostname of your NIS server to generate the database.

# /usr/lib/yp/ypinit -m

Please continue to add the names for the other hosts, one
per line.  When you are done with the list, type a .
        next host to add:  prod-db
        next host to add: 

The current list of NIS servers looks like this: prod-db

Is this correct?  [y/n: y]  y
We need a few minutes to build the databases...
Building /var/yp/thegeekstuff.com/ypservers...
Running /var/yp/Makefile...
gmake[1]: Entering directory `/var/yp/thegeekstuff.com'
Updating passwd.byname...
Updating passwd.byuid...
Updating group.byname...
Updating group.bygid...
Updating hosts.byname...
Updating hosts.byaddr...
...
gmake[1]: Leaving directory `/var/yp/thegeekstuff.com'

prod-db has been set up as a NIS master server.

Now you can run ypinit -s prod-db on all slave server.

After generating the database, you can see a new directory for your domain is created under /var/yp as shown below.

# ls -l /var/yp
total 44
drwxr-xr-x 2 root root  4096 Oct  8 10:59 thegeekstuff.com
drwxr-xr-x 2 root root  4096 May 18  2010 binding
-rw-r--r-- 1 root root 16669 Oct 31  2008 Makefile
-rw-r--r-- 1 root root   185 Jun  6  2007 nicknames
-rw-r--r-- 1 root root    10 Aug 31 10:58 ypservers

The /var/yp/ypservers will contain the name of your NIS server hostname.

# cat /var/yp/ypservers
prod-db


5)Verify the installation

Verify the NIS server installation by checking whether the passwd file can be accessed using the ypcat NIS client program.

# ypcat passwd
No such map passwd.byname. Reason: Can't bind to server which serves this domain

You might get the above error message because ypbind might not running on your system. Just start the ypbind and verify the configuration.

# service ypbind start

# ypcat passwd
ramesh:R7EFEGJ1mxRGwVLVC.:401:401::/home/ramesh:/bin/bash
john:QtlRW$Fx.uZvD:402:402::/home/john:/bin/bash

If you don’t like to display the encrypted passwd field in the ypcat passwd output, set the MERGE_PASSWD to false in the /var/yp/Makefile as shown below.

# vi /var/yp/Makefile
MERGE_PASSWD=false

After you do the above, the ypcat passwd command will just display a ‘x’ in the passwd file.

# ypcat passwd
ramesh:x:401:401::/home/ramesh:/bin/bash
john:x:402:402::/home/john:/bin/bash

Anytime you make a change (either updates to the Makefile, or changes to a database). For example, when you add a new user, or modify an existing user account, you should do the following. Without this, the changes will not be reflected to any of your NIS client.

# cd /var/yp
# make

I recommend that you add this to the root cron job on your NIS server to execute this every 15 minutes. This way, you don’t need to worry about running this manually anytime you make some changes to the NIS database.





NIS Client Configuration :

The following steps needs to be executed on the NIS client. In the above example, we installed NIS server on a servername called prod-db. If you want another Linux server dev-db, to use the /etc/passwd file on the prod-db for authentication, you need to do the following steps on the dev-db server (NIS client).
6. Set the Domainname on Client

Verify the domainname is set properly on this server. If this doesn’t return the proper domainname. Execute ‘domainname {your-domain}’ to set the domainname on the server.

# domainname
thegeekstuff.com

domainname command will set the domainname temporarily. i.e if you reboot the system, the domainname will be gone. To make the domainname permanent, update the network file and set the NISDOMAIN parameter as shown below.

# cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=dev-db
GATEWAY=192.168.1.4
NISDOMAIN=thegeekstuff.com

7. Set the NIS Server Name on Client

Add the following line to the /etc/yp.conf file. This instructs the NIS client that the NIS server is prod-db. Instead of prod-db below, you can also give the ip-address of the prod-db server.

# vi /etc/yp.conf
domain thegeekstuff.com server prod-db

8. Start the ypbind on Client

ypbind is a NIS binding program. This searches for a NIS server for your NIS domain and maintains NIS binding information.

Make sure ypbind is up and running on the NIS client server. Most Linux distributions has ypbind installed already. If it is not running, start it.

# ps -ef | grep ypbind

# service ypbind start

Verify the NIS server installation by checking whether the passwd file can be accessed using the ypcat NIS client program.

# ypcat passwd
No such map passwd.byname. Reason: Can't bind to server which serves this domain

You might get the above error message because ypbind might not running on your system. Just start the ypbind and verify the configuration.

# service ypbind start

# ypcat passwd
ramesh:x.:401:401::/home/ramesh:/bin/bash
john:x:402:402::/home/john:/bin/bash

**********************************


NFS:(window is not NFS compatible)

NFS, or Network File System, is a server-client protocol for sharing files between computers on a common network. NFS enables you to mount a file system on a remote computer as if it were local to your own system. You can then directly access any of the files on that remote file system. The server and client do not have to use the same operating system. The client system just needs to be running an NFS client compatible with the NFS server.


Configure nfs server:

    1)A linux server with ip address 192.168.0.254 and hostname Server
    2)A linux client with ip address 192.168.0.1 and hostname Client1
    3)Updated /etc/hosts file on both linux system
    4)Running portmap and xinetd services
    5)Firewall should be off on server

Three rpm are required to configure nfs server. nfs, portmap, xinetd check them if not found then install:::

now create a /data directory and grant full permission to it

mkrir /data
chmod 777/data


vi /etc/exports

/data  192.168.0.0/24(rw,sync)


restart nfs service

3 services to be started:
service nfs start
service nfslock start
service portmap start


1.To export all directories in the /etc/exports file:
exportfs -a 


verify with showmount command that you have successfully shared data folder:

showmount -e


Configure client system:::

ping form nfs server and check the share folder :
showmount -e 192.168.0.254


mkdir /nfsshare
mount -t nfs 192.168.0.254:/data /nfsshare
(so now data and nfsshare will have same contents)

vi /etc/fstab:

192.168.0.254:/data  /nfsshare   nfs defaults 00


*********************************

How to start and stop printer queues:
How can I start and stop printer queues for printer called HPLJF2?

/usr/bin/enable HPLJF2
/usr/bin/disable HPLJF2

Verify queue is stopped or started
lpq


Linux print commands
Linux has several different print commands that you can use from a terminal window. Most of the commands can be used by both the logged on user id as well as by root. Others work only from root. The following is a list of the utilities that allow you to administer and control the printing system that you can use when logged in as either user or root:
/usr/bin/lp - Used to submit print jobs
/usr/bin/lp.cups - Used to submit print jobs
/usr/bin/lpoptions - Gets and sets printer options for a single user when run by a user or for the system when used by root
/usr/bin/lppasswd - Changes printing passwords for an individual user or adds, deletes, and changes printer users and passwords when run by root
/usr/bin/lpq - Shows the status of a printer queue
/usr/bin/lpq.cups - Shows the status of a printer queue
/usr/bin/lpr - Used to submit print jobs
/usr/bin/lpr.cups - Used to submit print jobs, forcing use of CUPS
/usr/bin/lprm - Used to remove print jobs from a queue
/usr/bin/lprm.cups - Used to remove print jobs from a queue
/usr/bin/lpstat - Gives status of the CUPS system, such as queue lengths and printers
/usr/bin/lpstat.cups - Gives status of the CUPS system, such as queue lengths and printers
/usr/bin/cancel - Cancels a print job
/usr/bin/enable - Enables a print queue or class of printers, requires a management password
/usr/bin/disable - Disables a print queue or class of printers, requires a management password
/usr/sbin/lpadmin - Manages printers and classes, requires a management password
/usr/sbin/lpc - A compatibility program for Berkley style printers, limited to queue status in CUPS
/usr/sbin/lpc.cups - A compatibility program for Berkley style printers, limited to queue status in CUPS
/usr/sbin/lpdomatic - A filter script provided to be used in setting up printers
/usr/sbin/lpinfo - Shows available printer devices and drivers on the system
/usr/sbin/lpmove - Moves the jobs destined for a queue to another queue

These commands only work when you're logged in as root, either when you log in or after switching to root using the su command:
/usr/bin/lprsetup.sh - A shell script provided to help set up ghostscript printers
/usr/sbin/accept - Causes the print queue to accept requests
/usr/sbin/reject - Causes the print queue to reject requests


************************************

Linux shutdown command:

shutdown -h time "message"

Where,

-h : Poweroff the system.

time : When to shutdown. You can poweroff immediately or after 2 minutes. It can be an absolute time in the format hh:mm, in which hh is the hour (1 or 2 digits) and mm is the minute of the hour (in wo digits). Second, it can be in the format +m, in which m is the number of minutes to wait. The word now is an alias for +0.

message: Send warning message to send to all users.


# shutdown -h now

See log of system shutdown:
# last reboot
# last shutdown


********************
http://landoflinux.com/linux_tune2fs_command.html


tune2fs Command:
Adjusting Tunable filesystem parameters for ext2, ext3 and ext4 filesystems with tune2fs

To display the current values that are set you can use 
tune2fs -l (or)
dumpe2fs command.

a)Displaying FS Check Intervals:::::

$ sudo tune2fs -l /dev/sda1 |grep interval
Check interval:           15552000 (6 months)

b)Displaying FS Mount counts:::::

john@john-desktop:~$ sudo tune2fs -l /dev/sda1 |grep -i count

Inode count:              9609216
Block count:              38419456
Reserved block count:     1920972
Mount count:              6
Maximum mount count:      35

**********************
dumpe2fs - dump filesystem information  
dumpe2fs prints the super block and blocks group information for the filesystem present on device.


*********************

Procfs: The proc file system acts as an interface to internal data structures in the kernel. It can be used to obtain information about the system and to change certain kernel parameters at runtime using sysctl command. For example you can find out cpuinfo with following command:
# cat /proc/cpuinfo
(or)
you can enable or disable routing/forwarding of IP packets between interfaces with following command:
# cat /proc/sys/net/ipv4/ip_forward
# echo "1" > /proc/sys/net/ipv4/ip_forward
# echo "0" > /proc/sys/net/ipv4/ip_forward

******************

What is a UNIX/Linux File system?

A UNIX file system is a collection of files and directories stored. Each file system is stored in a separate whole disk partition. The following are a few of the file system:

/ - Special file system that incorporates the files under several directories including /dev, /sbin, /tmp etc
/usr - Stores application programs
/var - Stores log files, mails and other data
/tmp - Stores temporary files
See The importance of Linux partitions for more information.

But what is in a File system?

Again file system divided into two categories:

User data - stores actual data contained in files
Metadata - stores file system structural information such as superblock, inodes, directories

******************

Understanding UNIX / Linux filesystem Superblock::

Unix / Linux filesystem blocks

The blocks used for two different purpose:

Most blocks stores user data aka files (user data).
Some blocks in every file system store the file system's metadata. So what the hell is a metadata?
In simple words Metadata describes the structure of the file system. Most common metadata structure are superblock, inode and directories. Following paragraphs describes each of them.

Superblock

Each file system is different and they have type like ext2, ext3 etc. Further each file system has size like 5 GB, 10 GB and status such as mount status. In short each file system has a superblock, which contains information about file system such as:

File system type
Size
Status
Information about other metadata structures
If this information lost, you are in trouble (data loss) so Linux maintains multiple redundant copies of the superblock in every file system. This is very important in many emergency situation, for example, you can use backup copies to restore damaged primary super block. Following command displays primary and backup superblock location on /dev/sda3:

# dumpe2fs /dev/hda3 | grep -i superblock

Output:

Primary superblock at 0, Group descriptors at 1-1
Backup superblock at 32768, Group descriptors at 32769-32769
Backup superblock at 98304, Group descriptors at 98305-98305
Backup superblock at 163840, Group descriptors at 163841-163841
Backup superblock at 229376, Group descriptors at 229377-229377
Backup superblock at 294912, Group descriptors at 294913-294913


**********************

Superblock Corrupted :

Q. How can I Recover a bad superblock from a corrupted ext3 partition to get back my data? I'm getting following error:

/dev/sda2: Input/output error
mount: /dev/sda2: can't read superblock


Mount partition using alternate superblock::::::
A)Find out superblock location for /dev/sda2:
dumpe2fs /dev/sda2 | grep superblock
B)Now check and repair a Linux file system using alternate superblock # 32768::
fsck -b 32768 /dev/sda2
C)Now try to mount file system using mount command:
# mount /dev/sda2 /mnt   ............. if does not work then mount with alt super block...
D)Try to browse and access file system:
# cd /mnt
# mkdir test
# ls -l

***********************
# e2fsck -f /dev/sda3

Where,

-f : Force checking even if the file system seems clean.
Please note that If the superblock is not found, e2fsck will terminate with a fatal error. However Linux maintains multiple redundant copies of the superblock in every file system, so you can use -b {alternative-superblock} option to get rid of this problem. The location of the backup superblock is dependent on the filesystem's blocksize:

For filesystems with 1k blocksizes, a backup superblock can be found at block 8193
For filesystems with 2k blocksizes, at block 16384
For 4k blocksizes, at block 32768.
Tip you can also try any one of the following command(s) to determine alternative-superblock locations:
# mke2fs -n /dev/sda3

OR
# dumpe2fs /dev/sda3|grep -i superblock



****************
INODE Definition :

An inode is a data structure on a traditional Unix-style file system such as UFS or ext3. An inode stores basic information about a regular file, directory, or other file system object.


The inode (index node) is a fundamental concept in the Linux and UNIX filesystem. Each object in the filesystem is represented by an inode. But what are the objects? Let us try to understand it in simple words. Each and every file under Linux (and UNIX) has following attributes:

=> File type (executable, block special etc)
=> Permissions (read, write etc)
=> Owner
=> Group
=> File Size
=> File access, change and modification time (remember UNIX or Linux never stores file creation time, this is favorite question asked in UNIX/Linux sys admin job interview)
=> File deletion time
=> Number of links (soft/hard)
=> Extended attribute such as append only or no one can delete file including root user (immutability)
=> Access Control List (ACLs)

All the above information stored in an inode. In short the inode identifies the file and its attributes (as above) . Each inode is identified by a unique inode number within the file system. Inode is also know as index number.



How do I see file inode number?:::
a)ls -i
ls -i /etc/passwd

b)stat:
stat /etc/passwd



Inode application:::

Many commands used by system administrators in UNIX / Linux operating systems often give inode numbers to designate a file. Let us see he practical application of inode number. Type the following commands:
$ cd /tmp
$ touch \"la*
$ ls -l

Now try to remove file "la*

You can't, to remove files having created with control characters or characters which are unable to be input on a keyboard or special character such as ?, * ^ etc. You have to use inode number to remove file

************************
Linux / UNIX Delete or Remove Files With Inode Number:


An inode identifies the file and its attributes such as file size, owner, and so on. A unique inode number within the file system identifies each inode. But, why to delete file by an inode number? Sure, you can use rm command to delete file. Sometime accidentally you creates filename with control characters or characters which are unable to be input on a keyboard or special character such as ?, * ^ etc. Removing such special character filenames can be problem. Use following method to delete a file with strange characters in its name:


2 Steps::::::::::::

1)Find out file inode

First find out file inode number with any one of the following command:

stat {file-name}

OR

ls -il {file-name}

2)Use find command to remove file:

Use find command as follows to find and remove a file:

find . -inum [inode-number] -exec rm -i {} \;
ie
find . -inum 782263 -exec rm -i {} \;

When prompted for confirmation, press Y to confirm removal of the file.


****************************

Server Decommissioning:


The time will come when a built and running server will reach its end of life. This will be due to either the hardware is old and requires changing, the operating system version is out of date and not supported anymore, the services are no longer required or if a physical server - could be changed for a virtual server to reduce hardware resources.

Once the server is no longer required and its running services can be closed, the data on the disks will need to be managed - either the disks attached will be required to be destroyed or the data on the disks to be wiped accordingly. To wipe the data on the disks, this can be achieved by either overwriting all partitions with zeros or random data or if a hardware RAID is employed, run a full initialization on the disks.

Then the task of removing the (physical) server(s) from their racks/cabinets/datacentre and the configuring of network switches, load balancers, firewall rules be actioned as required.

Decommissioning of physical servers is on the increase as more companies are reducing the costs involved with the maintenance and hosting of physical servers and as a result are now employing virtual servers where possible.


*******************
Automount and AutoFS:


Introduction

Dependent on the physical support (hard disks, floppies, cdrom, ...) and/or the operating system, the process of writing of data (not its meaning) is different : this is what we call file system (I hope experts and purists will forgive me for the short cuts ;-) The /etc/fstab file holds the hard mount points to be installed at boot time. Each point corresponds to a place and to a file system (for example, one of your hard disk partitions). Later on, if you want to access other points, only root can use the mount (unless the special option "user" is provided in /etc/fstab) command. As root, you must specify the mount point, what you want to mount and possibly, the file system and some options. A common user not having, (fortunately ;-) the same rights, won't be able to access all the data.

Mount and fstab man pages give a more accurate and complete description of these commands and concepts.

Both (automount, autofs) allow the administrator to configure all file systems a machine can access, the same way he would using mount. The user can then access these systems in a fully transparent manner, without worrying about how the kernel will answer his request.

Description

The pair (automount, autofs) can be seen as the client/server model used in networking. A server is running, waiting for a request. When a request comes, the server autoduplicates, one part answering the request and the other one waiting for new requests.

Here, autofs plays the role of the waiting server and automount the one of the "duplicated server". The requests are hold in configuration files.

autofs

Usually, autofs is launched at boot time and is closed at shutdown (or reboot) time. However, the administrator is able to start or stop it "manually".

autofs can have 4 different options :

start :
 as its name says, starts the proccess. When starting, autofs looks for "maps" (maps specifying mount points) in the configuration file /etc/auto.master. Then it starts automount for every mount point. Next, autofs looks for NIS maps (we won't say more about yellow pages in this article);
stop :  stops autofs and all automounts ;
status :  displays the present configuration and all running automounts ;
reload :  rereads the auto.master map and kills the no more appearing automounts and starts those for new mount points. Note that changes made in the maps are taken into account at next startup. On the other hand, the changes made in auto.master imply an autofs restart.


In short, autofs is nothing else but a script consulting auto.master before starting the automounts associated to each described mount point.
automount

automout works from an initial mount point (the one found by autofs to start automount) and from a new map describing the features of this initial mount point. The map associated to the automount will hold all required information for mounting the file system automatically (hence the name ;-). This automatic mount is done as soon as someone tries to accesses something in the directory tree starting from the mount point.

Next, the file systems are automatically "unmount" after an inactive period (the default time out is 5 minutes).

Configuration

The configuration is done with the help of 2 files. The auto.master file, holding the mount points, and a mount point file providing the system options for this specific mount point.
/etc/auto.master file

Here, we describe the maps using Sun format. The other format (hesiod) can't handle the syntax described below.

/etc/auto.master is the autofs's main file. Each line describes a mount point and refers to the file containing the file system's descriptions having to be mounted from that point. A last field allows to pass options to the mount to be started.

The line syntax is  :
mount-point map-for-the-associated-automount [ -mount-options-separated-by-comma ]

Example :
/home	/etc/auto.home
/misc	/etc/auto.misc	--timeout 60
/mnt	yp:mnt.map	-intr,nosuid,nodev


This configures 3 mount points, /home, /misc and /mnt. To access files in /misc, automount will read the file /etc/auto.misc to find the mount options and the key associated to the file system.
The last 2 lines hold options. They are described in the mount man page and are standardized. As shown in the last line of the example, autofs and automount are fully compatible with yellow pages maps (NIS and others).

Automount's maps

The syntax of this file is almost the same as the auto.master one (it's normal since it's a map too ;-) :
key [ -mount-options-separated-by-comma ] place

The key represents an identifier for the file system under the mount point . The place is the true physical location.
Let's go back to the previous example with an /etc/auto.misc file :

kernel	-ro,soft,intr	ftp.kernel.org:/pub/linux
cdrom	-fstype=iso9660,ro	:/dev/cdrom
floppy	-fstype=auto	:/dev/fd0
windoz	-fstype=vfat	:/dev/hda1
The absolute path to a file is then :
/mount-point/key/path/file

Then to read my windows config.sys, I just have to execute "cat /misc/windoz/config.sys" and it accesses the /dev/hda1 device.

We can note that the first line of the file refers to an NFS exported directory (just to show the flexible use of automount and autofs ;-)

Last details

First, we must note that when we try to reach a file in one of the maps, the path completion is not working as usual. To avoid typing the full path, we often use the TAB key automatic completion. If the map is not loaded, the key doesn't work. Going back to the previous example, if I want to access the same config.sys and I do :
%1 > cat /misc/ <TAB >
I will only get a disappointment "beep" since it will find nothing to complete (don't mistake it with the happy "beep" issued when there are several choices - to distinguish them, you have to type a <CTRL+D> and you can see all the available completions appearing, if they are some).

As a matter of fact, this behavior is quite normal. Obviously, when you try to use completion, the directory's content is scanned, then you watch what matches the provided symbols. In that case, the /misc directory is empty since the goal is to mount the file system only when accessing it. On the other hand, you can't use completion until the mount point is automatically cancelled (i.e. when no more resource is required from this file system and the time out is over).

Let's have a look at the result of a mount before and after the access to a map, considering the previous /etc/auto.master, with a single mount : 

/dev/hda6 on / type ext2 (rw)
none on /proc type proc (rw)
/dev/hda9 on /home type ext2 (rw)
none on /dev/pts type devpts (rw,gid=5,mode=620)
automount(pid362) on /home type autofs (rw,fd=5,pgrp=362,minproto=2,maxproto=3)
automount(pid364) on /misc type autofs (rw,fd=5,pgrp=364,minproto=2,maxproto=3)
automount(pid366) on /mnt type autofs (rw,fd=5,pgrp=366,minproto=2,maxproto=3)

We notice that there is a daemon (deamon - roughly a system process) for each mount point. Furthermore, each associated "type" is "autofs". After having viewed my /misc/windoz/config.sys, here is the result of the mount : 

/dev/hda6 on / type ext2 (rw)
none on /proc type proc (rw)
/dev/hda9 on /home type ext2 (rw)
none on /dev/pts type devpts (rw,gid=5,mode=620)
automount(pid362) on /home type autofs (rw,fd=5,pgrp=362,minproto=2,maxproto=3)
automount(pid364) on /misc type autofs (rw,fd=5,pgrp=364,minproto=2,maxproto=3)
automount(pid366) on /mnt type autofs (rw,fd=5,pgrp=366,minproto=2,maxproto=3)
/dev/hda1 on /misc/windoz type vfat (rw)

The last line holds what we expected :) If we wait 60 seconds without accessing this mount point, it will disappear at next mount.
Conclusion

On a personal computer running Linux and, for example, Windows, automount and autofs are rather useless, except to avoid a frequent root login (what is, in fact, not so bad). Enough to define a map for the FAT and FAT32 partitions and so accessing data.

In the case of a network, the flexibility is even greater since, combined with yellow pages (we will study them in another serie of articles) and NFS among others, we can access everywhere without worrying about where we physically are.

A great benefit of automount is that a failing server will only affect those clients that currently use this specific file server. This can reduce the down time in big companies (with perhaps 10 or more nfs servers) significantly.

******************
LDAP:


LDAP (similar to AD)
create users on 1 server and other servers refer this server for auth
Save user data on 1 server (other servers can pull data using nfs)

Protocol for reading and writing directories over an IP n/w
Its organised set of records like phone dir.
phone companies usually use it.
LDAP is lightweight , uses TCPIP (tcp/389 udp/389)
Its a protocol to query

LDAP dir service is based on client-server model

LDAP stands for Lightweight Directory Access Protocol. As the name suggests, it is a lightweight client-server protocol for accessing directory services, specifically X.500-based directory services. LDAP runs over TCP/IP or other connection oriented transfer services

A directory is similar to a database, but tends to contain more descriptive, attribute-based information

**************************
LDAP:

LDAP stands for Lightweight Directory Access Protocol. As the name suggests, it is a lightweight client-server protocol for accessing directory services, specifically X.500-based directory services. LDAP runs over TCP/IP or other connection oriented transfer services. LDAP is defined in RFC2251 "The Lightweight Directory Access Protocol (v3).

A directory is similar to a database, but tends to contain more descriptive, attribute-based information. The information in a directory is generally read much more often than it is written. Directories are tuned to give quick-response to high-volume lookup or search operations. They may have the ability to replicate information widely in order to increase availability and reliability, while reducing response time. When directory information is replicated, temporary inconsistencies between the replicas may be OK, as long as they get in sync eventually.

There are many different ways to provide a directory service. Different methods allow different kinds of information to be stored in the directory, place different requirements on how that information can be referenced, queried and updated, how it is protected from unauthorized access, etc. Some directory services are local, providing service to a restricted context (e.g., the finger service on a single machine). Other services are global, providing service to a much broader context.

1.2. How does LDAP work ?

LDAP directory service is based on a client-server model. One or more LDAP servers contain the data making up the LDAP directory tree or LDAP backend database. An LDAP client connects to an LDAP server and asks it a question. The server responds with the answer, or with a pointer to where the client can get more information (typically, another LDAP server). No matter what LDAP server a client connects to, it sees the same view of the directory; a name presented to one LDAP server references the same entry it would at another LDAP server. This is an important feature of a global directory service, like LDAP.

1.3. LDAP backends, objects and attributes


The LDAP server daemon is called Slapd. Slapd supports a variety of different database backends which you can use.

They include the primary choice BDB, a high-performance transactional database backend; LDBM, a lightweight DBM based backend; SHELL, a backend interface to arbitrary shell scripts and PASSWD, a simple backend interface to the passwd(5) file.

BDB utilizes Sleepycat Berkeley DB 4. LDBM utilizes either Berkeley DB or GDBM.

BDB transactional backend is suited for multi-user read/write database access, with any mix of read and write operations. BDB is used in applications that require:

Transactions, including making multiple changes to the database atomically and rolling back uncommitted changes when necessary.

Ability to recover from systems crashes and hardware failures without losing any committed transactions.

In this document I assume that you choose the BDB database.

To import and export directory information between LDAP-based directory servers, or to describe a set of changes which are to be applied to a directory, the file format known as LDIF, for LDAP Data Interchange Format, is typically used. A LDIF file stores information in object-oriented hierarchies of entries. The LDAP software package you're going to get comes with an utility to convert LDIF files to the BDB format

A common LDIF file looks like this:

dn: o=TUDelft, c=NL
o: TUDelft
objectclass: organization
dn: cn=Luiz Malere, o=TUDelft, c=NL
cn: Luiz Malere
sn: Malere
mail: malere@yahoo.com
objectclass: person
As you can see each entry is uniquely identified by a distinguished name, or DN. The DN consists of the name of the entry plus a path of names tracing the entry back to the top of the directory hierarchy (just like a tree).

In LDAP, an object class defines the collection of attributes that can be used to define an entry. The LDAP standard provides these basic types of object classes:

Groups in the directory, including unordered lists of individual objects or groups of objects.

Locations, such as the country name and description.

Organizations in the directory.

People in the directory.

An entry can belong to more than one object class. For example, the entry for a person is defined by the person object class, but may also be defined by attributes in the inetOrgPerson, groupOfNames, and organization objectclasses. The server's object class structure (it's schema) determines the total list of required and allowed attributes for a particular entry.

Directory data is represented as attribute-value pairs. Any specific piece of information is associated with a descriptive attribute.

For instance, the commonName, or cn, attribute is used to store a person's name . A person named Jonas Salk can be represented in the directory as

cn: Jonas Salk
Each person entered in the directory is defined by the collection of attributes in the person object class. Other attributes used to define this entry could include:

givenname: Jonas
surname: Salk
mail: jonass@airius.com
Required attributes include the attributes that must be present in entries using the object class. All entries require the objectClass attribute, which lists the object classes to which an entry belongs.

Allowed attributes include the attributes that may be present in entries using the object class. For example, in the person object class, the cn and sn attributes are required. The description, telephoneNumber, seeAlso, and userpassword attributes are allowed but are not required.

Each attribute has a corresponding syntax definition. The syntax definition describes the type of information provided by the attribute, for instance:

bin binary.

ces case exact string (case must match during comparisons).

cis case ignore string (case is ignored during comparisons).

tel telephone number string (like cis but blanks and dashes `- ' are ignored during comparisons).

dn distinguished name.

Note: Usually objectclass and attribute definitions reside on schema files, on the subdirectory schema under the OpenLDAP installation home.


Chapter 2. Installing the LDAP Server::


Five steps are necessary to install the server:

1)Install the pre-required packages (if not already installed).
((OpenSSL TLS Libraries,Kerberos Authentication Services,Cyrus's Simple Authentication and Security Layer Libraries,Database Software,Threads,TCP Wrappers (IP level access control filters))

2)Download the server (http://www.openldap.org)

3)Unpack the software.
(First copy the package to a desirable directory, for example /usr/local. Next use the following command:

tar xvzf openldap-2.2.5.tgz)

4)Configure the Makefiles.
./configure --help

5)Build the server.
First build the dependencies, using the command:
make depend

Build the server after that, using the command:
make

To ensure a correct build, you should run the test suite (it only takes a few minutes):
make test

Now install the binaries and man pages. You may need to be superuser to do this (depending on where you are installing things):
su root -c 'make install'


************************************************************************

Cluster Added info:


http://schlutech.com/2011/07/demystifying-high-availability-linux-clustering-technologies/

Development of heartbeat has slowed down and so now (according to pacemaker’s website) corosync is considered the superior service. Heartbeat is still available though; and arguably simpler. So when developing your HA cluster you (at this point) know of two clustering configurations to choose from: "Heartbeat + Pacemaker", or "Corosync + Pacemaker".

cluster designer/administrator has an option of using "Corosync + Pacemaker", or "Corosync + OpenAIS + Pacemaker" depending on their needs. OpenAIS does not function without Corosync. Similarly Pacemaker does not function without either Corosync or Heartbeat.


In Steps Red Hat:::

If there is one thing that can be said of Free/Open Software it is that it certainly encourages competition and innovation. Insomuch an alternative to the Cluster Layering Models discussed above, Red Hat developed there own clustering suite.

The first component of the Red Hat Cluster Suite is cman (Cluster Manager.) The cman application is not a cluster messaging mechanism like heartbeat nor corosync. As a matter of fact it simply adds functionality to (and is dependent upon) the corosync/OpenAIS stack.

The second major component of the Red Hat Cluster Suite is an application known as the “Resource Group Manager” (rgmanager.) The rgmanager application is a fully functional replacement for pacemaker intended to work exclusively with Red Hat’s Cluster Manager. So now you have another clustering option consisting of Corosysnc + OpenAIS + CMan + RGManager — aka the Red Hat Cluster Suite.

Openais :

The Application Interface Specification (AIS) is an API and set of policies for developing applications that maintain service during faults. The OpenAIS Standards Based Cluster Framework is an OSI-certified implementation of the Service Availability Forum AIS. The openais package contains the openais executive, openais service handlers, default configuration files and init script.

Corosync :

The Corosync Cluster Engine is an open source project licensed under the new BSD License derived from the OpenAIS project. The mission of the Corosync effort is to develop, release, and support a community-defined, de facto open source cluster executive for use by multiple open source and commercial cluster projects or products.

The Corosync Cluster Engine is a group communication system with additional features for implementing high availability within applications.

Heartbeat :

Heartbeat is a daemon that provides cluster infrastructure (communication and membership) services to its clients. This allows clients to know about the presence (or disappearance!) of peer processes on other machines and to easily exchange messages with them.
In order to be useful to users, the Heartbeat daemon needs to be combined with a cluster resource manager (CRM) which has the task of starting and stopping the services (IP addresses, web servers, etc.) that cluster will make highly available. Pacemaker is the preferred cluster resource manager for clusters based on Heartbeat.


Pacemaker :

In order to be useful to users, the Heartbeat daemon needs to be combined with a cluster resource manager (CRM) which has the task of starting and stopping the services (IP addresses, web servers, etc) that cluster will make highly available.
Heartbeat originally came with a primitive resource manager, however this is only capable of managing 2 nodes and could not detect resource-level failures.
A new resource manager which addressed these limitations and more was written for Heartbeat 2.0.0.
In 2007 the new resource manager was spun-off to become in order to better support additional cluster stacks (such as Corosync)...


---> Pacemaker is a cluster resource manager. It achieves maximum availability for your cluster services (aka. resources) by detecting and recovering from node and resource-level failures by making use of the messaging and membership capabilities provided by your preferred cluster infrastructure (either Corosync or Heartbeat).
Pacemaker’s key features include:
Detection and recovery of node and service-level failures
Storage agnostic, no requirement for shared storage
Resource agnostic, anything that can be scripted can be clustered
Supports STONITH for ensuring data integrity
Supports large and small clusters
Supports both quorate and resource driven clusters
Supports practically any redundancy configuration
Automatically replicated configuration that can be updated from any node
Ability to specify cluster-wide service ordering, colocation and anti-colocation
Support for advanced service types
Clones: for services which need to be active on multiple nodes
Multi-state: for services with multiple modes (eg. master/slave, primary/secondary)
Unified, scriptable, cluster management tools.
******************************

iscsi:
IP based protocol used to connect host and storage
ENcapsulates SCSI commands and data into IP Packet and transports them using TCP/IP.

Components of iSCSI:
1)iSCSI initiator: (eg : iSCSI HBA)
2)iSCSI target : storage array with iSCSI port
               :iSCSI gateway - enables communication with FC storage array.
3)IP network


iSCSI Discovery :

for iSCSI communication, initiator must discover location and name of target on network.

iSCSI discovery takes place in 2 ways :

1) SendTargets discovery:
Initiator is manually configured with the target's n/w protocol.
Initiator issues SendTargets command;target responds with required parameters

2) Internet Storage Name Service (iSNS) :
Initiator and targets register themselves with iSNS server
initiator can query iSNS server for a list of available targets.


iSCSI Name:
iSCSI name is a unique iSCSI identifier that is used to identify initiators and targets within an iSCSI network 

Two common types of iSCSI names :
1) iqn (iSCSI qualified name) : iqn.2008-02.com.example:optional_string
2) eui (Extended unique identifier) : qui.0300732A32


*******************************
Q)

Backup Linux Using dd Command : (dd = convert and copy a file)

dd if=/dev/sda of=/dev/sdb conv=noerror,sync

“if” represents inputfile, and “of” represents output file. So the exact copy of /dev/sda will be available in /dev/sdb.
If there are any errors, the above command will fail. If you give the parameter “conv=noerror” then it will continue to copy if there are read errors.
Input file and output file should be mentioned very carefully, if you mention source device in the target and vice versa, you might loss all your data.
In the copy of hard drive to hard drive using dd command given below, sync option allows you to copy everything using synchronized I/O.

**********************

Q)

rsync:rsync stands for remote sync.

rsync is used to perform the backup operation in UNIX / Linux.

rsync utility is used to synchronize the files and directories from one location to another in an effective way. Backup location could be on local server or on remote server.

Syntax

$ rsync options source destination

*******************************************************


Q)
dmesg :


When the system boots up, it prints number of messages on the screen that displays information about the hardware devices that the kernel detects during boot process.

These messages are available in kernel ring buffer and whenever the new message comes the old message gets overwritten. You could see all those messages after the system bootup using the dmesg command.

dmesg -c (Clear the ring buffer contents after printing , C is to directly clear)

dmesg | grep "L2 cache" -----> to check time : grep "L2 cache" kern.log.1

dmesg | less

dmesg to show all serial ports:
dmesg | grep -i tty

lists all references to USB devices in the kernel messages
dmesg | grep -i usb

show how much physical memory (i.e., RAM) is available on the system:

dmesg | grep -i memory

checks to confirm that the HDD(s) is running in DMA (direct memory access) mode

dmesg | grep -i dma

Display hardware information related to Ethernet port eth0

dmesg | grep -i eth0

accidentally deleted dmesg messages by running dmesg -c? Dont worry these messages are stored in /var/log/kern.log or /var/log/dmesg files.

The output of dmesg is maintained in the log file /var/log/dmesg

cat /var/log/dmesg | less

**************************

Q)

RPM Command:

Installing a RPM package Using rpm -ivh  

Query all the RPM Packages using rpm -qa

Query a Particular RPM Package using rpm -q : 

rpm -q MySQL
package MySQL is not installed

Which RPM package does a file belong to? – Use rpm -qf :

 rpm -qf /usr/bin/mysqlaccess
MySQL-client-3.23.57-1


Information about Installed RPM Package using rpm -qi:

# rpm -qi MySQL-client


List the Dependency Packages using rpm -qRP :

rpm -qRp MySQL-client-3.23.57-1.i386.rpm
/bin/sh
/usr/bin/perl

Upgrading a RPM Package using rpm -Uvh

Verifying all the RPM Packages using rpm -Va

**************************
Q) Task: Find Out Owner Of a Process

Use the following command to find out the owner of a process PID called 3813:
# ps aux | grep 3813

************
Q)

 Find Out Current Working Directory Of a Process

pwdx 3813

Sample outputs:
3813: /home/vivek
****************
Q)

to list the open ports and the process that owns them :
lsof -i

netstat -tulpn | less

to see which process is bound to port 22

lsof -i :22

************

Q)

Find Out Which Process Is Listening Upon a Port :

netstat - a command-line tool that displays network connections, routing tables, and a number of network interface statistics.

fuser - a command line tool to identify processes using files or sockets.

lsof - a command line tool to list open files under Linux / UNIX to report a list of all open files and the processes that opened them.

/proc/$pid/ file system - Under Linux /proc includes a directory for each running process (including kernel processes) at /proc/PID, containing information about that process, notably including the processes name that opened port.


1)
netstat -tulpn
netstat -tulpn | grep :80

2)
fuser command

Find out the processes PID that opened tcp port 7000, enter:
# fuser 7000/tcp

Sample outputs:
7000/tcp:             3813


3)

lsof -i :portNumber
lsof -i tcp:portNumber
lsof -i udp:portNumber
lsof -i :80
lsof -i :80 | grep LISTEN

(((  linux common network utilities/tools ------- netstat -rn and resolv.conf : we will see the gateway
ping , telnet , netstat traceroute , nslookup and dig )))


4)

Discover an Open Port Which I Don't Recognize At All

The file /etc/services is used to map port numbers and protocols to service names. Try matching port numbers:
$ grep port /etc/services
$ grep 443 /etc/services

Sample outputs:

https		443/tcp				# http protocol over TLS/SSL
https		443/udp




***********************************

1)list all ports:
netstat -a
netstat -at
netstat -au

2)List Sockets which are in Listening State :
netstat -l
netstat -lt
netstat -lu
netstat -lx

3)Show the statistics for each protocol :

netstat -s
netstat -su
netstat -st

4)Display PID and program names in netstat output using netstat -p

netstat -pt

5)Don’t resolve host, port and user name in netstat output ::::

netstat -an

If you don’t want only any one of those three items ( ports, or hosts, or users ) to be resolved, use following commands.

# netsat -a --numeric-ports

# netsat -a --numeric-hosts

# netsat -a --numeric-users

6)

Print netstat information continuously
netstat -c

7)

Find the non supportive Address families in your system

netstat --verbose

8)

Display the kernel routing information using:

netstat -r
Note: Use netstat -rn to display routes in numeric format without resolving for host-names.


9)

Find out on which port a program is running :

netstat -ap | grep ssh

Find out which process is using a particular port:

netstat -an | grep ':80'

10)

 Show the list of network interfaces

netstat -i

netstat -ie ( extended info )

*************************************

Q)

Fuser:

gives information about file user or the process that is currently using the file or directory.


1) Check processes using the current directory with verbose\

fuser -v .


2) Check processes using an executable

ps -aef | grep firefox

3) To output process owner use -u

fuser -uv .

4)

List process IDs and login names of processes that have the password ?le open.
fuser -u /etc/passwd

5)

Simply typing lsof will provide a list of all open files belonging to all active processes.


6)
List processes which opened a specific file:

lsof /var/log/syslog

7)
List opened files under a directory

lsof +D /var/log/

(You can list the processes which opened files under a specified directory using ‘+D’ option. +D will recurse the sub directories also. If you don’t want lsof to recurse, then use ‘+d’ option.)

8)

List opened files based on process names starting with :

lsof -c ssh -c init

9)

List processes using a mount point

Sometime when we try to umount a directory, the system will say “Device or Resource Busy” error. So we need to find out what are all the processes using the mount point and kill those processes to umount the directory. By using lsof we can find those processes :

lsof /home (or) lsof +D /home/


10)

List files opened by a specific user

In order to find the list of files opened by a specific users, use ‘-u’ option.

lsof -u jitesh


to see list of files except some 1 or 2 :

lsof -u ^jitesh (except jitesh)


List all open files by a specific process :

lsof -p 111


When you want to kill all the processes which has files opened by a specific user, you can use ‘-t’ option to list output only the process id of the process, and pass it to kill as follows  :

kill -9 `lsof -t -u jitesh`


Combine more list options using OR/AND :


The below command uses two list options, ‘-u’ and ‘-c’. So the command will list process belongs to user ‘lakshmanan’ as well as process name starts with ‘init’.

lsof -u lakshmanan -c init

But when you want to list a process belongs to user ‘lakshmanan’ and the process name starts with ‘init’, you can use ‘-a’ option

lsof -u lakshmanan -c init -a


list all network connections:

lsof -i

List all network files in use by a specific process :

lsof -i -a -p 234

List processes which are listening on a particular port :

lsof -i :25

List all TCP or UDP connections

lsof -i tcp; lsof -i udp;

list all NFS files used by user ‘lakshmanan’\

lsof -N -u lakshmanan -a

******************************************

Q)

modprobe Command Examples to View, Install, Remove Modules :

List Available Kernel Modules :
modprobe -l | less

List Currently Loaded Modules :
lsmod | less

****************
Q)

Ethtool examples :

ethtool eth0

Full duplex : Enables sending and receiving of packets at the same time. This mode is used when the ethernet device is connected to a switch.
Half duplex : Enables either sending or receiving of packets at a single point of time. This mode is used when the ethernet device is connected to a hub.
Auto-negotiation : If enabled, the ethernet device itself decides whether to use either full duplex or half duplex based on the network the ethernet device attached to


Change NIC Parameter Using ethtool Option -s autoneg :

ifdown eth0
ethtool  -s eth0 autoneg off
ifup eth0

Change the Speed of Ethernet Device
ethtool -s eth0 speed 100 autoneg off
ifup eth0

Display Ethernet Driver Settings :

ethtool -i eth0

Display Auto-negotiation, RX and TX of eth0 :
ethtool -S eth0

Use ethtool option -p, which will make the corresponding LED of physical port to blink :

ethtool -p eth0


Make Changes Permanent After Reboot :::

vim /etc/sysconfig/network/ifcfg-eth-id
POST_UP_SCRIPT='eth1'

Then, create a new file scripts/eth1 as shown below under /etc/sysconfig/network directory. Make sure that the script has execute permission and ensure that the ethtool utility is present under /sbin directory.

# cd /etc/sysconfig/network/

# vim scripts/eth1
#!/bin/bash
/sbin/ethtool -s duplex full speed 100 autoneg off

******************************

Q) 2 Methods To Change TimeZone in Linux ::

1)Change TimeZone Using /etc/localtime File :

assume that your current timezone is UTC as shown below. You would like to change this to Pacific Time :

date
Mon Sep 17 22:59:24 UTC 2010

On some distributions (for example, CentOS), the timezone is controlled by /etc/localtime file.
Delete the current localtime file under /etc/ directory

cd /etc
rm localtime

All US timezones are located under under the /usr/share/zoneinfo/US directory :

ls /usr/share/zoneinfo/US/

Link the Pacific file from the above US directory to the /etc/localtime directory as shown below :

cd /etc
ln -s /usr/share/zoneinfo/US/pacific localtime   <------ good


2)
Change TimeZone Using /etc/timezone File :

For example, your current timezone might be US Eastern time (New York) as shown below.

# cat /etc/timezone
America/New_York

To change this to US Pacific time (Los Angeles), modify the /etc/timezone file as shown below.

# vim /etc/timezone
America/Los_Angeles

Also, set the timezone from the command line using the TZ variable.
# export TZ=America/Los_Angeles

*****************************

2 Ways to Add Swap Space Using dd, mkswap and swapon :

You can either use a dedicated hard drive partition to add new swap space, or create a swap file on an existing filesystem and use it as swap space :

check How much swap space is currently used by the system?

free -k
swapon -s
cat /proc/swaps


Method 1: Use a Hard Drive Partition for Additional Swap Space :

If you have an additional hard disk, (or space available in an existing disk), create a partition using fdisk command. Let us assume that this partition is called /dev/sdc1:

Now setup this newly created partition as swap area using the mkswap command as shown below.

mkswap /dev/sdc1

Enable the swap partition for usage using swapon command as shown below.
swapon /dev/sdc1

To make this swap space partition available even after the reboot, add the following line to the /etc/fstab file.

cat /etc/fstab
/dev/sdc1               swap                    swap    defaults        0 0


Verify  ----> Using : swapon -s 

Note: In the output of swapon -s command, the Type column will say “partition” if the swap space is created from a disk partition.  <---------- Good

Method 2: Use a File for Additional Swap Space :

If you don’t have any additional disks, you can create a file somewhere on your filesystem, and use that file for swap space.

The following dd command example creates a swap file with the name “myswapfile” under /root directory with a size of 1024MB (1GB).

dd if=/dev/zero of=/root/myswapfile bs=1M count=1024
1024+0 records in
1024+0 records out

# ls -l /root/myswapfile
-rw-r--r--    1 root     root     1073741824 Aug 14 23:47 /root/myswapfile


Change the permission of the swap file so that only root can access it.

# chmod 600 /root/myswapfile
Make this file as a swap file using mkswap command.

# mkswap /root/myswapfile
Setting up swapspace version 1, size = 1073737 kB
Enable the newly created swapfile.

# swapon /root/myswapfile
To make this swap file available as a swap area even after the reboot, add the following line to the /etc/fstab file.

# cat /etc/fstab
/root/myswapfile               swap                    swap    defaults        0 0


Verify : swapon -s 


If you don’t want to reboot to verify whether the system takes all the swap space mentioned in the /etc/fstab, you can do the following, which will disable and enable all the swap partition mentioned in the /etc/fstab

# swapoff -a

# swapon -a


**********************************

Q)

sysctl - configure kernel parameters at runtime :

The /sbin/sysctl command is used to view, set, and automate kernel settings in the /proc/sys/ directory

The parameters available are those listed under /proc/sys/.
Procfs is required for sysctl(8) support in Linux. 
You can use sysctl(8) to both read and write sysctl data.  

vi /etc/sysctl.conf (or) /proc/sys/


Q)

Disable Ping :

1)Disable ping reply Temporarily :

echo "1" > /proc/sys/net/ipv4/icmp_echo_ignore_all

(Please note that this setting will be erased after the reboot)

To enable the ping reply back, set the value to “0" :

echo "0" > /proc/sys/net/ipv4/icmp_echo_ignore_All


2)

Disable ping reply Permanently :

Edit the sysctl.conf file and add the following line ::

net.ipv4.icmp_echo_ignore_all = 1

sysctl -p (To enforce this setting immediately)


************************

ipv4: (allows only approximately 4.3 billion addresses)

IPv4 is a connectionless protocol for use on packet-switched networks. It operates on a best effort delivery model, in that it does not guarantee delivery, nor does it assure proper sequencing or avoidance of duplicate delivery. These aspects, including data integrity, are addressed by an upper layer transport protocol, such as the Transmission Control Protocol (TCP).

IPv4 uses 32-bit (four-byte) addresses, which limits the address space to 2 (power 32) addresses

As addresses were assigned to users, the number of unassigned addresses decreased. IPv4 address exhaustion occurred on February 3, 2011



Of the approximately four billion addresses allowed in IPv4, three ranges of address are reserved for use in private networks. These ranges are not routable outside of private networks, and private machines cannot directly communicate with public networks. They can, however, do so through network address translation.
The following are the three ranges reserved for private networks (RFC 1918):


Name	Address range	Number of addresses	Classful description	Largest CIDR block
24-bit block	10.0.0.0–10.255.255.255	16777216	Single Class A	10.0.0.0/8
20-bit block	172.16.0.0–172.31.255.255	1048576	Contiguous range of 16 Class B blocks	172.16.0.0/12
16-bit block	192.168.0.0–192.168.255.255	65536	Contiguous range of 256 Class C blocks	192.168.0.0/16


IPV6 :

Internet Protocol version 6 (IPv6) is the latest revision of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion.

IPv6 is intended to replace IPv4, which still carries the vast majority of Internet traffic as of 2013

IPv6 uses a 128-bit address, allowing 2 (power 128)


IPv6 does not specify interoperability features with IPv4, but essentially creates a parallel, independent network. Exchanging traffic between the two networks requires translator gateways or other transition technologies.

************************

Q) Default Gateway :

In computer networking, a gateway is a node (a router) on a TCP/IP network that serves as an access point to another network. A default gateway is the node on the computer network that the network software uses when an IP address does not match any other routes in the routing table.

The default gateway commonly connects the internal networks and the outside network (Internet)

In other words, a default gateway provides an entry point and an exit point in a network.

**************************

PING :

1. Increase or Decrease the Time Interval Between Packets

ping -i 5 IP 

ping -i 0.5 IP

2. Check whether the local network interface is up and running


ping 0 /localhost

3. Send N packets and stop

ping -c 5 google.com

4. Show Version and Exit

ping -V

5. Flood the network

ping -f localhost

6.6. Audible ping

ping -a IP

7.change ping packet size

Example: Change the default packet size from 56 to 100

ping -s 100 localhost

8. Timeout -w

ping -w 5 localhost


************************
Q)

Safe Reboot Of Linux Using Magic SysRq Key :

It is often used to recover from freezes, or to reboot a computer without corrupting the filesystem

The key combination consists of Alt+SysRq+commandkey. In many systems the SysRq key is the printscreen key

First, you need to enable the SysRq key, as shown below.

echo "1" > /proc/sys/kernel/sysrq

***************************
Q)

Check Ram Size From Redhat Linux Desktop System :

cat /proc/meminfo
free -m
free -g
free -k
top
vmstat 
dmidecode --type memory

*********************************
VI Editor:

vi filename	Creates a new file if it already does not exist, otherwise opens existing file.
vi -R filename	Opens an existing file in read only mode.
view filename	Opens an existing file in read only mode.


k	Moves the cursor up one line.
j	Moves the cursor down one line.
h	Moves the cursor to the left one character position.
l	Moves the cursor to the right one character position.


0 or |	Positions cursor at beginning of line.
$	Positions cursor at end of line.
w	Positions cursor to the next word.
b	Positions cursor to previous word.
(	Positions cursor to beginning of current sentence.
)	Positions cursor to beginning of next sentence.
E	Move to the end of Blank delimited word
{	Move a paragraph back
}	Move a paragraph forward
1G	Move to the first line of the file
G	Move to the last line of the file
nG	Move to nth line of the file
H	Move to top of screen
nH	Moves to nth line from the top of the screen
M	Move to middle of screen
L	Move to botton of screen

******************************

/Proc File System:

Inside the /proc directory, you’ll see two types of content — 
1)numbered directories
2)system information files
/proc is not a real file system, it is a virtual file system
For example : 
if you do ls -l /proc/stat, you’ll notice that it has a size of 0 bytes, but if you do “cat /proc/stat”, you’ll see some content inside the file.


1./proc Directories with names as numbers :

Do a ls -l /proc, and you’ll see lot of directories with just numbers. These numbers represents the process ids, the files inside this numbered directory corresponds to the process with that particular PID.

Following are the important files located under each numbered directory (for each process):

cmdline – command line of the command.
environ – environment variables.
fd – Contains the file descriptors which is linked to the appropriate files.
limits – Contains the information about the specific limits to the process.
mounts – mount related information

Following are the important links under each numbered directory (for each process):

cwd – Link to current working directory of the process.
exe – Link to executable of the process.
root – Link to the root directory of the process.




2. /proc Files about the system information

Following are some files which are available under /proc, that contains system information such as cpuinfo, meminfo, loadavg.

/proc/cpuinfo – information about CPU,
/proc/meminfo – information about memory,
/proc/loadvg – load average,
/proc/partitions – partition related information,
/proc/version – linux version

Some Linux commands read the information from this /proc files and displays it. For example, free command, reads the memory information from /proc/meminfo file, formats it, and displays it.

************************
FS Structure :

http://www.thegeekstuff.com/2010/09/linux-file-system-structure/

*****************
Crontab : (minute,hour,date,month,day)

Crontab Format:

Field	Description	Allowed Value
MIN	Minute field	0 to 59
HOUR	Hour field	0 to 23
DOM	Day of Month	1-31
MON	Month field	1-12
DOW	Day Of Week	0-6
CMD	Command	        Any command to be executed.

1)07th June 8:30 AM

30 08 07 06 * /home/jitesh/full-backup

2. Schedule a Job For More Than One Instance (e.g. Twice a Day)

This example executes the specified incremental backup shell script (incremental-backup) at 11:00 and 16:00 on every day :

00 11,16 * * * /home/jitesh/bin/incremental-backup


3. Schedule a Job for Specific Range of Time (e.g. Only on Weekdays)

Cron Job everyday during working hours 9am -6pm

00 09-18 * * * /home/jitesh/bin/check-db-status


Cron Job every weekday during working hours :


00 09-18 * * 1-5 /home/jitesh/bin/check-db-status

4.Schedule a Job for Every Minute Using Cron :

* * * * * CMD

5.Schedule a Background Cron Job For Every 10 Minutes

*/10 * * * * /home/ramesh/check-disk-space

************************
VIM:

Text Editor Upward Compatible to VI
Useful for Editing programs
Multilevel Undo, Multi Windows and buffers , Syntax Highlighting , Command line Editing ,
Online Help, Visual Selection


**************************************

The clock that is managed by Linux kernel is not the same as the hardware clock.
When the system starts, it takes the time from the hardware clock

Hardware clock is also called as BIOS clock. You can change the date and time of the hardware clock from the BIOS.

hwclock
hwclock -r
hwclock --show
hwclock --systohc (or) hwclock -w
hwclock - hctosys (or) hwclock -s
----> the date command gets the date and time from the clock managed from Linux kernel
hwclock --set --date "8/11/2013 23:10:45" 
hwclock --adjust (taken from /etc/adjtime)




************************************************

Find Network card(Wired/wireless) details in Linux/Unix

Example 1: Find all your network cards(Ethernet, Gigabit Ethernet, Wireless) names attached to a given system.

ifconfig | cut -c1-8 | sort -u

Example 2 : find how many network connections(network cables are connected to your machine) are active i.e. link is up.

ip link show

or for wireless network card

ethtool interfacename (ethttol wlan0)


Example 3: How can we find out network card speeds so that we can see throughput of the devices attached to a machine.

ethtool eth0 | grep speed


Example 4: How can we get MAC address of a given NIC card?

ethtool -P devicename (ethtool -P eth0)


Example 5: Find IP address assigned to an interface and all interfaces in Linux.

ifconfig interfacename (ifconfig wlan0)

Example 6: How can we find network cared driver details in Linux?

ethtool -i wlan0

Example 7: How can we find network card manufacture details for both Ethernet card and wireless cards

lspci -v | grep -iE 'Wire|Ether'

Example 8: How can we get NIC card mode details like full/half duplex

ethtool eth0 | grep Duplex


Example 9: How about finding my network card is auto negotiation enabled or not in Linux?

ethtool eth0 | grep Auto

Example 10: Find complete network card software details like speed, modes, negotiations, link status etc.

root@linuxnix.com:/home/surendra# ethtool eth0


Example 11: Find complete network card hardware details.

lshw -class network

*******************************************


Activate NIC or network card at boot time for Redhat/CentOS

Open the file In file /etc/sysconfig/network-scripts/devices/ifcfg-eth0 and change ONBOOT value to yes


DEVICE=eth0
HWADDR=52:54:00:60:af:6d
NM_CONTROLLED=yes
ONBOOT=yes
BOOTPROTO=dhcp
TYPE=Ethernet
USERCTL=no
PEERDNS=yes
IPV6INIT=no

Reboot the machine and your NIC interface will be activated at the time of booting.


**************************************************************

How To View Default Gateway In Linux?

1)
To see default gateway by view network interface card file content :
cat /etc/sysconfig/networking/devices/interface-no

2)
Through route command
route

3)
ip route

4)
netstat -nr

Note:For any system/server there will be only one default gateway, if suppose if you assign two default gateways to your machine, your system will not come to know where to send the packets.


**************************

How to Assign Default gateway in linux

1)
route
route add default gw default-gateway

2)
Through GUI tools
Use system-config-network in terminal or GUI to set up default gateway.

*****************************

How To Assign Or Change Hostname In Linux

Temporary way :
Use hostname command to change the host name :
hostname your-system-name (hostname office-laptop.example.com)

Permanent way :

Change hostname in Redhat/CentOS/Fedora : 

vi /etc/sysconfig/network
HOSTNAME=office-laptop.example.com
Save the file and exit


Change hostname in Ubuntu/Debian :

Edit /etc/hostname, just write down what is your hostname in to that file.
#vi /etc/hostname

office-laptop.example.com

*******************************
How To Change The Date In Linux

To change the date use below command
#date MMDDHHMMYYYY.ss

Example1 : I want to change the date to Nov 24 1:36 PM 2010

#date 112413362010

Example2 : I want to change the date to Jan 9th 8:05.04 AM 2010

#date 010908052010.04

*******************************


How To Change The Background Of A Terminal:

Open an terminal> Edit > Profiles>

Now press edit butten > background >background image

Then browse your image then press close. voyala

************************************
How To Switch From One Terminal to Other?

ctrl+alt+function keys for example you want to access 6th terminal you can press ctrl+alt+f6 keys

Simultaneously there is another procedure to access the terminals there is a command chvt(change virtual terminal) is used to change the terminals for example you want to switch to terminal 3 just type the below thing in your shell. 
chvt 3
****************************************
How To View Or List Only Directories In Linux?


Ans : This can be achieved in two ways
1. Through ls command
2. Through find command

Ls –l grep ^d

A. To display all the directories and sub-directories in present directory
#find . -type d

B. Displaying only directories in present directory
#find /root/ -type d –maxdepth 1

C. Displaying just directories in present directry and its sub-directories
#find /root/ -type d –maxdepth 2

*******************************************

How many types of files are there in Linux/Unix and what are they


1. Regular files
2. Directory files
3. Special files(This category is having 5 sub types in it.)

So in practical we have total 7 types of files are there. And in Solaris we have 8 types.

Here are those files type. And you can see the file type indication at leftmost part of “ls -l” command

1. Regular file(-) 
2. Directory files(d) 
Special files 
3. Block file(b) 
4. Character device file(c) 
5. Named pipe file or just a pipe file(p) 
6. Symbolic link file(l) 
7. Socket file(s)


We will learn about different types of files as below sequence for every file type.
1)Defination and information of the file type
2)How to create particular file type
3)How to list/see particular file type

Regular file type Explained in Linux
These are the files which are indicated with “-” in ls -l output at the starting of the line. And these files are.

1. Readable files or
2. A binary files or
3. Image files or
4. Compressed files etc.

How to create them?

Ans : Use touch/vi command and redirection operators etc.

How can we list them in my present working directory?
ls -l | grep ^-
Example listing of regular files :

-rw-r–r– 1 krishna krishna 20986522 2010-01-31 13:48 test.wmv
-rw-r–r– 1 krishna krishna 173448 2010-01-30 21:20 Transformers-Teaser-Wallpaper-310.jpg
-r-xr-xr-x 1 root root 135168 2009-12-12 19:14 VIDEO_TS.VOB
-rw-r–r– 1 krishna krishna 2113536 2009-12-01 13:32 Aditya 365 – Janavule.mp3
-rwxrwxrwx 1 root root 168 2010-02-14 14:12 xyz.sh

Directory file type explained in Linux/Unix
These type of files contains regular files/folders/special files stored on a physical device. And this type of files will be in blue in color with link greater then or equal 2.

How can we list them in my present working directory?

ls -l | grep ^d

Example listing of directories.
drwxr-xr-x 2 surendra surendra 4096 2010-01-19 18:37 bin
drwxr-xr-x 5 surendra surendra 4096 2010-02-15 18:46 Desktop
drwxr-xr-x 2 surendra surendra 4096 2010-01-18 14:36 Documents
drwxr-xr-x 2 surendra surendra 4096 2010-02-13 17:45 Downloads

How to create them?
Ans : Use mkdir command

Block file type explained
These files are hardware files most of them are present in /dev

How to create them?
Ans : Use fdisk command or create virtual partition.

 Linuxnix-free-e-book
How can we list them in my present working directory?
ls -l | grep ^b
Example listing of Block files(for you to see these file, they are located in /dev).

brw-rw—- 1 root disk 8, 1 2010-02-15 09:35 sda1
brw-rw—- 1 root disk 8, 2 2010-02-15 09:35 sda2
brw-rw—- 1 root disk 8, 5 2010-02-15 09:35 sda5

Character device files
Provides a serial stream of input or output.Your terminals are clasic example for this type of files.
How can we list chracter files in my present working directory?
ls -l | grep ^c

Example listing of character files(located in /dev)
crw-rw-rw- 1 root tty 5, 0 2010-02-15 16:52 tty
crw–w—- 1 root root 4, 0 2010-02-15 09:35 tty0
crw——- 1 root root 4, 1 2010-02-15 09:35 tty1

Pipe files explaned
The other name of pipe is a “named” pipe, which is sometimes called a FIFO. FIFO stands for “First In, First Out” and refers to the property that the order of bytes going in is the same coming out. The “name” of a named pipe is actually a file name within the file system.

How to create them?

Ans : Use mkfifo command.

How can we list chracter files in my present working directory?
ls -l | grep ^p

Example listing of pipe files
prw-r—– 1 root root 0 2010-02-15 09:35 /dev/.initramfs/usplash_outfifo
prw-r—– 1 root root 0 2010-02-15 09:35 /dev/.initramfs/usplash_fifo
prw——- 1 syslog syslog 0 2010-02-15 15:38 /var/run/rsyslog/kmsg

Sybolic link files
These are linked files to other files. They are either Directory/Regular File. The inode number for this file and its parent files are same. There are two types of links ie soft and hard link.

How to create them?
Ans : use ln command

How can we list linked files in my present working directory?
ls -l | grep ^l

Example listing of linked files
lrwxrwxrwx 1 root root 24 2010-02-15 09:35 sndstat -> /proc/asound/oss/sndstat
lrwxrwxrwx 1 root root 15 2010-02-15 09:35 stderr -> /proc/self/fd/2
lrwxrwxrwx 1 root root 15 2010-02-15 09:35 stdin -> /proc/self/fd/0
lrwxrwxrwx 1 root root 15 2010-02-15 09:35 stdout -> /proc/self/fd/1

Socket files in Linux
A socket file is used to pass information between applications for communication purpose

How to create them?
Ans : You can create a socket file using socket() system call avialable under

Example
int sockfd = socket(AF_INET, SOCK_STREAM, 0);
You can refer to this socket file using the sockfd. This is same as the file descriptor, and you can use read(), write() system calls to read and write from the socket.

How can we list chracter files in my present working directory?
ls -l | grep ^s

Example listing of socket files.
srw-rw-rw- 1 root root 0 2010-02-15 09:35 /dev/log
srwxrwxrwx 1 root root 0 2010-02-15 10:07 /var/run/cups/cups.sock
srwxrwxrwx 1 root root 0 2010-02-15 09:35 /var/run/samba/winbindd_privileged/pipe
srwxrwxrwx 1 mysql mysql 0 2010-02-15 09:35 /var/run/mysqld/mysqld.sock

A tip for you guys.
How to find your desired type of a file ?

Ans : Use find command with -type option.

For example if you want to find socket file, just use below command.

find / -type s

If you want to find linked file then how?

Find / -type l



*********************************

Difference between char and Block file

Q. What is a char file and a block file? Is there any difference between them?

These both files are related to writing data and reading data from one place to other place. But the difference between them is how they read/write data.


character file: A char file is a hardware file which reads/write data in character by character fashion. Some classic examples are keyboard, mouse, serial printer. If a user use a char file for writing data no other user can use same char file to write data which blocks access to other user. Character files uses synchronise Technic to write data. Of you observe char files are used for communication purpose and they can not be mounted.

Example character files: /dev/autofs, /dev/console, /dev/crash, /dev/lp0, /dev/null, /dev/ppp, /dev/random, /dev/tty etc

ls -l output for a char file:

[root@yumserver dev]# ls -l tty0
crw–w—- 1 root root 4, 0 Dec 31 11:45 tty0
[root@yumserver dev]#



Block file: A block file is a hardware file which read/write data in blocks instead of character by character. This type of files are very much useful when we want to write/read data in bulk fashion. All our disks such are HDD, USB and CDROMs are block devices. This is the reason when we are formatting we consider block size. The write of data is done in asynchronous fashion and it is CPU intensive activity. These devices files are used to store data on real hardware and can be mounted so that we can access the data we written. Have a look at our other post on getting block size of a device.

Block file examples:/dev/loop0, /dev/ram0, /dev/sda1, /dev/sr0.

ls -l output for a block file

[root@yumserver dev]# ls -l sda1
brw-rw—- 1 root disk 8, 1 Dec 31 11:45 sda1
[root@yumserver dev]#


************************************

List all users in Linux:

cat /etc/passwd | cut -d: -f1

we are opening file /etc/passwd with cat command and then redirecting output to a cut command which will cut the first field(-f1) ie user ID field using : as delineated(-d:). As you know passwd file content are separated with colon(:)

**************************************

How to setup two IP address on single NIC in Linux\

This is a small how-to to set up two are more IP address on single LAN card. There are some times which require two IP address to set up so that we can make a Linux box as a router. This can be possible without even having two NIC cards. We can configure two different IP address on single Network Card as shown below.

Setting up 2 IP address on “One” NIC. This example is on ethernet.

STEP 1:Setting up first IP address. Edit /etc/sysconfig/network-scripts/ifcfg-eth0 on Redhat Linux box and give the following entries as shown.

vi /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=eth0
BOOTPROTO=static
IPADDR=192.168.1.10
NETMASK=255.255.255.0
NETWORK=192.168.1.0
ONBOOT=yes

 Linuxnix-free-e-book
STEP 2: Setting up second IP address. Create one file as /etc/sysconfig/network-scripts/ifcfg-eth0:1 and give the entries as below in to this file.

vi /etc/sysconfig/network-scripts/ifcfg-eth0:1

DEVICE=eth0:1
BOOTPROTO=static
IPADDR=192.168.1.11
NETMASK=255.255.255.0
NETWORK=192.168.1.0
ONBOOT=yes

STEP 3: Once you configure above files and save them. Now reload the network service on your machine.

service network reload

STEP 4: Check if you get the IP address assigned to the eth0 and eth0:1 interfaces respectively.

ifconfig

Note1: We can assign virtual IP to the same interface with ifconfig but that one is not permanent so not giving info on that.

Note2: We can assign up to 16 virtual IP address to a single NIC card.



***************************************

Find Sound card details in Linux/Unix

cat /proc/asound/cards (or) /proc/asound/cardX


to see number of cards available

ls -ld /proc/asound/card* | grep ^d

Example 2 : How to get card manufacture details using lspci command.

lspci -v | grep -i audio

Example 3: How can I check how many number of devices attached to sound card like speakers, mic, woofers etc

cat /proc/asound/devices (or) aplay –list-devices

Example 4 : How can I get which Kernel module or sound driver details for my sound card. The module related to sound card is represented snd string in its name. So if we can search /proc/asound/modules file or lsmod command output we can easily find which sound card driver is in use as shown below

lsmod | grep snd

Example 5: How can I find sound card software version in Linux?

cat /proc/asound/version

***************************************

Find PCI hardware details using lspci command in Linux

Example1 : To find all the hardware attached to PCI slots in a given machine.

lspci

Example 2: Not able to distungush different hardwares attached to the machine use -vmm option to display lspci command output.

lspci -vmm

Example 3:  Display PCI slot information in tree format

lspci -t

Example 4: Display PCI devices atached their speeds, IRQ assigned Kernel drivers and modules etc.

lspci -v


*************************************


Find hardware info with lshw, hardinfo, sysinfo Linux/Unix commands


Example1 : Get all the hardware attached to a machine

lshw

Example 2: Display all the BUS information in a system.

lshw -businfo

Example3: We can even use lshw command to get RAM/Memory details as well Cache details such as L1, L2 and L3 levels.

lshw -class memory

*****************

Find RAM slots info in Linux

dmidecode -t 16
dmidecode -t 17
dmidecode -t memory
lshw -class memory


*********************

Get BIOS, Firmware, Hardware And Drivers Details in Linux/Unix

1.Hardware
2.CPU information
3.Drivers installed in Linux machine.

dmidecode 
To get CPU info

#cat /etc/cpuinfo
To get Hardware info

#lshal
or

#lshw
To get PCI info

#lspci
To get USB info

#lsusb

****************

Linux User info gathering commands
MONITORING USERS
User-management is always one of the basic tasks for Linux administrators, here we are going to see some basics user related “Monitoring Commands”.


#finger username
id username
chage -l username
who
w
groups username
***************
Why we can create only up to 4 primary partations?

This is the question which pops in my mind that why not more than that 4 partations

here is the info how many partations we can create

actually in basic hard disk, we can create 4 partition(either primary or extended)

we can create maximum 4 primary
or 3 primary + 1 extended
or 2 primary + 1 extended
or 1 primary + 1 extended
not more then that? why and what is the reason? 
The reason is because of a limitation of the MBR(Master Boot Record- the first sector of the harddisk.)
The MBR is only 512bytes of size, it is needed to store the primary boot loader, and the partition table. Typically, the area reserved for partition table is only 64 bytes. And the partition table entry for one partition is 16 bytes. So, 16×4=64. The space is over. so we cant create more than this

******************
3.How many partition we can create on a hard disk?
Ans : Totally we can create four partitions as below
a.Four primary parathions.
b.Three primary and one extended partition.
c.Two primary and one extended parathion.
d.One primary and one extended parathion.

Note : In extended parathion we can create logical partitions up to 24 in number.

*******************



********************************


Find USB device details in Linux/Unix using lsusb command

Example1: List all the USB ports available
lsusb

Example 2: Check how many USB ports available in your machine so that we can connect USB devices to these ports.
find /dev/bus/

Example 3: Get all the USB ports available and devices connected using verbose mode.

lsusb -v

*************************

Find PCI hardware details using lspci command in Linux

Example1 : To find all the hardware attached to PCI slots in a given machine.

lspci

Example 2: Not able to distungush different hardwares attached to the machine use -vmm option to display lspci command output.

lspci -vmm

Example 3:  Display PCI slot information in tree format

lspci -t

Example 4: Display PCI devices atached their speeds, IRQ assigned Kernel drivers and modules etc.

lspci -v

*************************

lsof

USAGE1 : To see all the open files in system with out filtering which lists all open files belonging to all active processes. 
lsof

lsof -i TCP:ftp  (i =internet)

USAGE3 : To see what files are opened when you execute a command 
#lsof -c httpd (-c is for specifying command.)

USAGE6 : To monitor network, what people are doing with what network services 
#watch lsof -i 

USAGE7 : To see all open INTERNET files 
#lsof -i -U 

USAGE9:To watch all the files accessing by a user in live..?
#watch lsof –u user-name

********************************
Check if a machine runs on 64 bit or 32 bit Processor/Linux OS?
lscpu | grep op-mode
grep -w lm /proc/cpuinfo

grep lm /proc/cpuinfo

If you don’t get any output that indicates its not a 64 bit processor.

Option 1: check with uname command if your OS is 32 bit or 64 bit.

uname -m

uname -p

Option 2: Check with lscpu command as below.

lscpu | grep -i arch


****************************

18TH JULY

/boot folder

/boot folder contains all the boot related info files and folders such as grub.conf, vmlinuz 

/boot/config-2.6.18-194.17.1.el5 file - This config file contains kernel configuration settings

/boot/initrd-2.6.18-194.17.1.el5.img file - Initrd in other words called as Initializing RAM Disk which loads temporary file system in to RAM at the time of booting process before actual file system mounting. This file is loaded by your kernel image file at the time of booting. This initrd image knows only one configuration file /etc/inittab file. The inittab file contains what are run levels, what init has to do in each run level and what is the default runlevel etc.

/boot/vmlinuz-2.6.18-194.17.1.el5 file - vmlinuz is a compressed Linux Kernel image which is used at the time of booting Linux operating system. Vmlinuz stands for virtual memory Linux kernel zipped. /boot/System.map-2.6.18-194.17.1.el5 file: This is a map file used by kernel. This file contains memory location mapped to the kernel variables or functions. Again this file is used by vmlinuz kernel image at the time of booting to set symbol names

/boot/grub/grub.conf or menu.lst file - This file is used for boot loader grub to load grub related configuration so that this is used at the time of booting

**************************

How to mount DVD or CDROM in Linux

If your device is a DVD reader use below command

mount -t iso9660 /dev/dvdrom /media/

If your device is a DVD writer use below command

mount -t iso9660 /dev/dvd-rw /media/

***************************

ISO IMAGE FILES: (DISK IMAGE) 	

1)How to create an ISO image?
As of now I know two methods to create an ISO file. One is through dd command(disk dump) and other is mkisofs command

A)Creating ISO files from cd-rom/dvd by using dd command

#dd if=/dev/cdrom of=/temp/songs.iso
dd if=/dev/dvd of=/temp/songs1.iso # for dvd
dd if=/dev/scd0 of=/temp/songs2.iso # if cdrom is scsi


Let me explain what actually the above command will do. dd is a disk dump command which will present in most of the Linux/nix systems and “if” specifies input file and “of” specifies output file. So we are specifying to dd command what is our input file and where is our output file.
Here my songs.iso file is created in /temp folder.

B)Creating ISO file using mkisofs command.
Method1:Creating an iso image from cd-rom

#mkisofs /dev/cdrom -o /temp/songs3.iso

Method2:Creating an iso image from a folder

Suppose i am having a folder /opt/data which contains some data to be converted in to .iso file, follow below step

#mkisofs -o /temp/songs4.iso /opt/data

2)How to mount an ISO file?
#mount -o loop /temp/songs4.iso /opt/mountpoint

3)How to unmount an ISO file?
#umount -lf /opt/mountpoint

here /opt/mountpoint is the place where my iso file is mounted.

********************************

How to mount ISO image files in Linux?
Access ISO or IMG files locally in Linux :::


Mounting iso images is a bit tricky one step procedure to access image file content. There are hardware files called loop devices which are located in /dev/. These loop hardware files are used to mount already formated files such as ISO, IMG, NRG files for local access. They range from loop0 to loop7. So in other words we can mount total 8 iso/img files in Linux at a time. We can not mount more than these number. Below is the command to mount a iso image file


 
 Linuxnix-free-e-book
My ISO file location and the filename: /var/ftp/rhel.iso

My mount point: /mnt/mymount

Step1: Become root, as mount command should be executed by only root user

sudo su

Step2: Make a directory /mnt/mymount

mkdir /mnt/mymount

Step3: Now mount iso file on to the mount point.

mount -o loop /var/ftp/rhel.iso /mnt/mymount

for img

mount -o loop /var/ftp/rhel.img /mnt/mymount

**************************
Linux directory structure explained:

Today we will see what is /dev folder and its uses. Linux/Unix treat everything as files. One classic example is it treats hardware devices too as files. All hardware files are present in /dev(Device ) folder. If we observe the /dev folder you can find files/folders related to different hardware’s present in the machine.

Below are some hardware files and their uses and explanation.

1. /dev/alarm
This is a hardware file used to keep track of time when system goes hibernation or suspended when it is idle. When your system goes hibernation most of your hardware will be shutdown, HDD rotation is reduced to as low as possible in order to save power. This file is very much useful to keep track of system time, to do calender updates to user etc. You can say in other words that /dev/alarm will have current time status. This file is used in portable devices such as laptops and mobiles mostly. Click here to know more about this file.

2. /dev/autofs
This file is used to mount remote directories locally. This is done automatically when user tries to login by mounting remote directory. The mounting is done by using this hardware file. With out this hardware file we can not do automount in a Linux box.

3. /dev/block folder
This folder is legacy location for your block devices. This is still existing to support legacy applications.

4. /dev/cdrom, /dev/dvd, /dev/cdrw, /dev/dvd-rw etc
These files corresponds to Compact-Disk(/dev/cdrom or /dev/cdrw) hardware or to Digital Versatile Disk(/dev/dvd or /dev/dvd-rw) hardware. These files are required to mount your local CDrom and DVD’s so that you can access the content of the file. Know how to mount your CD/DVD in Linux.

5. /dev/char
This is the folder where char files are located in legacy machines. This is still kept to support legacy applications.

Know what is the difference between char and block file here.

 Linuxnix-free-e-book
6. /dev/console, /dev/tty, /dev/tty1 to /dev/tty63, /dev/ttyS, /dev/ttyS0 to /dev/ttyS31 files and /dev/pts folder
These device files are called as terminals or consoles which are char files used for communication between user and system. /dev/console file is used in Runlevel 1 and none of these terminals are available for access on runlevel 1. tty(Teletype) is a device file to do remote connection so that we can work remotely The name is derived from typewriters which are default communicating devices in early stages of Computers. The terminals range from tty0 to tty63 and serial port terminals(ttys or ttyS) are from 0 to 31. We have pseudo terminals which is used when users login from remote machines, these virtual terminals are kept in /dev/pts folder in most Linux flavors.

7. /dev/loop(/dev/loop0 to /dev/loop7)
A loop device is a Pseudo device useful for mounting Virtual CD(ISO files), HDD etc. The loop devices are useful for mounting already formatted drive and access the data in a folder mounted on different filesystem. Virtual devices are nothing but a hardware file created by kernel/OS so that we can use them as a physical drives. These loop devices are also used by virtual softwares such as KVM, VMWARE to mount CD-ROM’s, HDD(.img files for example) as physical devices in your Virtual machine. Planing to mount your ISO file, know it how to do it here.

8. /dev/sda, /dev/hda etc
These files corresponding to hard disks and storage devices such as USB hard disks, SATA disks and External HDD’s. For Intel machines you may find /dev/hda, /dev/hdb, /dev/hdc, /dev/hdd corresponds to Primary master, Primary slave, Secondary master and Secondary Slave devices. Partitions with in disks are created as /dev/sda1, sda2 etc and so on.

9. /dev/random and /dev/urandom

Used for generating random chars for Kernel purpose.

10. /dev/null and /dev/zero

Used for generating empty files, observing unwanted outputs etc. Click here to know the difference

11. /dev/ppp
This file is used to connect your mobile or GPRS/3G enabled devices to connect and communicate. This is pseudo file which communicates with GPRS enabled hardware file to send data. Click here to know how to connect internet using your GPRS/3G enabled device.\

******************************

Enbable Ip forwarding:
vi /etc/sysctl.conf
net.ipv4.ip_forward = 1
echo 1 > /proc/sys/net/ipv4/ip_forward

*****************************

NETWORK MONITORING/ INFO GATHERING TOOLS IN LINUX.

Network monitoring tools can be divided in to three types :

Traffic monitoring tools(which will do just monitoring traffic in network) .
Network monitoring tools(these tools will do just monitoring such as services and devices).
Network Info gathering tools(these tools will just get you info about your network, tools such as ifconfig, traceroute will come under this).
Some of the tools which we are going to cover in it are:-

1.nmap
2.NGIOS
3.echoping
4.smokeping
5.iptraf
6.MRTG(Multi Router Traffic Grapher)
7.rrdtool
8.ntop
9.mii-tool
10.mii-diag
11.tcpdump
12.mtr

*************************

Creqte files:
cat > file name
vi
touch

*********

DNS FAQ:
What are root name servers?
DNS is a hierarchical in structure where if a server is unable to give answer to host requests, it will send this to its superior servers and so on. This will reach to the upper levels to the initial master servers which are called as root name server which can serve all the answer queries if the host exists in internet.

Why there are only 13 root name servers?
This is due to UDP porotocal size which stores DNS information.

How many TOP level and Country specific domains are there?
There are 20 generic TLD’s (gTLD) and 248 country specific TLD’s(ccTLD) are there.

***************************
top :


l –To display or to hide load average line
t –To display or to hide task/cpu line
1 –To display or hide all other CPU’s
m –to display or to hide RAM and SWAP details
s –To change the time interval for updating top results(value is in sec’s)
R –To sort by PID number
u — Press u then username to get only that user process details
P –To sort by CPU utilization
M –To sort by RAM utilization
c –To display or hide command full path
r –To renice a process, press r then the PID no then the renice value to renice a process.
k –To kill a process, press k then PID number then enter to kill a process
w –To save the modified configuration permanently.
q –To quit the top command.
h –for getting help on top command

***************************




Fencing:
Fencing is the process of isolating a node of a computer cluster or protecting shared resources when a node appears to be malfunctioning.[1][2]

There are two classes of fencing methods, one which disables a node itself, the other disallows access to resources such as shared disks

***************************
for every partition we have a lost+found directory

***************************
To copy file to another system, use scp command:

scp /path/to/file1 user@server1:/tmp
For example, copy /home/vivek/data.txt to server called server1 and into a directory called /home/sales, enter:

scp /home/vivek/data.txt roja@server1:/home/sales

scp vs cp command

The scp copies files between hosts on a network.
The cp copies files between two directories on same host.
*************************

A pioroty is a number given to a process that tells the kernel how many recources to allocate to it
this prioroty number is 0 by default when a process is launched. you can change the defaut prioroty at launch
time by using the nice command. for example "nice -n -10 glxgears" will launch the glxgears program
with a prioroty numbet of -10. Prioroty number range from is from -20 (highest) to 19 (lowest).
the nice command is for changing the default prioroty number when you start a process but if you 
want to change the prioroty number of a process that is already running than you use the renice
command. For example if the process id for glxgears was 2002 than you "renice 17 -p 2002"
would change the prioroty of glxgears to 17.

Nice is a command that can be used to alter a prioroty number but a prioroty number tells the kernel 
how many recources (CPU cycles basically) to allocate to process. 

*****************

Packet Analyzer: 15 TCPDUMP Command Examples :

tcpdump command will work on most flavors of unix operating system. tcpdump allows us to save the packets that are captured, so that we can use it for future analysis. The saved file can be viewed by the same tcpdump command. We can also use open source software like wireshark to read the tcpdump pcap files.


1. Capture packets from a particular ethernet interface using tcpdump -i

In this example, tcpdump captured all the packets flows in the interface eth1 and displays in the standard output:

tcpdump -i eth1


2. Capture only N number of packets using tcpdump -c

tcpdump -c 2 -i eth0

Display Captured Packets in ASCII using tcpdump -A

tcpdump -A -i eth0

4. Display Captured Packets in HEX and ASCII using tcpdump -XX

tcpdump -XX -i eth0

5. Capture the packets and write into a file using tcpdump -w

tcpdump allows you to save the packets to a file, and later you can use the packet file for further analysis.

tcpdump -w 08232010.pcap -i eth0

-w option writes the packets into a given file. The file extension should be .pcap, which can be read by any network protocol
analyzer

6. Reading the packets from a saved file using tcpdump -r

tcpdump -tttt -r data.pcap

7. Capture packets with IP address using tcpdump -n

tcpdump -n -i eth0

*************************
In this quick tip, let us review how to make your process running even after you logout, using nohup.
Nohup stands for no hang up, which can be executed as shown below.

nohup command-with-options &


If you log-out of the shell and login again, you’ll still see the custom-script.sh running in the background.
$ ps aux | grep sathiya
sathiya  12034  0.0  0.1   4912  1080 pts/2    S    14:10   0:00 sh custom-script.sh

**************************
Bg, Fg, &, Ctrl-Z :

1. Executing a background job :

Appending an ampersand ( & ) to the command runs the job in the background :
find / -ctime -1 > /tmp/changed-file-list.txt &

2. Sending the current foreground job to the background using CTRL-Z and bg command

# find / -ctime -1 > /tmp/changed-file-list.txt

# [CTRL-Z]
[2]+  Stopped                 find / -ctime -1 > /tmp/changed-file-list.txt

# bg


3. View all the background jobs using jobs command :
jobs

4.Taking a job from the background to the foreground using fg command

You can bring a background job to the foreground using fg command. When executed without arguments, it will take the most recent background job to the foreground.

fg

If you have multiple background ground jobs, and would want to bring a certain job to the foreground, execute jobs command which will show the job id and command.

In the following example, fg %1 will bring the job#1 (i.e download-file.sh) to the foreground.

# jobs
[1]   Running                 bash download-file.sh &
[2]-  Running                 evolution &
[3]+  Done                    nautilus .

# fg %1

5. Kill a specific background job using kill %

If you want to kill a specific background job use, kill %job-number. For example, to kill the job 2 use

# kill %2

****************************
Kill command
Example: Kill the firefox process.

$ ps -ef | grep firefox
1986 ?        Sl     7:22 /usr/lib/firefox-3.5.3/firefox

$ kill -9 1986

2. Killall Command – Kill processes by name

Instead of specifying a process by its PID, you can specify the name of the process. If more than one process runs with that name, all of them will be killed.
Example: Kill all the firefox processes

$ killall -9 firefox

******************************




TCPIP :

TCP provides reliable, ordered and error-checked delivery of a stream of octets between programs running on computers connected to a local area network, intranet or the public Internet. It resides at the transport layer.

Web browsers use TCP when they connect to servers on the World Wide Web, and it is used to deliver email and transfer files from one location to another. HTTP, HTTPS, SMTP, POP3, IMAP, SSH, FTP, Telnet and a variety of other protocols are typically encapsulated in TCP.

Applications that do not require the reliability of a TCP connection may instead use the connectionless User Datagram Protocol (UDP), which emphasizes low-overhead operation and reduced latency rather than error checking and delivery validation.

UDP :

The User Datagram Protocol (UDP) is one of the core members of the Internet protocol suite (the set of network protocols used for the Internet). With UDP, computer applications can send messages, in this case referred to as datagrams, to other hosts on an Internet Protocol (IP) network without prior communications to set up special transmission channels or data paths

UDP uses a simple transmission model with a minimum of protocol mechanism.[1] It has no handshaking dialogues, and thus exposes any unreliability of the underlying network protocol to the user's program. As this is normally IP over unreliable media, there is no guarantee of delivery, ordering, or duplicate protection. UDP provides checksums for data integrity, and port numbers for addressing different functions at the source and destination of the datagram.


UDP is suitable for purposes where error checking and correction is either not necessary or is performed in the application, avoiding the overhead of such processing at the network interface level. Time-sensitive applications often use UDP because dropping packets is preferable to waiting for delayed packets

A number of UDP's attributes make it especially suited for certain applications.

It is transaction-oriented, suitable for simple query-response protocols such as the Domain Name System or the Network Time Protocol.
It provides datagrams, suitable for modeling other protocols such as in IP tunneling or Remote Procedure Call and the Network File System.
It is simple, suitable for bootstrapping or other purposes without a full protocol stack, such as the DHCP and Trivial File Transfer Protocol.
It is stateless, suitable for very large numbers of clients, such as in streaming media applications for example IPTV
The lack of retransmission delays makes it suitable for real-time applications such as Voice over IP, online games, and many protocols built on top of the Real Time Streaming Protocol.
Works well in unidirectional communication, suitable for broadcast information such as in many kinds of service discovery and shared information such as broadcast time or Routing Information Protocol



*****************************************

IPSEC :
Internet Protocol Security (IPsec) is a protocol suite for securing Internet Protocol (IP) communications by authenticating and encrypting each IP packet of a communication session. IPsec includes protocols for establishing mutual authentication between agents at the beginning of the session and negotiation of cryptographic keys to be used during the session. IPsec can be used in protecting data flows between a pair of hosts (host-to-host), between a pair of security gateways (network-to-network), or between a security gateway and a host (network-to-host).[1]

Internet Protocol security (IPsec) uses cryptographic security services to protect communications over Internet Protocol (IP) networks. IPsec supports network-level peer authentication, data origin authentication, data integrity, data confidentiality (encryption), and replay protection.

IPsec is an end-to-end security scheme operating in the Internet Layer of the Internet Protocol Suite
*****************************************

Vlan :

A network of computers that behave as if they are connected to the same wire even though they may actually be physically located on different segments of a LAN. VLANs are configured through software rather than hardware, which makes them extremely flexible. One of the biggest advantages of VLANs is that when a computer is physically moved to another location, it can stay on the same VLAN without any hardware reconfiguration.

VLANs are usually associated with IP subnetworks. For example, all the end stations in a particular IP subnet belong to the same VLAN. Traffic between VLANs must be routed. LAN port VLAN membership is assigned manually on an port-by-port basis. 


VLANs  Range  Usage  Propagated
by VTP  
0, 4095 
 Reserved 
 For system use only. You cannot see or use these VLANs. 
 — 
 
1 
 Normal 
 Cisco default. You can use this VLAN but you cannot delete it. 
 Yes 
 
2-1001 
 Normal 
 For Ethernet VLANs; you can create, use, and delete these VLANs. 
 Yes 
 
1002-1005 
 Normal 
 Cisco defaults for FDDI and Token Ring. You cannot delete VLANs 1002-1005. 
 Yes 
 
1006-4094 
Extended 
 For Ethernet VLANs only. 
 No 
 
To display the VLANs used internally, enter the "show vlan internal usage" command. With earlier releases, enter the show vlan internal usage and "show cwan vlans" commands. 

•You can configure ascending internal VLAN allocation (from 1006 and up) or descending internal VLAN allocation (from 4094 and down). 


******************************************

QOS: (Quality of Service) :
Quality of service (QoS) is the overall performance of a telephony or computer network, particularly the performance seen by the users of the network.

To quantitatively measure quality of service, several related aspects of the network service are often considered, such as error rates, bandwidth, throughput, transmission delay, availability, jitter, etc.

Quality of service is particularly important for the transport of traffic with special requirements. In particular, much technology has been developed to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter service demands.

******************************************
Routing :

Routing is the process of selecting best paths in a network. In the past, the term routing was also used to mean forwarding network traffic among networks. However this latter function is much better described as simply forwarding. Routing is performed for many kinds of networks, including the telephone network (circuit switching), electronic data networks (such as the Internet), and transportation networks

*****************************************





 


 



 


Related: Linux CPU performance Monitoring

 




**********************************

Virtualisaion Benefits:

1) Less Heat
2) Reduced Cost
3) Easier Backups
4) Greener Environment (Reducing the carbon emission)
5) Better Testing
6) Better DR (with upto date snapshots)
7) Single Minded servers
8) 

**********************************


LUN :

In computer storage, a logical unit number, or LUN, is a number used to identify a logical unit, which is a device addressed by the SCSI protocol or protocols which encapsulate SCSI, such as Fibre Channel or iSCSI. A LUN may be used with any device which supports read/write operations, such as a tape drive, but is most often used to refer to a logical disk as created on a SAN.

******************************
LUN Masking :

LUN masking is an authorization process that makes a Logical Unit Number available to some hosts and unavailable to other hosts.

When LUN masking is implemented at the storage controller level, the controller itself enforces the access policies to the device and as a result it is more secure. However, it is mainly implemented not as a security measure per se, but rather as a protection against misbehaving servers which may corrupt disks belonging to other servers. For example, Windows servers attached to a SAN will, under some conditions, corrupt non-Windows (Unix, Linux, NetWare) volumes on the SAN by attempting to write Windows volume labels to them. By hiding the other LUNs from the Windows server, this can be prevented, since the Windows server does not even realize the other LUNs exist.

******************************

LUN Masking and Zoning :

In a SAN (Storage Area Network), if all the hosts are allowed to access all the drives in the SAN, it may lead to many issues like device contention and mainly data corruption. In addition some Operating Systems (eg. Windows ) writes header information to disks which will lead to data loss (We will discuss it later in this article). To avoid this zoning and LUN masking is used. By doing Zoning and LUN masking storage units are isolated or made invisible to some or all of the hosts in the SAN. Lets look into zoning and LUN masking in detail.


LUN Masking :
LUN masking is similar to zoning in the sense that both are used to provide a way of access control. But LUN masking is entirely different from zoning. Mainly LUN masking is implemented at HBA level. Some storage controllers also support LUN masking.

The main purpose of LUN masking is preventing access to LUNs from some specific hosts, a way of protection against data loss. For example when a host running Windows operating system is connected to SAN, Windows may try to assign volume labels to the LUNs by writing header information. This may corrupt the data written by other operating system on a file system alien to Windows.

Due to possible compromises at HBA level, LUN masking implemented at HBA level is more  prone to attacks . But when LUN masking is done at storage controller level, controller itself enforces all the grouping relations, its more secure and more or less as strong as zoning security.

Zoning :

Zoning is used to specify which host can see which storage array. Zoning is done at the switch level. This is explained in the figure given below.


Hard Zoning

In hard zoning, members of the zone group are specified using actual port id ( physical port id ). So hard zoning physically blocks access to a device from a device which is outside of the zone.

Soft Zoning

To understand zoning first we should know what is WWN (World Wide Name).

WWN - World Wide Name

WWN is a 64 bit unique identifier assigned to each device or port in the SAN. If one device has more than one port, it each port will be having unique WWN. WWN is equivalent to MAC address of your NIC (Network adapters) in Ethernet.

In soft zoning, the zone configuration is specified using WWN of the ports. Soft zoning is implemented at software level. It internally uses a name server to identify which all WWN belongs to particular zone, and does a look up in the name server to validate membership.


Advantages and Disadvantages of Hard Zoning :

Advantages	Disadvantages
Easier to create and manage since the its using direct port addressing instead of long WWN	Moving a device from one switch port to another switch port requires reconfiguration of the zoning
Since the switch hardware does not allow any traffic between unauthorized nodes, its more secure.	 
Advantages and Disadvantages of Soft Zoning :
 
Advantages	Disadvantages
Devices can be moved to different switch port without reconfiguring the zoning.	Since the membership validation is not done at hardware level, its possible for HBAs to bypass the name server and communicate with the node which is not in the configured zone.
Easy to maintain the zones and is flexible.	 It is possible to spoof the WWN number, and access the device in different zone.


**********************************


On SLES 10 based vhsts running Heartbeat, you should be using the crm_resource and crm_attribute commands.

		 Setting a whole cluster to Unmanaged Mode
		 		 crm_attribute -n is-managed-default -v false

		 Setting a whole cluster back to Managed Mode
		 		 crm_attribute -n is-managed-default -v true

		 Setting a resource to Unmanaged Mode ( You should only be using the "--meta" format of this command)
		 		 crm_resource --meta -r <resource name> -p is-managed -v false

		 Setting a resource back to Managed Mode ( This deletes the "is-managed" parameter. This is the preferred way )
		 		 crm_resource -r <resource name> -d is-managed



On SLES 11 based vhsts running Pacemaker, you should be using only the crm commands.

		 To set a whole cluster to unmanaged mode (this can not be overridden by individual resource settings) 
		 		 crm configure property maintenance-mode=true

		 To take a cluster out of "Maintenance Mode"
		 		 crm configure property maintenance-mode=false

		 To set an individual resource to unmanaged
		 		 crm resource unmanaged <resource name>

		 To set an individual resource back to managed
		 		 crm resource managed <resource name>


************

/sbin/rcmultipathd stop
/sbin/rcopen-iscsi stop
/sbin/rcopen-iscsi start
/sbin/rcmultipathd start
/sbin/rcmultipathd restart


************
dmesg
********

root@vhst01.s01155.ca:/etc/sysconfig/network $ ethtool eth7

***************************************************


To do FSCk :::::(also can check RO or not  (cat /proc/mounts | grep ro)

Here to login :  
ssh root@s06056tsm01us.s06056.us 

umount /fs
e2fsck -f -y /dev/mapper/vg01-lvol7
dumpe2fs /dev/mapper/vg01-lvol7
then can mount /fs 

Then to check :
cat /tmp/fsck.log | grep "/dev/vg01/lvol7"
(should come out - CLEAN WITHOUT ERRORS)


root@s07048tsm01us.s07048.us:/tmp/.shell $ dumpe2fs /dev/vg01/lvol7
dumpe2fs 1.38 (30-Jun-2005)
Filesystem volume name:   <none>
Last mounted on:          <not available>
Filesystem UUID:          2e023392-3f53-4a97-b931-14dcd42880e0
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal filetype needs_recovery sparse_super large_file
Default mount options:    (none)
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              183123968
Block count:              366230528
Reserved block count:     18311526
Free blocks:              176795856
Free inodes:              183123779
First block:              0
Block size:               4096
Fragment size:            4096
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         16384
Inode blocks per group:   512
Filesystem created:       Fri Oct  3 03:37:16 2008
Last mount time:          Sat Nov  3 12:19:41 2012
Last write time:          Sat Nov  3 12:19:41 2012
Mount count:              2
Maximum mount count:      34
Last checked:             Fri Nov  2 11:40:53 2012
Check interval:           15552000 (6 months)
Next check after:         Wed May  1 11:40:53 2013
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:               128
Journal inode:            8
Default directory hash:   tea
Directory Hash Seed:      7d8c6d41-3db4-42d7-97e2-2d3481df7c1f
Journal backup:           inode blocks

***************************************************




**********************

Procs crit:
du -sch *

ps -ef|grep defu|wc -l
ps aux|wc -l
ls -l|wc -l

****************


**********

#crm resource cleanup *vm* ---- to clear failed action on vhst's fr DRBD.
#crm configure show cli-prefer-saps01
#crm resource unmove saps02; crm resource move saps02 vhst02xxxx
#crm configure show cli-prefer-saps01
#crm resource cleanup resDRBD-saps02:1
11:26:56 AM: Venkatappala N Yenni: **************** to OFF-LINE the drive in a DRBD server ******************************
MegaCli -PDOffline -PhysDrv [E:S] -aN (E- Enclosure, S - slot, N - controller)

#/opt/MegaRAID/MegaCli/MegaCli64 -PDOffline -PhysDrv [16:2] -a0

make drive hotspare:
/opt/MegaRAID/MegaCli//MegaCli64 -PDHSP -Set -PhysDrv [16:7] -a0


Check rebuild:
/opt/MegaRAID/MegaCli/MegaCli64 -PDRbld -ShowProg -PhysDrv [16:7] -a0

check copyback:

/opt/MegaRAID/MegaCli/MegaCli64 -PDCpyBk -ShowProg -PhysDrv [252:0] -a0

1)First check the nagios log for disk error, based on which you can find out which disk is showing errors.

2)check with the command for errors.
 /opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 | grep Error

3)check all disks are online by the following command.
  /opt/MegaRAID/MegaCli/MegaCli64 -PDList -a0 | grep Firmware

3b) Check Virtual Drive Information :

/opt/MegaRAID/MegaCli/MegaCli64 -LDInfo -Lall -aALL

4)To offline MegaRaid drive.
  MegaCli -PDOffline -PhysDrv [E:S] -aN (E- Enclosure, S - slot, N - controller)

------------>
cd /opt/MegaRAID/MegaCli/
./MegaCli64 -PDOffline -PhysDrv [16:4] -a0



5) Once you offline the faulty drive, hotspace drive starts rebuilding automatically.
/opt/MegaRAID/MegaCli//MegaCli64 -PDRbld -ShowProg -PhysDrv [E:S] -aN (E- Enclosure, S - slot, N - controller)

6)Once the drive is offline you can check the MegaRaid state as degraded and once the resyncing is completed  you can observe the state as Optimal.
 /opt/MegaRAID/MegaCli/MegaCli64 -LDInfo -LALL -aALL

Check the below  example: 

In this case after the faulty drive has been offline, hot spare disk started rebuilding automatically.

root@vhst02.s04699.us:/root $ /opt/MegaRAID/MegaCli//MegaCli64 -PDRbld -ShowProg -PhysDrv [16:7] -a0



Rebuild Progress on Device at Enclosure 16, Slot 7 Completed 69% in 26 Minutes.

Exit Code: 0x00



Start the blinking:

# MegaCli -PdLocate -start -PhysDrv [E:S] -aALL
Stop the blinking:
# MegaCli -PdLocate -stop -PhysDrv [E:S] -aALL


**********





*************************


1. Install package

yum install CentrifyDC -y


2. Take backup of important files:

bash
TS=`date +'%Y%m%d'`
cd /etc
for file in passwd group shadow gshadow nsswitch.conf sudoers sysconfig/network yp.conf ssh/sshd_config centrifydc/user.ignore centrifydc/group.ignore centrifydc/centrifydc.conf centrifydc/users.allow centrifydc/groups.allow; do
cp -vip $file $file.back.$TS
done

3. Analyze all users and groups in sudoers

egrep -v "^$|^#|^Default" /etc/sudoers

4. Analyze users who logged in recently

last -w | cut -d " " -f 1 | sort -u

5. Find any users (non OS) for whom, processes are running

ps -ef|egrep -v "^root |^apache |^gdm |^postfix |^nscd |^ntp |^ganglia |^rpc |^rpcuser |^xfs |^dbus |^68 |^oracle |^UID |^nagios |^nobody "|awk '{print $1}'|sort -u


6. Check for any services using "prog" ID or a "sys_" ID

for n in 3 5 ;do
cd /etc/rc.d/rc$n.d
for file in `ls S*|egrep -v "smartd|cfenvd|anacron|xfs|crond|postfix|ntpd|rpcbind|ypbind|tog-pegasus|ksmtuned|messagebus|nfslock|lldpad|portreserve|ksm|libvirt-guests|centrifydc|xinetd|cups|sshd|snmpd|nscd|lm_sensors|setroubleshoot|rpc.gssd|rpc.idmapd|mdmonitor|portmap|irqbalance|cpuspeed|auditd|mcstrans|atd|rpcidmapd|rpcgssd|rpcsvcgssd|rsyslog|freenx-server|autofs|pcscd"`;do cat $file|egrep -v "^#|^$"|grep prog;done
echo;cd
done

for n in 3 5 ;do
cd /etc/rc.d/rc$n.d
for file in `ls S*|egrep -v "smartd|cfenvd|anacron|xfs|crond|postfix|ntpd|rpcbind|ypbind|tog-pegasus|ksmtuned|messagebus|nfslock|lldpad|portreserve|ksm|libvirt-guests|centrifydc|xinetd|cups|sshd|snmpd|nscd|lm_sensors|setroubleshoot|rpc.gssd|rpc.idmapd|mdmonitor|portmap|irqbalance|cpuspeed|auditd|mcstrans|atd|rpcidmapd|rpcgssd|rpcsvcgssd|rsyslog|freenx-server|autofs|pcscd"`;do cat $file|egrep -v "^#|^$"|grep sys_;done
echo;cd
done



----------------------------------------------------------------------------------

7. Who all will login:

Ensure Computer object, role login group to be created

All users/groups required to be granted need to be added to role_login group now

Add the local accounts in /etc/passwd and /etc/group as needed
Then run pwconv and grpconv

Again, if needed (i.e. if those users/groups also exist in Centrify), add those accounts to user.ignore and group.ignore (See step 11 below)

----------------------------------------------------------------------------------


8. Join to Centrify

adjoin --selfserve -z Universal -c "ou=UnixServers,ou=Computers,ou=Centrify,dc=nibr,dc=novartis,dc=net" nibr.novartis.net -n 

9. Ensure modifications to Centrify configuration:

ls -ld /etc/centrifydc/users.allow /etc/centrifydc/groups.allow
egrep -v "^$|^#" /etc/centrifydc/centrifydc.conf;echo;egrep -v "^$|^#" /etc/centrifydc/groups.allow

cat >> /etc/centrifydc/centrifydc.conf << EOF
dns.gc.intern.ebewe.at: nosuchhost
dns.dc.alconnet.com: nosuchhost
dns.dc.intern.ebewe.at: nosuchhost
dns.dc.023d.mgd.msft.net: nosuchhost
dns.gc.alconnet.com: nosuchhost
dns.gc.023d.mgd.msft.net: nosuchhost
pam.allow.users: file:/etc/centrifydc/users.allow
pam.allow.groups: file:/etc/centrifydc/groups.allow
nss.user.ignore: file:/etc/centrifydc/user.ignore
nss.uid.ignore: file:/etc/centrifydc/uid.ignore
nss.group.ignore: file:/etc/centrifydc/group.ignore
nss.gid.ignore: file:/etc/centrifydc/gid.ignore
pam.homedir.create: false
adclient.krb5.service.principals:
adclient.zone.group.count: 10000
adclient.sntp.enabled: false
log: WARN
EOF

touch /etc/centrifydc/users.allow
cat > /etc/centrifydc/groups.allow << EOF
admins_nibr_srv_gbl_a
nibr_scan_lx_srv_gbl_a
%admins_nibr_linux_srv_gbl_a
biouserus
phusca-MPA_Analysis
phusca-PG_Analysis
sbs-ct-imaging-i10115_rw
usca-bmd-flaxmap3d-wdl30399_rw
usca-da-serineracemase_rw
usca_bix_analytics-rw
usca_mpa_analytics-rw
usca_pg_analytics-rw
$(hostname -s)_role_login
EOF
chmod 440 /etc/centrifydc/users.allow /etc/centrifydc/groups.allow

10. Verify modifications to Centrify configuration:

egrep -v "^$|^#" /etc/centrifydc/centrifydc.conf;echo;egrep -v "^$|^#" /etc/centrifydc/groups.allow

If needed (Ex: Centrify hostname is different than Linux hostname), do:
vi /etc/centrifydc/groups.allow

11. Ensure exclusions:
/etc/centrifydc/user.ignore
/etc/centrifydc/group.ignore

12. Ensure extra groups (if any):
/etc/centrifydc/groups.allow

13. Reconfigure sshd

sed -i 's/ChallengeResponseAuthentication no/ChallengeResponseAuthentication yes/g' /etc/ssh/sshd_config;service sshd restart
/etc/init.d/centrifydc restart;time adflush


14. Test all logins (users granted in step 7)


15. Stop and disable ypbind

chkconfig --list ypbind
/etc/init.d/ypbind stop
chkconfig ypbind off
chkconfig --list ypbind

******************************







***********

NIS Password Reset :

Login to "NIS" Server
Become Sudo
/usr/local/bin/acs-nis-passwd <user id> (send the password to user via email )
make -C /var/yp

**********





***************************
Scan Disks :
for bus in `ls /sys/class/scsi_host`;do echo Scanning $bus; echo "- - -" > /sys/class/scsi_host/$bus/scan; done
(or)
echo "- - -" > /sys/class/scsi_host/host1/scan for host0, 1,2 ,3..)

The three values stand for channel, SCSI target ID, and LUN. The dashes act as wildcards meaning "rescan everything"


Rescan Sdd :

echo 1 > /sys/block/sdd/device/rescan




******************

Change owners/groups in Ubuntu

    Ubuntu

To change all files/folders recursively in subdirectory myfolder:

$ chown -R newowner:newgroup myfolder/

To change all folders:
$ chown -R newowner:newgroup */

To change owner only:
$ chown -R newowner myfolder/

To change group only:
$ chown -R :newgroup myname/

Or
$ chgrp -R newgroup myname/

**************

************************

http://www.binarytides.com/linux-scp-command/

From : nrchbs-slt0007 :::

Scp upload (to another server):
scp ~/my_local_file.txt user@remote_host.com:/some/remote/directory

scp test adm_narayji2@nrchbs-slt0008:.


Scp download (from another server):
scp user@remote_host.com:/some/remote/directory ~/my_local_file.txt

scp adm_narayji2@nrchbs-slt0008:/admhome/adm_narayji2/testt .

******************************

***********************************

Ulimits need to be adjusted for Oracle and any other applications that need settings that differ from out of the box settings. The parameters that can be changed are (these may differ between RHEL revisions):

    core - limits the core file size (KB)
    data - max data size (KB)
    fsize - maximum filesize (KB)
    memlock - max locked-in-memory address space (KB)
    nofile - max number of open files
    rss - max resident set size (KB)
    stack - max stack size (KB)
    cpu - max CPU time (MIN)
    as - address space limit
    maxlogins - max number of logins for this user
    maxsyslogins - max number of logins on the system
    priority - the priority to run user process with
    locks - max number of file locks the user can hold
    sigpending - max number of pending signals
    msgqueue - max memory used by POSIX message queues (bytes)
    nice - max nice priority allowed to raise to
    rtprio - max realtime priority

 

     To see all ulimit settings, run "ulimit -HSa".



***************************************************************************************

Puppet works in a classic client-server architecture. The client machine has the Puppet client installed and is configured to contact a server, called a Puppetmaster, to retrieve the description of what its configuration should be (called the catalog). The local Puppet client then “applies” the configuration described in this catalog, making local changes if necessary.

As the wanted configuration can depend on criterias like the OS release, the number of CPUs, etc.. the client locally computes so called facts which are passed to the puppetmaster together with the catalog request. There are a number of facts which are computed by facter out of the box, and it’s possible to develop so called custom-facts (i.e the datacenter name, depending on the machine’s IP address). All these facts are available to the puppetmaster when it compiles the client’s catalog.

There is a Puppetmaster per site, to manage the DEV/TEST/PROD servers on that site. There can be any number of DEV Puppetmaster servers, to which a number of experimental client machines are attached to.

The Puppet code is written in Puppet’s own language and stored in files called manifests on the Puppet master server. The catalog requested by client machines when they contact the Puppetmaster is compiled from these manifests.

The data (the values populated in the catalog and applied on the host) is written in YAML and stored in a hierarchical tree, called hiera in the rest of this document.

When the search of variable’s value stored in hiera is triggered, several YAML files will be used. Depending on the requested type of search:

    the first value found in the search path is used.
    all the files in the search path are processed. The result is an array or a hash containing all the values found at the different levels in the path. The search path determines the order of the resulting array or hash.

The chosen search path in hiera is the following

    %{nov_site}/hosts/%{fqdn}
    %{nov_site}/%{classification}
    %{nov_site}/common
    common/%{classification}
    common/common

Such a tree makes it easy to (depending on the case) set or append values at the global level (every host in every site), at the site-level, at the classifiation-level (dev/test/prod), and at the host-level, and is Puppetlabs’ recommended best practice.

The Puppet code and the Puppet data are stored in seperate git repositories. All changes are committed with a description This allows easy rollback in case of problems, traceability and accountability. Sanity checks are applied before a commit is accepted in the repository.

Most of the day to day administration tasks (i.e adding a new mount on a server, adding a new sudo rule on a host, etc) only require to edit a single YAML file, the host-specific one. The sanity checks in that case are easy to perform. Writing Puppet code, on the other hand, is more involved, much harder to check, and always impacts all the machines as it’s shared. If it’s broken Puppet can’t run on any machine until it’s fixed. For these reasons, changes in the Puppet code repository are restricted and go through an extra (and manual) review process.

Puppet runs on client machines once a day, with the possibility to trigger runs from the master server (on a single machine or on a group of machines) or the client itself. A dashboard is available to have a state overview (as well as run details) of the machines managed by each Puppetmaster server.

*****************************
Df hung:
df -l
umount -l mountpoint 

We will need to troubleshoot a bit before remounting the nfs share.  run rpcinfo -p <remotehost> and look up all of the ports thats required for NFS to work.  You will need portmapper/nlockmgr/nfs/mountd make sure that you are able to reach each of these ports on the remote host before attempting to remount the nfs share.

After the ports have been verified you can safely remount your share and it should mount without problems unless you have a problem on the NFS server side with permissions or access list.


****************************


Check pacemaker version:

Pacemaker 1.1.6

Depending on your setup and version there are multiple ways to find out:

Package management:

rpm -qa|grep -i pacemaker

dpkg -l|grep -i pacemaker

Cluster executable:

crmadmin --version

>From a running clusters cib:

cibadmin -Q|grep dc-version 



*******

xm info gives you (amongst others) the following lines:

xen_major : 3
xen_minor : 0
xen_extra : .2

which means that you are using version 3.0.2.

***********


***********

LVM supports multiple configuration files. At system startup, the lvm.conf configuration file is loaded from the directory specified by the environment variable LVM_SYSTEM_DIR, which is set to /etc/lvm by default. 

The lvm.conf file can specify additional configuration files to load. Settings in later files override settings from earlier ones. To display the settings in use after loading all the configuration files, execute the lvm dumpconfig command. 

For information on loading additional configuration files, see Section C.2, “Host Tags”. 





?B.1. The LVM Configuration Files

 The following files are used for LVM configuration: 

/etc/lvm/lvm.conf
 Central configuration file read by the tools. 
etc/lvm/lvm_hosttag.conf
 For each host tag, an extra configuration file is read if it exists: lvm_hosttag.conf. If that file defines new tags, then further configuration files will be appended to the list of files to read in. For information on host tags, see Section C.2, “Host Tags”. 

In addition to the LVM configuration files, a system running LVM includes the following files that affect LVM system setup: 

/etc/lvm/.cache
 Device name filter cache file (configurable). 
/etc/lvm/backup/
 Directory for automatic volume group metadata backups (configurable). 
/etc/lvm/archive/
 Directory for automatic volume group metadata archives (configurable with regard to directory path and archive history depth). 
/var/lock/lvm/
 In single-host configuration, lock files to prevent parallel tool runs from corrupting the metadata; in a cluster, cluster-wide DLM is used. 

******************


*********
Linux rebuild the initial ramdisk image :


The mkinitrd script constructs a directory structure that can serve as an initrd root file system. It then generates an image containing that directory structure using mkcramfs, which can be loaded using the initrd mechanism. The kernel modules for the specified kernel version will be placed in the directory structure. If version is omitted, it defaults to the version of the kernel that is currently running.

Find out your kernel version:
# uname -r

2.6.15.4
Make backup of existing ram disk:
# cp /boot/initrd.$(uname -r).img /root

To create initial ramdisk image type following command as the root user:
# mkinitrd -o /boot/initrd.$(uname -r).img $(uname -r)
# ls -l /boot/initrd.$(uname -r).img

You may need to modify grub.conf to point out to correct ramdisk image, make sure following line existing in grub.conf file:
initrd /boot/initrd.img-2.6.15.4.img

When the system boots using an initrd image created by mkinitrd command, the linuxrc will wait for an amount of time which is configured through mkinitrd.conf, during which it may be interrupted by pressing ENTER. After that, the modules specified in will be loaded.



*******

Copy file from one server to another using sftp:

[root@nrusca-slp9996 adm_narayji2]# ls -ltr
total 9280
-rwxr-xr-x 1 root         root            9476 Dec 18 06:34 acs-centrify-server-readycheck.sh
-rwxrwxr-x 1 root         root             121 Mar  3 05:00 runcommand.sh
-rw-rw-r-- 1 adm_narayji2 adm_narayji2    2984 Mar  3 05:21 list
-rw-rw-r-- 1 adm_narayji2 adm_narayji2 9482240 Jun 16 11:33 emcgrab_Linux_v4.6.6.tar



[root@nrusca-slp9996 adm_narayji2]# sftp adm_narayji2@chbslx1553
Connecting to chbslx1553...
Password:
Password:
sftp> pwd
Remote working directory: /admhome/adm_narayji2
sftp> cd /tmp
sftp>
sftp> put emcgrab_Linux_v4.6.6.tar
Uploading emcgrab_Linux_v4.6.6.tar to /tmp/emcgrab_Linux_v4.6.6.tar
emcgrab_Linux_v4.6.6.tar                                                                                                              100% 9260KB   4.5MB/s   00:02
sftp> bye



Then go to target server and :

tar xf emcgrab_Linux_v4.6.6.tar


(((OR)))  USING SCP :

**** scp source_file_name username@destination_host:destination_folder *****

from source server :

scp runcommand.sh adm_narayji2@chbslx1553:/tmp

 ---->  Multiple files can be specified as the source files. For example, to transfer the contents of the directory downloads/ to an existing directory called uploads/ on the remote machine penguin.example.net, type the following at a shell prompt:

scp downloads/* username@penguin.example.net:uploads/   <----


(((OR)))


from target server (where file needs ot be copied - chbslx1553)

scp -p adm_narayji2@nrusca-slp9996:/admhome/adm_narayji2/emcgrab_Linux_v4.6.6.tar .


*******


Check Kernels Installed according to date and time :

rpm -qa --last|grep ^kernel 

rpm -e kernel-debug-2.6.18-398.el5 (to remove that old kernel)




While discussion with Patrick I observed important root cause for the disk utilization:

Below open file was consuming high disk space
[root@nrusca-slp0156 ~]# lsof /apps/ | grep "logos.channeladapter.log"
java       3441 sys_eventhub1  323w   REG  253,2 363410405441 16778966 /apps/sys_eventhub1/apps/event_hub/shared/logs/logos.channeladapter.log (deleted)
[root@nrusca-slp0156 ~]#





Command to check soft link :
Example if we need softlink under / then:

[root@uscalx1301 ~]# ls -la /|grep "\->"
lrwxrwxrwx    1 root root     22 May 28  2013 clscratch -> /mnt/XS/usca/tmscratch
lrwxrwxrwx    1 root root      8 Oct 12  2011 da -> /usca/da
lrwxrwxrwx    1 root root     15 Mar  8  2013 db -> /mnt/XS/usca/db
lrwxrwxrwx    1 root root     13 Oct 12  2011 dlab -> /usca/labdata
lrwxrwxrwx    1 root root     17 Oct 12  2011 programs -> /usca/prog/sbgrid


*****************



Please find below commands to change a servers timezone from Non-UTC (eg: EDT) to UTC :

1)vi /etc/sysconfig/clock  ----? here change UTC=true
2) ln –sf /usr/share/zoneinfo/UTC  /etc/localtime   (Link to local time)


******

Scp best :


[root@uscalx1321 bin]# scp -p acs-oracle-run-as-root adm_narayji2@uscalx1099:.
[adm_narayji2@uscalx1099 ~]$ cp -vip acs-oracle-run-as-root /tmp
sudo su -
chown root:root acs-oracle-run-as-root

************


*****
Copy entire directory (recursively)

To copy an entire directory from one host to another use the r switch and specify the directory

$ scp -v -r ~/Downloads root@192.168.1.3:/root/Downloads


[root@uscalx1321 bin]# scp -p acs-oracle-run-as-root adm_narayji2@uscalx1099:/tmp/
[adm_narayji2@uscalx1099 ~]$ 

sudo su -
chown root:root acs-oracle-run-as-root

*****


**************************************






******************

IP tool :

1) The following command used to assign IP Address to a specific interface (eth1) on the fly.

ip addr add 192.168.50.5 dev eth1 (will last until system restart)

2) Check Ip address

ip addr show

3) remove ip addr:

ip addr del 192.168.50.5/24 dev eth1

4) Enable Network Interface :

ip link set eth1 up

5) Disable Network Interface
ip link set eth1 down

6) Check Route Table
ip route show

7) Add Static Route
ip route add 10.10.20.0/24 via 192.168.50.100 dev eth0

8) Remove Static Route
ip route del 10.10.20.0/24

9)Add Persistence Static Routes

For RHEL/CentOS/Fedora

# vi /etc/sysconfig/network-scripts/route-eth0
10.10.20.0/24 via 192.168.50.100 dev eth0


For Ubuntu/Debian/Linux Mint

Open the file /etc/network/interfaces and at the end add the persistence Static routes. IP Addresses may differ in your environment.

$ sudo vi /etc/network/interfaces

auto eth0
iface eth0 inet static
address 192.168.50.2
netmask 255.255.255.0
gateway 192.168.50.100
#########{Static Route}###########
up ip route add 10.10.20.0/24 via 192.168.50.100 dev eth0

Next, restart network services after entering all the details using the following command.

# /etc/init.d/network restart

10. How do I Add Default Gateway

ip route add default via 192.168.50.100



*********************

Rpm:

1. How to Check an RPM Signature Package

rpm --checksig

2. How to Install an RPM Package
rpm -ivh

3. How to check dependencies of RPM Package before Installing
rpm -qpR

-p : List capabilities this package provides.
-R: List capabilities on which this package depends..

4. How to Install a RPM Package Without Dependencies
rpm -ivh --nodeps

5. How to check an Installed RPM Package
rpm -q BitTorrent


6. How to List all files of an installed RPM package
rpm -ql BitTorrent

7. How to List Recently Installed RPM Packages
rpm -qa --last

8. How to List All Installed RPM Packages
rpm -qa

9. How to Upgrade a RPM Package

rpm -Uvh

10. How to Remove a RPM Package
rpm -evv nx

11. How to Remove an RPM Package Without Dependencies
rpm -ev --nodeps vsftpd

12. How to Query a file that belongs which RPM Package
rpm -qf /usr/bin/htpasswd

13. How to Query a Information of Installed RPM Package
rpm -qi vsftpd


14. Get the Information of RPM Package Before Installing
rpm -qip sqlbuddy-1.3.3-1.noarch.rpm

15. How to Query documentation of Installed RPM Package
rpm -qdf /usr/bin/vmstat

16. How to Verify a RPM Package
rpm -Vp sqlbuddy-1.3.3-1.noarch.rpm

17. How to Verify all RPM Packages
rpm -Va

**********************


Crontab :

Field 	Description 	Allowed Value
MIN 	Minute field 	0 to 59
HOUR 	Hour field 	0 to 23
DOM 	Day of Month 	1-31
MON 	Month field 	1-12
DOW 	Day Of Week 	0-6
CMD 	Command 	Any command to be executed.

********************


1. Creating an archive using tar command

tar cvf archive_name.tar dirname/


    c – create a new archive
    v – verbosely list files which are processed.
    f – following is the archive file name

1 b).Creating a tar gzipped archive :

tar cvzf archive_name.tar.gz dirname/

z – filter the archive through gzip

1 c) Creating a bzipped tar archive using option cvjf
Create a bzip2 tar archive as shown below:
tar cvfj archive_name.tar.bz2 dirname/

j – filter the archive through bzip2

gzip vs bzip2: bzip2 takes more time to compress and decompress than gzip. bzip2 archival size is less than gzip.

***

2. Extracting (untar) an archive using tar command

Extract a *.tar file using option xvf

tar xvf archive_name.tar

x – extract files from archive

****

Extract a gzipped tar archive ( *.tar.gz ) using option xvzf

Use the option z for uncompressing a gzip tar archive.
$ tar xvfz archive_name.tar.gz

****

Extracting a bzipped tar archive ( *.tar.bz2 ) using option xvjf

Use the option j for uncompressing a bzip2 tar archive.

$ tar xvfj archive_name.tar.bz2

*****

3. Listing an archive using tar command

View the tar archive file content without extracting using option tvf

You can view the *.tar file content before extracting as shown below.

$ tar tvf archive_name.tar

View the *.tar.gz file content without extracting using option tvzf

You can view the *.tar.gz file content before extracting as shown below.

$ tar tvfz archive_name.tar.gz

View the *.tar.bz2 file content without extracting using option tvjf

You can view the *.tar.bz2 file content before extracting as shown below.

$ tar tvfj archive_name.tar.bz2

********

5. Extract a single file from tar, tar.gz, tar.bz2 file

To extract a specific file from a tar archive, specify the file name at the end of the tar xvf command as shown below. The following command extracts only a specific file from a large tar file.

$ tar xvf archive_file.tar /path/to/file

************

Use the relevant option z or j according to the compression method gzip or bzip2 respectively as shown below.

$ tar xvfz archive_file.tar.gz /path/to/file

$ tar xvfj archive_file.tar.bz2 /path/to/file


*********

6. Extract a single directory from tar, tar.gz, tar.bz2 file

To extract a single directory (along with it’s subdirectory and files) from a tar archive, specify the directory name at the end of the tar xvf command as shown below. The following extracts only a specific directory from a large tar file.

$ tar xvf archive_file.tar /path/to/dir/

To extract multiple directories from a tar archive, specify those individual directory names at the end of the tar xvf command as shown below.

$ tar xvf archive_file.tar /path/to/dir1/ /path/to/dir2/

Use the relevant option z or j according to the compression method gzip or bzip2 respectively as shown below.

$ tar xvfz archive_file.tar.gz /path/to/dir/

$ tar xvfj archive_file.tar.bz2 /path/to/dir/


*******

7. Extract group of files from tar, tar.gz, tar.bz2 archives using regular expression

You can specify a regex, to extract files matching a specified pattern. For example, following tar command extracts all the files with pl extension.

$ tar xvf archive_file.tar --wildcards '*.pl'

Options explanation:

    –wildcards *.pl – files with pl extension


****

8. Adding a file or directory to an existing archive using option -r

You can add additional files to an existing tar archive as shown below. For example, to append a file to *.tar file do the following:

$ tar rvf archive_name.tar newfile

This newfile will be added to the existing archive_name.tar. Adding a directory to the tar is also similar,

$ tar rvf archive_name.tar newdir/

Note: You cannot add file or directory to a compressed archive. If you try to do so, you will get “tar: Cannot update compressed archives” error as shown below.

$ tar rvfz archive_name.tgz newfile
tar: Cannot update compressed archives
Try `tar --help' or `tar --usage' for more information.

************

9. Verify files available in tar using option -W

As part of creating a tar file, you can verify the archive file that got created using the option W as shown below.

$ tar cvfW file_name.tar dir/

If you are planning to remove a directory/file from an archive file or from the file system, you might want to verify the archive file before doing it as shown below.

$ tar tvfW file_name.tar
Verify 1/file1
1/file1: Mod time differs
1/file1: Size differs
Verify 1/file2
Verify 1/file3

If an output line starts with Verify, and there is no differs line then the file/directory is Ok. If not, you should investigate the issue.

Note: for a compressed archive file ( *.tar.gz, *.tar.bz2 ) you cannot do the verification.


Finding the difference between an archive and file system can be done even for a compressed archive. It also shows the same output as above excluding the lines with Verify.

Finding the difference between gzip archive file and file system

$ tar dfz file_name.tgz

Finding the difference between bzip2 archive file and file system

$ tar dfj file_name.tar.bz2


***************

10. Estimate the tar archive size

The following command, estimates the tar file size ( in KB ) before you create the tar file.

$ tar -cf - /directory/to/archive/ | wc -c
20480

The following command, estimates the compressed tar file size ( in KB ) before you create the tar.gz, tar.bz2 files.

$ tar -czf - /directory/to/archive/ | wc -c
508

$ tar -cjf - /directory/to/archive/ | wc -c
428


****************

Copy data using dd :

1) Backup entire Hard disk :

To backup an entire copy of a hard disk to another hard disk connected to the same system, execute the dd command as shown below. In this dd command example, the UNIX device name of the source hard disk is /dev/hda, and device name of the target hard disk is /dev/hdb.


# dd if=/dev/sda of=/dev/sdb

    “if” represents inputfile, and “of” represents output file. So the exact copy of /dev/sda will be available in /dev/sdb.
    If there are any errors, the above command will fail. If you give the parameter “conv=noerror” then it will continue to copy if there are read errors.

In the copy of hard drive to hard drive using dd command given below, sync option allows you to copy everything using synchronized I/O.

# dd if=/dev/sda of=/dev/sdb conv=noerror,sync


2) Create an Image of a Hard Disk


Instead of taking a backup of the hard disk, you can create an image file of the hard disk and save it in other storage devices.There are many advantages to backing up your data to a disk image, one being the ease of use. This method is typically faster than other types of backups, enabling you to quickly restore data following an unexpected catastrophe.


dd if=/dev/hda of=~/hdadisk.img


3) Restore using HD image:


To restore a hard disk with the image file of an another hard disk, use the following dd command example.

# dd if=hdadisk.img of=/dev/hdb

The image file hdadisk.img file, is the image of a /dev/hda, so the above command will restore the image of /dev/hda to /dev/hdb

4) Creating a Floppy Image

Using dd command, you can create a copy of the floppy image very quickly. In input file, give the floppy device location, and in the output file, give the name of your floppy image file as shown below.

# dd if=/dev/fd0 of=myfloppy.img
5) backup a partition 

You can use the device name of a partition in the input file, and in the output either you can specify your target path or image file as shown in the dd command example below.

# dd if=/dev/hda1 of=~/partition1.img



6) CDROM backup


dd command allows you to create an iso file from a source file. So we can insert the CD and enter dd command to create an iso file of a CD content.

# dd if=/dev/cdrom of=tgsservice.iso bs=2048

dd command reads one block of input and process it and writes it into an output file. You can specify the block size for input and output file. In the above dd command example, the parameter “bs” specifies the block size for the both the input and output file. So dd uses 2048bytes as a block size in the above command.


*****************************

DNS :

http://computernetworkingnotes.com/network-administrations/dns-server.html


main configuration file for dns server is  "named.conf"

chroot features

chroot feature is run named as user  named, and it also limit the files named can see. When installed,  named is fooled into thinking that the directory  /var/named/chroot is actually the  root or / directory. Therefore, named files normally found in the  /etc directory are found in  /var/named/chroot/etc directory instead, and those you would expect to find in  /var/named are actually located in  /var/named/chroot/var/named.

The advantage of the chroot feature is that if a hacker enters your system via a BIND exploit, the hacker's access to the rest of your system is isolated to the files under the chroot directory and nothing else. This type of security is also known as a chroot jail.

Configure dns server:::::::::::::::::

a)DNS packages needed : bind and caching-nameserver 
b)set hostname to  server.example.com and ip address to  192.168.0.254 (vi /etc/sysconfig/network)

main configuration file for dns server is  named.conf
creating a new named.conf file :
c)vi /var/named/chroot/etc/named.conf:

options{
        directory "/var/named/";

};


Configure zone file :::::::::

We have defined two zone files  (example.com.zone for forward zone) and  (0.168.192.in-addr.arpa.zone for reverse zone). These files will be store in /var/named/chroot/var/named/ location. We will use two sample files for creating these files.

Change directory to  /var/named/chroot/var/named and copy the sample files to name which we have set in named.conf


a)cd /var/named/chroot/var/named
b)cp localhost.zone example.com.zone
c)cp named.local 0.168.192.in-addr.arpa for reverse zone

d)update example.com.zone
e)update 0.168.192.in-addr.arpa.zone


change the ownership of these zone files to  named group:
f)chgrp named example.com.zone
g)chgrp named 0.168.192.in-addr.arpa.zone

h)chkconfig named on
service named restart



Conf dns slave server:::::::::::::::


For this example we are using three systems one linux server one linux clients and one window clients:

We have configured master DNS server with ip address of  192.168.0.254 and hostname  server.example.com on linux server. Now we will configure  slave DNS server on linux clients

a)bind and  caching-nameserver rpm is required to configure dns. check them for install if not found install them.

b)set hostname to  client1 and ip address to  192.168.0.1 And create a new  named.conf file:
c)vi /var/named/chroot/etc/named.conf


options{
        directory "/var/named/";

};



d)service named restart


Configure Linux DNS Client ::::::::::::

a)vi /etc/resolv.conf
set  nameserver ip to  192.168.0.254 and  search option to  example.com:

search example.com
nameserver 192.168.0.254

b)service network restart

dig server.example.com to test DNS Server:


c)dig server.example.com

**************8


Read:
http://simplylinuxfaq.blogspot.in/p/major-difference-between-rhel-7-and-6.html

*****************


lvreduce..
reduce "root"

*********

boot partition corrupted

user unable to login after changing password:
pam_tally --user jitesh --reset

user unable to ssh to a specific server:
check the contents of /etc/ssh/sshd_config on the target machine - it is possible that your specific user is not permitted to log in remotely. Specific lines to check for:

PermitRootLogin no # should never allow remote root login

AllowUsers someusername # whitelist of users who are allowed to ssh to the machine

You can probably do a comparison of that file from one of the working machines to see if any lines are different. 


How To rebuild Corrupted RPM Database:

Sometimes rpm database gets corrupted and stops all the functionality of rpm and other applications on the system. So, at the time we need to rebuild the rpm database and restore it with the help of following command.
[root@tecmint]# cd /var/lib
[root@tecmint]# rm __db* (Remove /var/lib/rpm/__db* files to avoid stale locks)
[root@tecmint]# rpm --rebuilddb
[root@tecmint]# rpmdb_verify Packages

**************

Linear vs Striped LV :

lvcreate --extents 100%FREE --stripes 8 --stripesize 256 --name root vol_e27
lvs --segments (ok command to verify)
lvdisply -vm  (best command to verify)


linear volumes writes to the disks in series, as one disk fills up, second one starts..
Striped volumes reads/writes to the disks in roundrobin:
For large sequential reads and writes, this can improve the efficiency of the data I/O.


******




**************

kernel upgrade
rpm -ivh kernel-<kernel_version>.<arch>.rpm 

**************

old version :
service udev status
new version:
systemctl status servicename

*************

Check cpu usage of a specific process:
ps aux|grep nginx
(or)top (or) htop

a = show processes for all users
u = display the process's user/owner
x = also show processes not attached to a terminal

*******

rpm commands:
http://www.tecmint.com/20-practical-examples-of-rpm-commands-in-linux/
*******
chmod 444 /bin/chmod then ? :

rsync -p --chmod=u=rwx,g=rx /bin/chmod /bin/chmod
ls -la /bin/chmod

****************

nmap?

***************
no space left on device
due to inode full

*****************
https://www.youtube.com/watch?v=Qlg8OnClWXw

difference between tar(archive) and zip (archive&compress)
archive = single file out of multiple files without size reduction
zip = single file out of multiple files "with" size reduction

1)tar is an archiver, zip is archiver+compressor
tar can be used as compressor using gz and bz
2)tar preserves metadata info for files & dir,zip does not preserve SUID/sticky bit preservation
3)tar with compression,first make an archive of multiple files & then compress as single;whereas
zip first compresses the file one by one & then make its archive

tar cvf testing.tar * ----------------> testing.tar
mkdir afteruntar
tar xvf testing.tar -C afteruntar\  (option C is to untar to a specific directory)

zip testing.zip file1 file2 file 3
mkdir afterunzip
unzip testing.zip -d afterunzip\


****************

1. Create tar Archive File

The below example command will create a tar archive file tecmint-14-09-12.tar for a directory /home/tecmint in current working directory:
tar -cvf tecmint-14-09-12.tar /home/tecmint/

2. Create tar.gz Archive File

To create a compressed gzip archive file we use the option as "z". For example the below command will create a compressed MyImages-14-09-12.tar.gz file for the directory /home/MyImages:
tar cvfz MyImages-14-09-12.tar.gz /home/MyImages

3. Create tar.bz2 Archive File

The bz2 feature compress and create archive file less than the size of the gzip. The bz2 compression takes more time to compress and decompress files as compared to gzip which takes less time. To create highly compressed tar file we use option as "j". The following example of command will create a Phpfiles-org.tar.bz2 file for a directory /home/php. (Note: tar.bz2 and tbz is similar as tb2):
tar cvfj Phpfiles-org.tar.bz2 /home/php

4. Untar tar Archive File

tar -xvf tecmint-14-09-12.tar

	
7.List Content of tar Archive File

To list the content of tar archive file, just run the following command with option t (list content). The below command will list the content of uploadprogress.tar file.

tar -tvf tecmint-14-09-12.tar

10.Untar Single file from tar File

To extract a single file called cleanfiles.sh from cleanfiles.sh.tar use the following command.
tar -xvf cleanfiles.sh.tar cleanfiles.sh

11. Untar Single file from tar.gz File

To extract a single file tecmintbackup.xml from tecmintbackup.tar.gz archive file:
tar -zxvf tecmintbackup.tar.gz tecmintbackup.xml

12. Untar Single file from tar.bz2 File
To extract a single file called index.php from the file Phpfiles-org.tar.bz2:
tar -jxvf Phpfiles-org.tar.bz2 index.php

13. Untar Multiple files from tar, tar.gz and tar.bz2 File

To extract or untar multiple files from the tar, tar.gz and tar.bz2 archive file. For example the below command will extract “file 1” “file 2” from the archive files.
# tar -xvf tecmint-14-09-12.tar "file 1" "file 2" 
# tar -zxvf MyImages-14-09-12.tar.gz "file 1" "file 2" 
# tar -jxvf Phpfiles-org.tar.bz2 "file 1" "file 2"

14. Extract Group of Files using Wildcard

To extract a group of files we use wildcard based extracting. For example, to extract a group of all files whose pattern begins with .php from a tar, tar.gz and tar.bz2 archive file.
# tar -xvf Phpfiles-org.tar --wildcards '*.php'
# tar -zxvf Phpfiles-org.tar.gz --wildcards '*.php'
# tar -jxvf Phpfiles-org.tar.bz2 --wildcards '*.php'

15. Add Files or Directories to tar Archive File

To add files or directories to existing tar archived file we use the option "r" (append). For example we add file xyz.txt and directory php to existing tecmint-14-09-12.tar archive file.
# tar -rvf tecmint-14-09-12.tar xyz.txt
# tar -rvf tecmint-14-09-12.tar php


NOTE :The tar command don’t have a option to add files or directories to a existing compressed tar.gz and tar.bz2 archive file

17. How To Verify tar, tar.gz and tar.bz2 Archive File

To verify any tar or compressed archived file we use option as "W" (verify). To do, just use the following examples of command
tar tvfW tecmint-14-09-12.tar

NOTE : You cannot do verification on a compressed ( *.tar.gz, *.tar.bz2 ) archive file

18. Check the Size of the tar, tar.gz and tar.bz2 Archive File

To check the size of any tar, tar.gz and tar.bz2 archive file, use the following command. For example the below command will display the size of archvie file in Kilobytes (KB).
# tar -czf - tecmint-14-09-12.tar | wc -c
12820480
# tar -czf - MyImages-14-09-12.tar.gz | wc -c
112640
# tar -czf - Phpfiles-org.tar.bz2 | wc -c
20480

Tar Usage and Options
?c – create a archive file.
?x – extract a archive file.
?v – show the progress of archive file.
?f – filename of archive file.
?t – viewing content of archive file.
?j – filter archive through bzip2.
?z – filter archive through gzip.
?r – append or update files or directories to existing archive file.
?W – Verify a archive file.
?wildcards – Specify patters in unix tar command.


******************


Rsync (Remote Sync): 10 Practical Examples of Rsync Command in Linux :

Rsync (Remote Sync) is a most commonly used command for copying and synchronizing files and directories remotely as well as locally in Linux/Unix systems
perform data backups and mirroring between two Linux machines.


Some advantages and features of Rsync command

    It efficiently copies and sync files to or from a remote system.
    Supports copying links, devices, owners, groups and permissions.
    It’s faster than scp (Secure Copy) because rsync uses remote-update protocol which allows to transfer just the differences between two sets of files. First time, it copies the whole content of a file or a directory from source to destination but from next time, it copies only the changed blocks and bytes to the destination.
    Rsync consumes less bandwidth as it uses compression and decompression method while sending and receiving data both ends.


rsync options source destination


1. Copy/Sync Files and Directory Locally
Copy/Sync a File on a Local Computer


This following command will sync a single file on a local machine from one location to another location. Here in this example, a file name backup.tar needs to be copied or synced to /tmp/backups/ folder.

[root@tecmint]# rsync -zvh backup.tar /tmp/backups/
created directory /tmp/backups
backup.tar
sent 14.71M bytes  received 31 bytes  3.27M bytes/sec
total size is 16.18M  speedup is 1.10

In above example, you can see that if the destination is not already exists rsync will create a directory automatically for destination.

Copy/Sync a Directory on Local Computer

The following command will transfer or sync all the files of from one directory to a different directory in the same machine. Here in this example, /root/rpmpkgs contains some rpm package files and you want that directory to be copied inside /tmp/backups/ folder.

[root@tecmint]# rsync -avzh /root/rpmpkgs /tmp/backups/ (a=to preserve timestamps)
sending incremental file list


2. Copy/Sync Files and Directory to or From a Server
Copy a Directory from Local Server to a Remote Server

rsync -avz rpmpkgs/ root@192.168.0.101:/home/


Copy/Sync a Remote Directory to a Local Machine

Here in this example, a directory /home/tarunika/rpmpkgs which is on a remote server is being copied in your local computer in /tmp/myrpms.

rsync -avzh root@192.168.0.100:/home/tarunika/rpmpkgs /tmp/myrpms

root@192.168.0.100's password:
receiving incremental file list
created directory /tmp/myrpms


3. Rsync Over SSH

Copy a File from a Remote Server to a Local Server with SSH :

To specify a protocol with rsync you need to give “-e” (specify the remote shell to use) option with protocol name you want to use. Here in this example, We will be using “ssh” with “-e” option and perform data transfer

rsync -avzhe ssh root@192.168.0.100:/root/install.log /tmp/

Copy a File from a Local Server to a Remote Server with SSH :

rsync -avzhe ssh backup.tar root@192.168.0.100:/backups/


4. Show Progress While Transferring Data with rsync

To show the progress while transferring the data from one machine to a different machine, we can use ‘–progress’ option for it. It displays the files and the time remaining to complete the transfer.


rsync -avzhe ssh --progress /home/rpmpkgs root@192.168.0.100:/root/rpmpkgs


5. Use of –include and –exclude Options

These two options allows us to include and exclude files by specifying parameters with these option helps us to specify those files or directories which you want to include in your sync and exclude files and folders with you don’t want to be transferred.

Here in this example, rsync command will include those files and directory only which starts with ‘R’ and exclude all other files and directory.

[root@tecmint]# rsync -avze ssh --include 'R*' --exclude '*' root@192.168.0.101:/var/lib/rpm/ /root/rpm

6. Use of –delete Option

If a file or directory not exist at the source, but already exists at the destination, you might want to delete that existing file/directory at the target while syncing .

We can use ‘–delete‘ option to delete files that are not there in source directory.

Source and target are in sync. Now creating new file test.txt at the target.

[root@tecmint]# touch test.txt
[root@tecmint]# rsync -avz --delete root@192.168.0.100:/var/lib/rpm/ .
Password:
receiving file list ... done
deleting test.txt
Target has the new file called test.txt, when synchronize with the source with ‘–delete‘ option, it removed the file test.txt.


7. Set the Max Size of Files to be Transferred

You can specify the Max file size to be transferred or sync. You can do it with “–max-size” option. Here in this example, Max file size is 200k, so this command will transfer only those files which are equal or smaller than 200k.

rsync -avzhe ssh --max-size='200k' /var/lib/rpm/ root@192.168.0.100:/root/tmprpm

8. Automatically Delete source Files after successful Transfer

rsync --remove-source-files -zvh backup.tar /tmp/backups/

9. Do a Dry Run with rsync

If you are a newbie and using rsync and don’t know what exactly your command going do. Rsync could really mess up the things in your destination folder and then doing an undo can be a tedious job.

Use of this option will not make any changes only do a dry run of the command and shows the output of the command, if the output shows exactly same you want to do then you can remove ‘–dry-run‘ option from your command and run on the terminal.

rsync --dry-run --remove-source-files -zvh backup.tar /tmp/backups/


10. Set Bandwidth Limit and Transfer File

You can set the bandwidth limit while transferring data from one machine to another machine with the the help of ‘–bwlimit‘ option. This options helps us to limit I/O bandwidth.

rsync --bwlimit=100 -avzhe ssh  /var/lib/rpm/  root@192.168.0.100:/root/tmprpm/

11. Also, by default rsync syncs changed blocks and bytes only, if you want explicitly want to sync whole file then you use ‘-W‘ option with it.

rsync -zvhW backup.tar /tmp/backups/backup.tar


12. Do Not Overwrite the Modified Files at the Destination (rsync -u)

rsync -avzu thegeekstuff@192.168.200.10:/var/lib/rpm /root/temp

13. Synchronize only the Directory Tree Structure (not the files) (rsync -d)

Use rsync -d option to synchronize only directory tree from source to the destination. The below example, synchronize only directory tree in recursive manner, not the files in the directories.

$ rsync -v -d thegeekstuff@192.168.200.10:/var/lib/ .

14. View the Changes Between Source and Destination :

rsync -avzi thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/
Password:
receiving file list ... done
>f.st.... Basenames
.f....og. Dirnames

In the output it displays some 9 letters in front of the file name or directory name indicating the changes.

In our example, the letters in front of the Basenames (and Dirnames) says the following:

> specifies that a file is being transferred to the local host.
f represents that it is a file.
s represents size changes are there.
t represents timestamp changes are there.
o owner changed
g group changed.



*******************

recover deleted file in your linux system :
shift delete the pdf file on desktop
ps -aux|grep pdf
cd /proc/9478
cd fd
cp 20 /root/Desktop

*******************
grub.conf and menu.lst?
understand chroot 

*******************



ACL:

we cannot set up different permission sets for different users on same directory or file. Thus, Access Control Lists (ACLs) were implemented.

1. Check Kernel for ACL Support :

Run the following command to check ACL Support for file system and POSIX_ACL=Y option (if there is N instead of Y, then it means Kernel doesn’t support ACL and need to be recompiled) :

[root@linux ~]# grep -i acl /boot/config*
CONFIG_EXT4_FS_POSIX_ACL=y
CONFIG_REISERFS_FS_POSIX_ACL=y



2. Check Required Packages:
yum install nfs4-acl-tools acl libacl

3. Check Mounted File System for ACLs Support :

mount  | grep -i root

we have another option to make sure that partition is mounted with acl option or not, because for recent system it may be integrated with default mount option :

[root@linux ~]# tune2fs -l /dev/mapper/fedora-root | grep acl
Default mount options:    user_xattr acl

In the above output, you can see that default mount option already have support for acl. Another option is to remount the partition as shown below.
[root@linux ~]# mount -o remount,acl /


Next, add the below entry to ‘/etc/fstab’ file to make it permanent.
/dev/mapper/fedora-root /	ext4    defaults,acl 1 1

Again, remount the partition.
[root@linux ~]# mount -o remount  /


4. For NFS Server

On NFS server, if file system which is exported by NSF server supports ACL and ACLs can be read by NFS Clients, then ACLs are utilized by client System.

For disabling ACLs on NFS share, you have to add option “no_acl” in ‘/etc/exportfs‘ file on NFS Server. To disable it on NSF client side again use “no_acl” option during mount time.


How to Implement ACL Support in Linux Systems

There are two types of ACLs:
?Access ACLs: Access ACLs are used for granting permissions on any file or directory.
?Default ACLs: Default ACLs are used for granting/setting access control list on a specific directory only.

Difference between Access ACL and Default ACL:
?Default ACL can be used on directory level only.
?Any sub directory or file created within that directory will inherit the ACLs from its parent directory. On the other hand a file inherits the default ACLs as its access ACLs.
?We make use of “–d” for setting default ACLs and Default ACLs are optionals.

Before Setting Default ACLs

To determine the default ACLs for a specific file or directory, use the ‘getfacl‘ command. In the example below, the getfacl is used to get the default ACLs for a folder ‘Music‘.
[root@linux ~]# getfacl Music/
# file: Music/
# owner: root
# group: root
user::rwx
group::r-x
other::r-x
default:user::rwx
default:group::r-x
default:other::rw-

After Setting Default ACLs

To set the default ACLs for a specific file or directory, use the ‘setfacl‘ command. In the example below, the setfacl command will set a new ACLs (read and execute) on a folder ‘Music’.
[root@linux ~]# setfacl -m d:o:rx Music/
[root@linux ~]# getfacl Music/
# file: Music/
# owner: root
# group: root
user::rwx
group::r-x
other::r-x
default:user::rwx
default:group::r-x
default:other::r-x

How to Set New ACLs

Use the ‘setfacl’ command for setting or modifying on any file or directory. For example, to give read and write permissions to user ‘tecmint1‘.
# setfacl -m u:tecmint1:rw /tecmint1/example

How to View ACLs

Use the ‘getfacl‘ command for viewing ACL on any file or directory. For example, to view ACL on ‘/tecmint1/example‘ use below command.
# getfacl /tecmint1/example
# file: tecmint1/example/
# owner: tecmint1
# group: tecmint1
user::rwx
user:tecmint1:rwx
user:tecmint2:r--
group::rwx
mask::rwx
other::---

How to Remove ACLs

For removing ACL from any file/directory, we use x and b options as shown below.
# setfacl -x ACL file/directory  	# remove only specified ACL from file/directory.
# setfacl -b  file/directory   		#removing all ACL from file/direcoty



********************************



tune2fs command


The "tune2fs" command is used by the system administrator to change/modify tunable parameters on ext2, ext3 and ext4 type filesystems. To display the current values that are set you can use the tune2fs command with the "-l" option or use the dumpe2fs command.

tune2fs -l /dev/sda1


Disable Filesystem Check on Boot :

The following parameters should only be used in a test environment where you may be carrying out multiple reboots during the course of the day. The Mount Count and check interval values below are set to "-1" which disables any checking!

tune2fs -c -1 /dev/sda1

tune2fs -i -1 /dev/sda1


-c max-mount-counts : Adjust  the  number  of mounts after which the filesystem will be checked by e2fsck
-C mount-count : Set the number of times the filesystem has been mounted


***********************************


Linux Server Hardening :

Encrypt transmitted data whenever possible with password or using keys / certificates:
1.Use scp, ssh, rsync, or sftp for file transfer
Avoid Using FTP, Telnet, And Rlogin

2: Minimize Software to Minimize Vulnerability
# yum list packageName
 # yum remove packageName

#4: Keep Linux Kernel and Software Up to Date

# yum update

#5: Use Linux Security Extensions
SElinux, strong password, password aging,lock accounts with many failures,

#7: Disable root Login
use sudo

#8: Physical Server Security

You must protect Linux servers physical console access. Configure the BIOS and disable the booting from external devices such as DVDs / CDs / USB pen. Set BIOS and grub boot loader password to protect these settings


#9: Disable Unwanted Services
# chkconfig --list | grep '3:on'

 To disable service, enter:

# service serviceName stop
 # chkconfig serviceName off

#11: Configure Iptables and TCPWrappers

12: Linux Kernel /etc/sysctl.conf Hardening


#13: Separate Disk Partitions

/usr /home/ 


#13.1: Disk Quotas


#16: Use A Centralized Authentication Service


******************
Logrotate runs daily as a cron job (/etc/cron.daily/logrotate) and reads its configuration from /etc/logrotate.conf and from files located in /etc/logrotate.d, if any.

******************

RedhaT satellite server:
Seamlessly Manage your environment, Automate manitenance, updates and processes


Satellite is an on-premise alternative to trying to download all of your content from the Red Hat content delivery network or managing your subscriptions through the Customer Portal. From a performance side, it reduces hits to your network bandwidth because local systems can download everything they need locally; from a security side, it can limit the risks of malicious content or access, even enabling entirely disconnected environments. 

Satellite is composed of a centralized Satellite Server. Depending on your data center setup, organization design, and geographic locations, you can have local Capsule Servers, which are proxies that locally manage content and obtain subscription, registration, and content from the central Satellite Server.


******************

Kdump Conf:
Kexec: Fast boot mechanism which allows booting a linux kernel from the context of already running kernel without going thru bios.


yum install kexec-tools
Conf kdump.conf so that ---> saves vmcore to the root partition:
vi /etc/kdump.conf
ext3 /dev/vda1
/etc/init.d/kdump restart;chkconfig kdump on
cat /proc/sys/kernel/sysrq (make sure sysrq is enabled if disabled 0 then: echo "1">/proc/sys/kernel/sysrq)
crash the kernel and then check:
cd /var/crash/127.0.0.1-2011-05-21-15\:41\:50/
scp vmcore root@192.168.1.9:/tmp


***************



*************

enable remote logging:



samba user,

systcl parameters 
SElinux enforce


******************
Disable root login over SSH:

vi /etc/ssh/sshd__config :

PermitRootLogin no (uncomment)
service sshd restart

********************

Limit SSH users login:

 vi /etc/ssh/sshd_config

Add an AllowUsers line at the bottom of the file with a space separated by list of usernames. For example, user tecmint and sheena both have access to remote ssh.
AllowUsers tecmint sheena
service sshd restart

*******************

http://www.differencebetween.net/object/difference-between-vmware-and-xen/


http://www.storagenetworks.com/writeups/strategies/iscsi-vs-fc/iscsi-vs-fc.php
Scan new luns :
http://sysadmincorner.wordpress.com/2013/02/16/how-to-scan-new-luns-provisioned-to-a-linux-server/
http://computernetworkingnotes.com/file-system-administration/how-to-delete-swap-partition.html


MBR = 512 bytes = Boot loader(446 bytes) + partition table (64 bytes) + boot signature (2 bytes)

chroot:

A chroot is basically a special directory on your computer which prevents applications, if run from inside that directory, from accessing files outside the directory. In many ways, a chroot is like installing another operating system inside your existing operating system. 

Technically-speaking, chroot temporarily changes the root directory (which is normally /) to the chroot directory (for example, /var/chroot). As the root directory is the top of the filesystem hierarchy, applications are unable to access directories higher up than the root directory, and so are isolated from the rest of the system. This prevents applications inside the chroot from interfering with files elsewhere on your computer. 


Uses of chroots

The following are some possible uses of chroots: 
1.Isolating insecure and unstable applications 
2.Running 32-bit applications on 64-bit systems 
3.Testing new packages before installing them on the production system 
4.Running older versions of applications on more modern versions of Ubuntu 
5.Building new packages, allowing careful control over the dependency packages which are installed 


********************************************

/etc/fstab:

# file system  mount-point  FS type     Mount options             dump  fsck
#                                                            

/dev/<xxx>     /            ext4    defaults            1     1


*******************************************

DRBD is a software-based, shared-nothing, replicated storage solution mirroring the content of block devices (hard disks, partitions, logical volumes etc.) between hosts.

DRBD mirrors data

*******************************************
Oracle Automatic Storage Management

Oracle ASM is a volume manager and a file system for Oracle database files
oracleasm createdisk DISK010 /dev/sdp1

*******************************************


DevOps (a clipped compound of development and operations) is a culture, movement or practice that emphasizes the collaboration and communication of both software developers and other information-technology (IT) professionals while automating the process of software delivery and infrastructure changes.[1][2] It aims at establishing a culture and environment where building, testing, and releasing software, can happen rapidly, frequently, and more reliably.[3][4][5]

Devops integrates developers and ops team in order to improve collboration and productivity by automating infrastructure,workflows and continuosly measuring app performance

write small chunks of code and test, instead of large codes 

benefits:
1)as automated, can focus on improving business
2)faster time to market


newcode -->Jenkins Testing Code -->tool for source control (Github - track all changes to code) --> Conf mgmt (Chef,Puppet)-->new code gets installed on servers


*****************************************


grub :

grub2 has been in ubuntu and suse since quite some time , from RHEL7 grub2 is used (earlier also grub.cfg was available but as a symbolic link to menu.lst)

https://www.youtube.com/watch?v=FjnT6utpeq4


1st stage: MBR
2nd stage: /boot


grub 1:
/boot/grub/menu.lst
grub-update , grub-install


grub 2:
/etc/grub2/grub.cfg
grub2-mkconfig -o /boot/grub2/grub.cfg
/etc/default/grub
/etc/grub.d/..........
grub2-install


1)debian/ubuntu -- uses grub2
2)Enterprise 6 -- uses grub1
3)Enterprise 7+ -- uses grub2 



difference between redhat 6 and 7 


Ubuntu  (always used grub2):
make changes here for grub2 :
/etc/default/grub (and) /etc/grub.d
Then run : grub-mkconfig -o /boot/grub/grub.cfg , then reboot to check 


Redhat 7:
make changes here for grub2 :
/etc/default/grub (and) /etc/grub.d
Then run : grub2-mkconfig -o /boot/grub2/grub.cfg , then reboot to check timeout 



***********************************


ifconfig eth0 160.62.173.96 netmask 255.255.255.0 broadcast 160.62.173.255
route add default gw 160.62.173.1 eth0

xm info gives you (amongst others) the following lines:

xen_major : 3
xen_minor : 0
xen_extra : .2

which means that you are using version 3.0.2.

*********

Check pacemaker version:

Pacemaker 1.1.6

Depending on your setup and version there are multiple ways to find out:

Package management:

rpm -qa|grep -i pacemaker


dpkg -l|grep -i pacemaker

Cluster executable:

crmadmin --version

>From a running clusters cib:

cibadmin -Q|grep dc-version

******

[root@nrusca-slp9996 ~]# puppet --version
3.7.4

****************

vipw -s : Edit /etc/passwd file (which is actually /etc/shadow) (s=secure file editing)
vigr -s : Edit /etc/group file  (which is actually /etc/gshadow) (s=secure file editing)

************************

To check if /u01 is currently in use:

find /u01 ! -type d -exec fuser -u {} \;

***********************
Scan Disks :
for bus in `ls /sys/class/scsi_host`;do echo Scanning $bus; echo "- - -" > /sys/class/scsi_host/$bus/scan; done
(or)
echo "- - -" > /sys/class/scsi_host/host1/scan for host0, 1,2 ,3..)

echo "- - -" > /sys/class/scsi_host/host1/scan

The three values stand for channel, SCSI target ID, and LUN. The dashes act as wildcards meaning "rescan everything"

****************************************


Drop Caches :
sync; echo 3 > /proc/sys/vm/drop_caches
Issued Command to free PageCache,Dentries and Inodes, need to monitor

**********************


1. copy script at /etc/init.d
2. chkconfig --add <service name>
3. chkconfig --level 35 i2eserver_mw on
4. chkconfig --list | grep i2eserver_mw


**********************

modprobe.d, modprobe.conf - Configuration directory/file for modprobe ::

modprobe is a Linux program originally written by Rusty Russell and used to add a loadable kernel module (LKM) to the Linux kernel or to remove an LKM from the kernel. It is commonly used indirectly: udev relies upon modprobe to load drivers for automatically detected hardware.
As of 2010 modprobe is distributed as part of the software package "module-init-tools",[1] for Linux kernel version 2.6 and late


blacklist modulename

 there are cases where two or more modules both support the same devices, or a module invalidly claims to support a device: the blacklist keyword indicates that all of that particular module's internal aliases are to be ignored.
Copyright

cd /etc/modprobe.d

************************
ethtool -s eth1 speed 1000 duplex full autoneg on
*****************

Failed actions and count :

crm_resource –C –H vhst02#### -r vsrv02
crm_resource –C –H vht01#### -r vsrv01
crm_failcount -G -r vsrv01
crm_failcount -v 0 -r vsrv01

******

crm resource stop saps01
cd /opt/wmxenadm/
./get_vm_location.pl saps01
crm resource start saps01
/sbin/rcdrbd status
show cli-prefer-saps01
crm confiure 

*********

no-quoram-policy:

What to do when the cluster does not have quorum. Allowed values:

    ignore - continue all resource management
    freeze - continue resource management, but don't recover resources from nodes not in the affected partition
    stop - stop all resources in the affected cluster parition
    suicide - fence all nodes in the affected cluster partition 

***********

for server in 'cat list';do ssh $server "grep oracle /etc/passwd";done

for server in `cat list`;do ping -c1 -w1 $server;done

for i in 1 2 3; do sudo ssh nrusca-clt0900$i 'umount /mongodb_backup'; done

for i in 1 2 3; do sudo ssh nrusca-clt0900$i 'puppet agent --test'; done

for i in 1 2 3; do sudo ssh nrusca-clt0900$i 'df -Ph |grep mongo'; done

for i in `seq 093 120`; do host nrusca-slt$(printf %04d $i); done

for server in `cat list`; do ssh $server 'puppet agent --test'; done 

for server in `cat list`;do echo $server `ssh $server "uname -m"`;done|grep -v x86_64 
for server in `cat list`;do echo $server `ssh $server "cat /etc/redhat-release"`;done
for server in `cat list`;do scp -p sbs_chroot.sh $server:/usr/local/bin/sbs_chroot.sh;done

grep "^Apr 29" /var/log/syslog|less
grep "^Apr 29" /var/log/syslog > syslog.20150429
du -sh syslog.2015042*
zip -m syslog syslog.2015042*
du -sh syslog.zip
mv syslog.zip ~adm_narayji2


***************
ssh from one server to another:

ssh-keygen -t rsa
copy this id_rsa.pub key under .ssh/authorised_keys of server b(after mkdir .ssh (700)and vi auth_keys (600))



Autologin using pageant

create a private key (puttygen.exe)and save it somewhere and copy the public key to the server u need to autologin -- ie: mkdir .ssh (700)and vi authorized_keys (paste)(600), launch putty agent and put the passphrase (HCohLEaE or hi) and then login it will be auto logged in until reboot

**************

Kernel panic, pv and vgmove, suspend process, kill and kill -9
Difference between clone and templat,lvm configuration filee, lvm limits
can we move vms between 2 datastores, initrd,grub conf file, kdump configure
why only 4 primary partitions

*************

FS goes in RO usually bcos of 
a)I/O Errors in the underlying disk 
b)High disk I/O retry error can mark low level disk call as failed. This will force ext3 to go into read only mode
c)High disk I/O on SAN
d) SAN is not configured properly for the path failover.

In all sort of problems ext3 goes read-only to protect the filesystem and further damage

*************

sync; echo 3 > /proc/sys/vm/drop_caches (vm - virtual memory)

"sync" only makes dirty cache to clean cache. cache is still preserved. drop_caches doesn't touch dirty caches and only drops clean caches. So to make all memory free, it is necessary to do sync first before drop_caches in case flushing daemons hasn't written the changes to disk

*************
du -kax /|sort -rn|less

Take a backup of /etc/fstab
cp -vip /etc/fstab /etc/fstab.back.`date +'%Y%m%d'` 

************

Linux colours:

Blue color - Directory

Green color - Executable or recognized data file

Sky Blue Color - Linked file

yellow with black background - device

Pink colour - graphic image file

Red - Archive file

************
fdisk -l | grep Disk|grep -v mapper | grep -v identifier | grep -v WARNING

************

phusca-s6679
\\phusca-s6679\d$

************

Putty logs :

H:\data\puttylogs\&Y&M&D-&T-&H-putty.log

**********

**** IT-5101 ****
nrusca-slt0134   (Closed)
Cambridge
OS : Redhat 6
Ram :4
processors:2
Disk space 10 Gb
Classification : Test
Server Role : Middleware
Project Name : Lime Survey
App Name : LimeSurvey
c64578269885d6de1a9cb7b89547d02d
112433

***********

History with date and time :

export HISTTIMEFORMAT='%F %T  '

**********
sudo -u root bash
sudo -u apache bash

**********
While discussion with Patrick I observed important root cause for the disk utilization:

Below open file was consuming high disk space
[root@nrusca-slp0156 ~]# lsof /apps/ | grep "logos.channeladapter.log"
java       3441 sys_eventhub1  323w   REG  253,2 363410405441 16778966 /apps/sys_eventhub1/apps/event_hub/shared/logs/logos.channeladapter.log (deleted)
[root@nrusca-slp0156 ~]#

***********


Quota :

1. Enable quota check on filesystem :


cat /etc/fstab
LABEL=/home    /home   ext2   defaults,usrquota,grpquota  1 2

(Reboot or remount the server after the above change) 

2. Initial quota check on Linux filesystem using quotacheck

quotacheck -avug 



    a: Check all quota-enabled filesystem
    v: Verbose mode
    u: Check for user disk quota
    g: Check for group disk quota

quotacheck -cug /home

The above command will create a aquota file for user and group under the filesystem directory as shown below.

# ls -l /home/

-rw-------    1 root     root        11264 Jun 21 14:49 aquota.user
-rw-------    1 root     root        11264 Jun 21 14:49 aquota.group

3. Assign disk quota to a user using edquota command

# edquota ramesh

Disk quotas for user ramesh (uid 500):
  Filesystem           blocks       soft       hard     inodes     soft     hard
  /dev/sda3           1419352          0          0       1686        0        0

4. Report the disk quota usage for users and group using repquota

 repquota /home
*** Report for user quotas on device /dev/sda3
Block grace time: 7days; Inode grace time: 7days
                        Block limits                File limits
User            used    soft    hard  grace    used  soft  hard  grace
----------------------------------------------------------------------
root      --  566488       0       0           5401     0     0
nobody    --    1448       0       0             30     0     0
ramesh    -- 1419352       0       0           1686     0     0
john      --   26604       0       0            172     0     0


5. Add quotacheck to daily cron job

Add the quotacheck to the daily cron job. Create a quotacheck file as shown below under the /etc/cron.daily directory, that will run the quotacheck command everyday. This will send the output of the quotacheck command to root email address.

# cat /etc/cron.daily/quotacheck
quotacheck -avug



*******************************************************


On Linux, you can setup disk quota using one of the following methods:

File system base disk quota allocation
User or group based disk quota allocation

On the user or group based quota, following are three important factors to consider:

Hard limit – For example, if you specify 2GB as hard limit, user will not be able to create new files after 2GB

Soft limit – For example, if you specify 1GB as soft limit, user will get a warning message “disk quota exceeded”, once they reach 1GB limit. But, they’ll still be able to create new files until they reach the hard limit

Grace Period – For example, if you specify 10 days as a grace period, after user reach their hard limit, they would be allowed additional 10 days to create new files. In that time period, they should try to get back to the quota limit.


1. Enable quota check on filesystem

cat /etc/fstab

LABEL=/home /home ext2 defaults,usrquota,grpquota 1 2


************************************
Disk space can be restricted by implementing disk quotas which alert a system administrator before a user consumes too much disk space or a partition becomes full.
Disk quotas can be configured for individual users as well as user groups. This makes it possible to manage the space allocated for user-specific files (such as email) separately from the space allocated to the projects a user works on (assuming the projects are given their own groups).
In addition, quotas can be set not just to control the number of disk blocks consumed but to control the number of inodes (data structures that contain information about files in UNIX file systems). Because inodes are used to contain file-related information, this allows control over the number of files that can be created.
The quota RPM must be installed to implement disk quotas.

15.1. Configuring Disk Quotas
To implement disk quotas, use the following steps:
Enable quotas per file system by modifying the /etc/fstab file.
Remount the file system(s).
Create the quota database files and generate the disk usage table.
Assign quota policies.


15.1.1. Enabling Quotas
As root, using a text editor, edit the /etc/fstab file.
Example 15.1. Edit /etc/fstab

For example, to use the text editor vim type the following:
# vim /etc/fstab

Add the usrquota and/or grpquota options to the file systems that require quotas:
Example 15.2. Add quotas

/dev/VolGroup00/LogVol00 /         ext3    defaults        1 1 
LABEL=/boot              /boot     ext3    defaults        1 2 
none                     /dev/pts  devpts  gid=5,mode=620  0 0 
none                     /dev/shm  tmpfs   defaults        0 0 
none                     /proc     proc    defaults        0 0 
none                     /sys      sysfs   defaults        0 0 
/dev/VolGroup00/LogVol02 /home     ext3    defaults,usrquota,grpquota  1 2 
/dev/VolGroup00/LogVol01 swap      swap    defaults        0 0 . . .
In this example, the /home file system has both user and group quotas enabled.

Note
The following examples assume that a separate /home partition was created during the installation of Red Hat Enterprise Linux. The root (/) partition can be used for setting quota policies in the /etc/fstab file.
15.1.2. Remounting the File Systems
After adding the usrquota and/or grpquota options, remount each file system whose fstab entry has been modified. If the file system is not in use by any process, use one of the following methods:
Issue the umount command followed by the mount command to remount the file system. Refer to the man page for both umount and mount for the specific syntax for mounting and unmounting various file system types.
Issue the mount -o remount file-system command (where file-system is the name of the file system) to remount the file system. For example, to remount the /home file system, the command to issue is mount -o remount /home.
If the file system is currently in use, the easiest method for remounting the file system is to reboot the system.
15.1.3. Creating the Quota Database Files
After each quota-enabled file system is remounted run the quotacheck command.
The quotacheck command examines quota-enabled file systems and builds a table of the current disk usage per file system. The table is then used to update the operating system's copy of disk usage. In addition, the file system's disk quota files are updated.
To create the quota files (aquota.user and aquota.group) on the file system, use the -c option of the quotacheck command.
Example 15.3. Create quota files

For example, if user and group quotas are enabled for the /home file system, create the files in the /home directory:
# quotacheck -cug /home

The -c option specifies that the quota files should be created for each file system with quotas enabled, the -u option specifies to check for user quotas, and the -g option specifies to check for group quotas.
If neither the -u or -g options are specified, only the user quota file is created. If only -g is specified, only the group quota file is created.
After the files are created, run the following command to generate the table of current disk usage per file system with quotas enabled:
# quotacheck -avug
The options used are as follows:
a
Check all quota-enabled, locally-mounted file systems
v
Display verbose status information as the quota check proceeds
u
Check user disk quota information
g
Check group disk quota information
After quotacheck has finished running, the quota files corresponding to the enabled quotas (user and/or group) are populated with data for each quota-enabled locally-mounted file system such as /home.
15.1.4. Assigning Quotas per User
The last step is assigning the disk quotas with the edquota command.
To configure the quota for a user, as root in a shell prompt, execute the command:
# edquota username
Perform this step for each user who needs a quota. For example, if a quota is enabled in /etc/fstab for the /home partition (/dev/VolGroup00/LogVol02 in the example below) and the command edquota testuser is executed, the following is shown in the editor configured as the default for the system:
Disk quotas for user testuser (uid 501):   
Filesystem                blocks     soft     hard    inodes   soft   hard   
/dev/VolGroup00/LogVol02  440436        0        0     37418      0      0
Note
The text editor defined by the EDITOR environment variable is used by edquota. To change the editor, set the EDITOR environment variable in your ~/.bash_profile file to the full path of the editor of your choice.
The first column is the name of the file system that has a quota enabled for it. The second column shows how many blocks the user is currently using. The next two columns are used to set soft and hard block limits for the user on the file system. The inodes column shows how many inodes the user is currently using. The last two columns are used to set the soft and hard inode limits for the user on the file system.
The hard block limit is the absolute maximum amount of disk space that a user or group can use. Once this limit is reached, no further disk space can be used.
The soft block limit defines the maximum amount of disk space that can be used. However, unlike the hard limit, the soft limit can be exceeded for a certain amount of time. That time is known as the grace period. The grace period can be expressed in seconds, minutes, hours, days, weeks, or months.
If any of the values are set to 0, that limit is not set. In the text editor, change the desired limits.
Example 15.4. Change desired limits

For example:
Disk quotas for user testuser (uid 501):   
Filesystem                blocks     soft     hard   inodes   soft   hard   
/dev/VolGroup00/LogVol02  440436   500000   550000    37418      0      0

To verify that the quota for the user has been set, use the command:
# quota username
Disk quotas for user username (uid 501): 
   Filesystem  blocks   quota   limit   grace   files   quota   limit   grace
     /dev/sdb    1000*   1000    1000               0       0       0
15.1.5. Assigning Quotas per Group
Quotas can also be assigned on a per-group basis. For example, to set a group quota for the devel group (the group must exist prior to setting the group quota), use the command:
# edquota -g devel
This command displays the existing quota for the group in the text editor:
Disk quotas for group devel (gid 505):   
Filesystem                blocks    soft     hard    inodes    soft    hard   
/dev/VolGroup00/LogVol02  440400       0        0     37418       0       0
Modify the limits, then save the file.
To verify that the group quota has been set, use the command:
# quota -g devel
15.1.6. Setting the Grace Period for Soft Limits
If a given quota has soft limits, you can edit the grace period (i.e. the amount of time a soft limit can be exceeded) with the following command:
# edquota -t
This command works on quotas for inodes or blocks, for either users or groups.


**************************************************

Linux User Disk Quota Implementation
Jun 9, 2008
What is disk quota?
Ans : Disk quota is nothing but restricting the disk-space usage to the users. We have to remember one thing when we are dealing with disk quota i.e Disk Quota can be applied only on disks/partitions not on files and folders.


 
So how we can implement disk quota?
Disk quota can be implemented in two ways

a. On INODE
b. On BLOCK

What is an INODE?
Ans : In Linux every object is consider as file, every file will be having an inode number associated and this is very much easy for computer to recognise where the file is located.

Inode stands for Index Node, and is the focus of all file activities in the UNIX file-system.
Each file has one inode that defines the file’s type (regular, directory, device etc),The location on disk, The size of the file, Access permissions, Access times.

Note that the file’s name is not stored in the inode.

So how to know what is your file Inode number?

Ans : Its just simple execute ls -i on your file.


ls -i xmls.txt


13662 xmls.txt

I think now you got what is INODE? Lets move on to BLOCK.

A block usually represents one least size on a disk, usually one block equal to 1kb. Some terms in Disk quota.

Soft limit : This is the disk limit where the user gets just a warning message saying that your disk quota is going to expire. This is just a warning, no restriction on data creation will occur at this point.

Hard limit : This is the disk limit where user gets error message, I repeat user gets error message stating that unable to create data.

Implementing QUOTA :
Step1 : Select/prepare the partition for quota, most of the time disk quota is implemented for restricting users not to create unwanted data on servers, so we will implement disk quota on /home mount point.


#vi /etc/fstab

Edit the /home mount point as follows
Before editing


/dev/hda2 /home ext3 defaults 0 0

after editing 


/dev/hda2 /home ext3 defaults,usrquota 0 0

Step2 : Remounting the partition(this is done because the mount table should be updated to kernel). Other wise you can reboot the system too for updating of mount table, which is not preferred for live servers.

#mount -o remount,rw /home
Here -o specifies options, with remounting /home partition with read and write options.

Step3 : Creating quota database

 Linuxnix-free-e-book
#quotacheck -cu /home
The option -c for creating disk quota DB and u for user
Check for user database is created or not when you give ls /home you have to see auota.user file in /home directory,which contains user database.

Step4 : Switching on quota

#quotaon /home
Now get the report for default quota values for user surendra

#repquoata -a | grep surendra
surendra_anne --       4       0       0              1     0     0
surendra_a --             4       0       0              1     0     0
surendra_test --       16       0       0              4     0     0

Step5 : Now implementing disk quota for user phani on /home mount point(/dev/hda2)

#setquota -u surendra_anne 100 110 0 0 /dev/hda2

Step6 : Checking quota is implemented or not login to user surendra_anne and execute this command

#repquota -a 
or

#quota
Step7 : Keep creating data, once 100MB is reached user will get an warning message saying, and when he reaches 110MB he can not create any more data.

Hint : To create a data file you can use seq command as below

#seq 1 10000 > test.txt

this command will create a file with 10000 lines with numbers in it.

Removing quota :
To do this one, all the users should log out from the system so better do it in runlevel one.

Step8 : Stop the disk quota

#quotaoff /home
Step9 : Removing quota database which is located /home

#rm /home/aquota.user

Step10 : Edit fstab file and remove usrdata from /home line

#vi /etc/fstab
Before editing

/dev/hda2 /home ext3 defaults,usrquota 0 0
After editing

/dev/hda2 /home ext3 defaults 0 0
Step11 : Remount the /home partition

#mount -o remount,rw /home
That’s it you are done with Disk Quota Implementation in Linux. Now test your self in creating Linux user disk quota on your own.


**************************

configure quotas :

1)edit /etc/fstab :
Add the usrquota and/or grpquota options to the file systems that require quotas: 

2)remount that FS

3)create quota files:
quotacheck -cug /home ( c-quota files to be created, u - check for user quotas, g - check for group quotas


After the files are created, run the following command to generate the table of current disk usage per file system with quotas enabled: 

quotacheck -avug ( a-Check all quota-enabled, locally-mounted file systems )

After quotacheck has finished running, the quota files corresponding to the enabled quotas (user and/or group) are populated with data for each quota-enabled locally-mounted file system such as /home. 

4) Assigning Quotas per User

edquota username


Disk quotas for user testuser (uid 501):   
Filesystem                blocks     soft     hard    inodes   soft   hard   
/dev/VolGroup00/LogVol02  440436        0        0     37418      0      0



Change desired limits : (vi)

Disk quotas for user testuser (uid 501):   
Filesystem                blocks     soft     hard   inodes   soft   hard   
/dev/VolGroup00/LogVol02  440436   500000   550000    37418      0      0

To verify that the quota for the user has been set, use the command: 

quota username


5)  Assigning Quotas per Group

edquota -g devel (This command displays the existing quota for the group in the text editor: )

 Modify the limits, then save the file.
To verify that the group quota has been set, use the command:
# quota -g devel

6)

16.1.6. Setting the Grace Period for Soft Limits
If a given quota has soft limits, you can edit the grace period (i.e. the amount of time a soft limit can be exceeded) with the following command:

# edquota -t

********
